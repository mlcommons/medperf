# Data Preparator MLCube
## Purpose
Data Preparators are in charge of standardizing the input data format models expect to receive. Additionally, they provide tools for testing the integrity of the data and extracting valuable insights from it.

## Hello World task
To showcase a basic example of how Medperf MLCubes work under the hood, we provide a toy Hello World benchmark. This benchmark implements a pipeline for ingesting people's names, and generates greetings for those names given some criteria. Although this is not the most scientific example, it provides a clear idea of all the pieces that are required to implement your own MLCubes for Medperf.

You can find the Data Preparator MLCube code [here](https://github.com/mlcommons/medperf/examples/HelloWorld/data_preparator)

## How to run
Before we dig into the code, let's first try to manually run the Data Preparator MLCube. During this process, it should be possible to see how MLCube interacts with the folders in the workspace, and what is expected to happen during each step:

### Setup

1. Clone the repository.
    ```bash
    git clone https://github.com/mlcommons/medperf
    cd medperf
    ```

2. Install mlcube and mlcube-docker using pip
    ```bash
    pip install mlcube mlcube-docker
    ```

3. Navigate to the HelloWorld directory within the examples folder with
    ```bash
    cd examples/HelloWorld
    ```

4. Change to the current example's `mlcube` folder with
    ```bash
    cd data_preparator/mlcube
    ```

### Running the Prepare Task
The first step that should be executed with a Data Preparator MLCube is the `prepare` task. This task ingests raw data, and makes preliminary transformations to ensure it is ready to be ingested by the Model MLCube later on.

1. Let's first observe the raw data for the Hello World example
   ```bash
   cat workspace/names/names.txt
   ```
   ```title="names/names.txt"
   --8<-- "examples/HelloWorld/data_preparator/mlcube/workspace/names/names.txt"
   ```
   Here we see the names that are expected to be used by the model to generate greetings from.
   ```bash
   cat workspace/labels/labels.csv
   ```
   ```title="labels/labels.csv"
   --8<-- "examples/HelloWorld/data_preparator/mlcube/workspace/labels/labels.csv"
   ```
   Additionally, we have the labels for this task. In this case, labels represent what we expect to be generated by the models for this problem.

2. Run the `prepare` task with the mlcube
    ```bash
    mlcube run --task=prepare
    ```
    Running this task will execute the HelloWorld MLCube with the default values as seen in the corresponding [`mlcube.yaml`](#mlcubemlcubeyaml). You can also define different values for the parameters in the following manner
    ```bash
    mlcube run --task=prepare data_path=names
    ```

3. Check the resulting data using
    ```bash
    cat workspace/data/names.csv
    ```
    ```title="data/names.csv"
    --8<-- "examples/HelloWorld/data_preparator/mlcube/workspace/data/names.csv"
    ```
    Here we see that the Data Preparator has transformed the raw names into a `csv` file, which no longer includes the full name, but only the first nad last name. This is the kind of transformations that a Data Preparator MLCube can do. By providing logic for handling raw data, we allow model authors to focus in the model design, and also ideally standardize the data even if it comes from different sources.

### Running the Sanity Check Task
Now that we have executed the logic to ingest and transform the raw data into usable data, we need to ensure that these transformations have gone well. For that, a Data Preparator contains the `sanity_check` task. This task ingests the prepared dataset, and computes assertions and quality checks to ensure everything looks as expected.

1. Run the `sanity_check` task with
     ```bash
     mlcube run --task=sanity_check
     ```

If everything went well, you shouldn't see anything happen after executing this task. This is because **The `sanity_check` task is only expected to fail and exit if there's an anomaly with the data**. Else, the `sanity_check` task will complete successfully. You can try this out too! We have provided an invalid version of the prepared data

2. Check the contents of the corrupted prepared data:
   ```bash
   cat workspace/invalid_data/names.csv
   ```
   ```title="invalid_data/names.csv"
   --8<-- "examples/HelloWorld/data_preparator/mlcube/workspace/invalid_data/names.csv"
   ```

   You may notice this version now contains an unexpected column. This is something the `sanity_check` task doesn't expect, and it should react accordingly

3. Run the `sanity_check` task with the invalid data
   ```bash
   mlcube run --task=sanity_check data_path=invalid_data
   ```

   You should see the command failing. This is exactly what we expect from the Sanity Check Task.

### Running the Statistics Task
Lastly, the data preparator contains a `statistics` task for obtaining and sharing insights on the prepared dataset. These insights are to be shared under the approval of the Data Owner, and as such they should not disclose private data, but instead general and aggregated statistics.

1. Run the `statistics` task using
    ```bash
    mlcube run --task=statistics
    ```

By running the `statistics` task, a new file is created (`workspace/statistics.yaml`). This file contains the computed statistics.

2.  Check the resulting statistics using the following command:
    ```bash
    cat workspace/statistics.yaml
    ```
    ```title="statistics.yaml"
    --8<-- "examples/HelloWorld/data_preparator/mlcube/workspace/statistics.yaml"
    ```
    Here we can see some simple statistics computed on the `names.csv` file we inspected before. These statistics may serve Benchmark and Model Owners to understand the nature and the challenges that this specific dataset posseses. 

That's it! You just built and ran a hello-world data preparator mlcube! Now it's time to build one from scratch!

## Building a Data Preparator MLCube
The following section will describe how you can create a Data Preparator Cube from scratch. We will go through the set of commands provided to help during this process, as well as the contents of a Data Preparator MLCube.

### Setup
MedPerf provides some cookiecutter templates for all the related MLCubes. Additionally, it provides commands to easily retreive and use these templates. For that, we need to make sure MedPerf is installed

1. If you haven't done so, clone the repository.
    ```bash
    git clone https://github.com/mlcommons/medperf
    cd medperf
    ```

2. Install the MedPerf CLI
    ```bash
    pip install -e cli
    ```

3. If you haven't done so, create a folder for keeping all MLCubes created in this tutorial
    ```bash
    mkdir tutorial
    cd tutorial
    ```

4. Create a Data Preparator MLCube through MedPerf
    ```bash
    medperf mlcube create data_preparator
    ```
    You should be prompted to fill in some configuration options through the CLI, below is an example of some good options to provide for this specific task
    ```bash
    $ medperf mlcube create data_preparator                      
    MedPerf 0.0.0
    project_name [Data Preparator MLCube]: Hello World Data Preparator MLCube # (1)!
    project_slug [hello_world_data_preparator_mlcube]: data_preparator_mlcube # (2)!
    description [Data Preparator MLCube Template. Provided by MLCommons]: Hello World Data Preparator implementation from scratch # (3)!
    author_name [John Smith]: John Smith # (4)!
    accelerator_count [0]: 0 # (5)!
    docker_image_name [docker/image:latest]: johnsmith/hello_world_dataprep:0.0.1 # (6)!
    use_separate_output_labels [n]: n # (7)!
    ```
    1. Gives a Human-readable name to the MLCube Project.
    2. Determines how the Data Preparator root folder will be named.
    3. Gives a Human-readable description to the MLCube Project.
    4. Documents the MLCube implementation by specifying the author. Please use your own name here.
    5. Indicates how many GPUS should be visible by the MLCube. Useful for Model MLCubes.
    6. MLCubes use containers under the hood. Medperf supports both Docker and Singularity. Here, you can provide an image tag to the image that will be created by this MLCube. **It's recommended to use a naming convention that allows you to upload it to Docker Hub.**
    7. Data Preparators can output the labels in a separate location to the data. This is useful in situations where you can't trust Model Owners to access the labels during inference (For example during challenges). For this example, we will output data and labels on the same folder

!!! note
    MedPerf is running CookieCutter under the hood. This medperf command provides additional arguments for handling different scenarios. You can see more information on this by running `medperf mlcube create --help`

After running the previous command, you should be able to see a folder created, named after the `project_slug` provided during configuration.

### Contents

5. Inspect the generated folder structure
    ```bash
    tree 
    ```
    ```bash
    .
    └── data_preparator_mlcube
        ├── mlcube
        │   ├── mlcube.yaml # (1)!
        │   └── workspace # (2)!
        │       ├── data # (3)!
        │       ├── input_data # (4)!
        │       ├── labels # (5)!
        │       └── parameters.yaml # (6)!
        └── project # (7)!
            ├── Dockerfile # (8)!
            └── mlcube.py # (9)!

    7 directories, 4 files
    ```
    1. The `mlcube.yaml` file contains metadata about your data preparation procedure, including its interface. For MedPerf, we require three tasks: `prepare`, `sanity_check`, and `statistics`.
    2. The `workspace` contains all the files and paths that can be used by the MLCube, as long as those paths are specified inside the `mlcube.yaml`.
    3. The `data` folder is where the prepared data will be contained after running the `prepare` task.
    4. The `input_data` is where the MLCube will look for raw data by default.
    5. The `labels` is where the MLCube will look for labels by default.
    6. This file provides ways to parameterize the data preparation process. You can set any key-value pairs that should be easily modifiable to adjust your mlcube's behavior. This file is mandatory but can be left blank if parametrization is unnecessary, like in this example.
    7. Contains the actual implementation code of the mlcube.
    8. A default `Dockerfile` which by default installs `python3.6` and any requirements for this MLCube to work.
    9. `mlcube.py` provides a bare-bones command-line interface for a Data Preparator MLcube to run. The logic inside each command is intentionally left blank.

Let's go through each of the created files and modify them as needed to create a Hello World Data Preparator MLCube.

#### `mlcube/mlcube.yaml`
The `mlcube.yaml` file contains metadata about the data preparation procedure. 

```yaml title="mlcube.yaml'
name: Hello World Data Preparator MLCube # (1)!
description: Hello World Data Preparator implementation from scratch # (2)!
authors:
 - {name: John Smith} # (3)!

platform:
  accelerator_count: 0 # (4)!

docker:
  # Image name
  image: johnsmith/hello_world_dataprep:0.0.1 # (5)!
  # Docker build context relative to $MLCUBE_ROOT. Default is `build`.
  build_context: "../project"
  # Docker file name within docker build context, default is `Dockerfile`.
  build_file: "Dockerfile"

tasks:
  prepare:
    parameters:
      inputs: {data_path: input_data, labels_path: input_labels, parameters_file: parameters.yaml}
      outputs: {
        output_path: data/, #(6)!
        
      }
  sanity_check:
    parameters:
      inputs: {data_path: data/, parameters_file: parameters.yaml}
  statistics:
    parameters:
      inputs: {data_path: data/, parameters_file: parameters.yaml}
      outputs: {output_path: {type: file, default: statistics.yaml}}
```

``` yaml title="mlcube.yaml"
tasks:
    prepare: # (1)!
        parameters:
            inputs: {
                data_path: names, # (2)!
                labels_path: labels, # (3)!
                parameters_file: parameters.yaml, # (4)!
            }
            outputs:
                output_path: {type: directory, default: data} # (5)!
                # output_labels_path: {type: directory, default: output_labels} (6)

    sanity_check: # (7)!
        parameters:
            inputs: {
                data_path: data, # (8)!
                parameters_file: parameters.yaml # (9)!
            }

    statistics: # (10)!
        parameters:
            inputs: {
                data_path: data, # (11)!
                parameters_file: parameters.yaml # (12)!
            }
            outputs: {
                output_path: statistics.yaml # (13)!
            }
```

1. **Required**. The prepare task transforms the input data and labels into the format expected by model cubes
2. **Required**. Where to find the input raw data. **MUST** be a folder.
3. **Required**. Where to find the input labels. **MUST** be a folder.
4. **Required**. Helper file to provide additional arguments. Value **MUST** be `parameters.yaml`
5. **Required**. Where to store prepared data and labels. **MUST** be a folder
6. **Optional**. Where to store prepared labels. If not provided, labels should be stored with the prepared data.
7. **Required**. The sanity_check task verifies the quality of the transformed data. If the data does not pass quality check, the `sanity_check` task **MUST** throw an error.
8. **Required**. Where to find the prepared data. This is usually the output of the prepare task. **MUST** be a folder.
9. **Required**. Helper file to provide additional arguments. Value **MUST** be `parameters.yaml`
10. **Required**. The statistics task computes general statistics over the prepared data. This serves as a brief description of the data being prepared.
11. **Required**. Where to find the prepared data. This is usually the output of the `prepare` task. **MUST** be a folder.
12. **Required**. Helper file to provide additional arguments. Value **MUST** be `parameters.yaml`
13. **Required**. Where to store the statistics value. **MUST** be `statistics.yaml`

!!! note Separate labels output
    We provide an option to physically separate data and labels by defining the `output_labels_path` inside the `prepare` task. This is useful for challenges or benchmarks where model authors are not necessarily trusted. By doing this, the models won't be able to access any of the labels during the `infer` task.
---

### `mlcube/workspace/parameters.yaml`

   This file provides ways to parameterize the data preparation process. You can set any key-value pairs that should be easily modifiable to adjust your mlcube's behavior. This file is mandatory but can be left blank if parametrization is unnecessary, like in this example.

---

### `project`

   Contains the actual implementation of the mlcube, including all project-specific code, `Dockerfile` for building docker containers of the project, and requirements for running the code.

---
    
### `project/mlcube.py`
   
MLCube expects an entry point to the project to run the code and the specified tasks. It expects this entry point to behave like a CLI, in which each MLCube task (e.g., `prepare`) is executed as a subcommand – and each input/output parameter is passed as a CLI argument. 

``` bash
python3 project/mlcube.py prepare --data_path=<DATA_PATH>  --labels_path=<LABELS_PATH> --parameters_file=<PARAMETERS_FILE> --output_path=<OUTPUT_PATH>
```

`mlcube.py` provides an interface for this toy example. You can implement such a CLI interface with any language or tool as long as you follow the command structure demonstrated above. We provide an example that requires minimal modifications to the original project code by running any project task through subprocesses.

---

## How to modify
If you want to adjust this template for your own use case, then the following list serves as a step-by-step guide:

1. Remove demo artifacts from `/mlcube/workspace`: 
     - `/mlcube/workspace/data`
     - `/mlcube/workspace/labels`
     - `/mlcube/workspace/names`
     - `/mlcube/workspace/statistics.yaml`

2. Pass your original code to the `/project` folder, removing everything but `mlcube.py`.

3. Adjust your code and the `/project/mlcube.py` file so the commands point to the respective code and receive the expected arguments.

4. Modify `/project/requirements.txt` so that it contains all code dependencies for your project.

5. Default `/project/Dockerfile` should suffice. However, feel free to add/modify it to work with your needs as long as it has an entry point pointing to `mlcube.py`.

6. Inside `/mlcube/workspace`, add the input folders for preparing data.

7. Inside `/mlcube/workspace/additional_files`, add any files required for model execution (e.g., model weights).

8. In `/mlcube/mlcube.yaml`, make the following changes:
    1. Modify the metadata fields (e.g., `name`, `description`, `authors`, `image_name`) to the correct values.
    2. Set the `data_path`, `labels_path`, and other IO parameters to the appropriate locations within the `workspace` directory.
    3. **DO NOT** modify `parameters_file` in any way.
    4. Add any required parameters that point to additional_files (e.g., model_weights). These files should be contained inside additional_files, and the naming can be arbitrary.
    5. **DO NOT** modify the `output_path`s in any way.

!!! note "Requirements are negotiable"
    The fields required in the mlcube task interface are currently defined by the Medperf platform. We encourage users to raise any concerns or requests regarding these requirements while the platform is in alpha, as this is an ideal time to make changes. Please feel free to contact us with your feedback.
