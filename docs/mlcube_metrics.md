## Purpose:
Specifically, a metrics MLCube receives two inputs: the `predictions` made by the model and the `output` of a `data preparation cube` containing the proper labels for the data. When this text was written, metrics MLCubes were designed to have this single purpose.

## How to run:
This template was built so it could work out of the box. Follow the next steps:

1. Clone the repository
2. Navigate to the repository:

   ```bash
   cd mlcube_examples
   ```
3. Install mlcube and mlcube-docker:

   ```bash
   pip install mlcube mlcube-docker
   ```
4. Navigate to the current example's `mlcube` folder:

   ```bash
   cd medperf/metrics/mlcube
   ```
5. Run the `evaluate` task with mlcube
   ```bash
   mlcube run --task=infer -Pdocker.build_strategy=auto
   ```
6. View the results in the `results.yaml` file:
   ```bash
   cat workspace/results.yaml
   ```
That's it! You just built and ran a hello-world metrics mlcube!

## Contents

MLCubes usually share a similar folder and file structure. Here's a brief description of the role for the relevant files:

1. __`mlcube/mlcube.yaml`__: 
   
   The `mlcube.yaml` file in your project contains metadata about your model, including its interface. To use your model with MedPerf, the `mlcube.yaml` file must define an infer function that takes in at least two arguments: `data_path` and `parameters_file`, and produces prediction artifacts in the `output_path`. This definition can be found in the `mlcube.yaml` file as:

    ```yml
    tasks:
    # Metrics MLCubes require only a single task: `evaluate`
    # This task takes the predictions generated by the model mlcube (as a directory)
    evaluate:
       # Computes metrics using the predictions and labels provided
        # The metrics to compute are specified in the parameters file
        parameters:
        inputs: 
            predictions:
                type: directory
                description: Directory containing the model's predictions
                required: true
            labels:
                type: directory
                description: Directory containing the true labels for the data
                required: true
            parameters_file:
                type: file
                description: File containing additional arguments for the metric computation
                required: true
                default: "parameters.yaml" #value MUST be as in default]
            additional_files:
                type: directory
                description: Directory containing any additional files needed for the task
                required: false
        outputs: 
            output_path: 
                type: file
                description: File to store the computed metric results
                required: true
                default: "results.yaml" #value MUST be as in default
            
            
    ```



    The output generated by the metrics mlcube is expected to be a file named `results.yaml`, which contains the results of the computed metrics.

2. __`mlcube/workspace/parameters.yaml`__:

   This file provides ways to parameterize your model. You can set any key-value pairs that should be easily modifiable to adjust your model's behavior. The current example shows how we can specify the `metrics ` we want to compute for what labels, i.e., `label columns`. The column we use for identifying each true-label/prediction, i.e., `id column`:

   ```yml
        # File for parametrizing your metrics calculations

        metrics:
        # List of metrics to run
        - ACC

        label columns:
        # Label columns that are going to be evaluated
        - greeting

        # Common identifier column for labels and predictions
        id column: id
   ```

   This structure follows how we've been specifying metrics parametrization. Your metrics don't need to follow this parameters structure.

3. __`mlcube/workspace/additional_files/*`__:
   
   Due to size or usability constraints, you may require additional files that should not be packaged inside the mlcube, like weights. For these cases, we provide an additional folder called `additional_files`. 
Here, you can provide any other files that should be present during inference. At the time of mlcube registration, this folder must be compressed into a tarball `.tar.gz` and hosted somewhere on the web. 

MedPerf will then be able to download, verify and reposition those files in the expected location for mlcube execution. 



4. __`project`__: 
   
       Contains the actual implementation of the mlcube, including all project-specific code, `Dockerfile` for building docker containers of the project, and equirements for running the code.

5. __`project/mlcube.py`__:
   
   MLCube expects an entry point to the project to run the code and the specified tasks. It expects this entry point to behave like a CLI, in which each MLCube task (e.g., `evaluate`) is executed as a subcommand – and each input/output parameter is passed as a CLI argument. 

   An example of the expected interface is:

   ```bash
    python3 project/mlcube.py evaluate --predictions=<PREDICTIONS_PATH>  --labels=<LABELS_PATH> --parameters_file=<PARAMETERS_FILE> --output_path=<OUTPUT_PATH>
   ```

   `mlcube.py` provides an interface for this toy example. You can implement such a CLI interface as long as you follow it. We provide an example that requires minimal modifications to the original project code by running any project task through subprocesses.

   #### __What is that “hotfix” function I see in mlcube.py?__

   To summarize, this issue is benign and can be safely ignored. It prevents a potential issue with the CLI and does not require further action. If you use the `typer/click ` library for your command-line interface (CLI) and have only one @app.command, the command line may not be parsed as expected by mlcube. This is due to a known issue that can be resolved by adding more than one task to the mlcube interface. To avoid a potential issue with the CLI, we add a dummy typer command to our model cubes that only have one task. If you're not using typer/click, you don't need this dummy command.

## How to modify

If you want to adjust this template for your own use case, then the following list serves as a step-by-step guide:

1. Remove the demo artifacts from the `/mlcube/workspace` folder:
     - `/mlcube/workspace/labels/*`
     - `/mlcube/workspace/predictions/*`
     - `/mlcube/workspace/`

2. Place your original code in the `/project folder`, removing `app.py`.

3. Adjust your code and the `/project/mlcube.py` file, so the commands point to the correct code and receive the expected arguments.
4. Modify ` /project/requirements.txt` to include all code dependencies for your project.
5. The default `/project/Dockerfile` should be sufficient, but you can modify it to meet your needs as long as it has an entry point pointing to `mlcube.py`.
6. Inside `/mlcube/workspace`, add the data you want your model to use for inference.
7. Inside `/mlcube/workspace/additional_files`, add any files required for model execution (e.g., `model weights`).

8. In `/mlcube/mlcube.yaml`, make the following changes:

   1. Assign the correct values to the metadata fields (`name`, `description`, `authors`, `image_name`).
   2. Set `labels` to the location of the labels inside the workspace directory.

   3. Set `predictions` to where you expect predictions to be inside the `workspace` directory.
   4. Do NOT modify `parameters_file`.
   5. Add any other required parameters that point to `additional_files`. The naming can be arbitrary, but all referenced files should be contained inside `additional_files`.
   6. Do NOT modify `output_path`.

## Requirements are negotiable
The fields required in the mlcube task interface are currently defined by the Medperf platform. We encourage users to raise any concerns or requests regarding these requirements while the platform is in alpha, as this is an ideal time to make changes. Please feel free to contact us with your feedback.
