---
name: Data Preparator MLCube
url: https://github.com/mlcommons/medperf/examples/HelloWorld/data_preparator
slug: data_preparator
---
# Data Preparator MLCube
## Purpose
Data Preparators are in charge of standardizing the input data format models expect to receive. Additionally, they provide tools for testing the integrity of the data and extracting valuable insights from it.

{% include "mlcubes/shared/hello_world.md" %}

{% include "mlcubes/shared/setup.md" %}

### Running the Prepare Task
The first step that should be executed with a Data Preparator MLCube is the `prepare` task. This task ingests raw data, and makes preliminary transformations to ensure it is ready to be ingested by the Model MLCube later on.

1. Let's first observe the raw data for the Hello World example
   ```bash
   cat workspace/names/names.txt
   ```
   ```title="names/names.txt"
   --8<-- "examples/HelloWorld/data_preparator/mlcube/workspace/names/names.txt"
   ```
   Here we see the names that are expected to be used by the model to generate greetings from.
   ```bash
   cat workspace/labels/labels.csv
   ```
   ```title="labels/labels.csv"
   --8<-- "examples/HelloWorld/data_preparator/mlcube/workspace/labels/labels.csv"
   ```
   Additionally, we have the labels for this task. In this case, labels represent what we expect to be generated by the models for this problem.

2. Run the `prepare` task with the mlcube
    ```bash
    mlcube run --task=prepare
    ```
    Running this task will execute the HelloWorld MLCube with the default values as seen in the corresponding [`mlcube.yaml`](#mlcubemlcubeyaml). You can also define different values for the parameters in the following manner
    ```bash
    mlcube run --task=prepare data_path=names
    ```

3. Check the resulting data using
    ```bash
    cat workspace/data/names.csv
    ```
    ```title="data/names.csv"
    --8<-- "examples/HelloWorld/data_preparator/mlcube/workspace/data/names.csv"
    ```
    Here we see that the Data Preparator has transformed the raw names into a `csv` file, which no longer includes the full name, but only the first nad last name. This is the kind of transformations that a Data Preparator MLCube can do. By providing logic for handling raw data, we allow model authors to focus in the model design, and also ideally standardize the data even if it comes from different sources.

### Running the Sanity Check Task
Now that we have executed the logic to ingest and transform the raw data into usable data, we need to ensure that these transformations have gone well. For that, a Data Preparator contains the `sanity_check` task. This task ingests the prepared dataset, and computes assertions and quality checks to ensure everything looks as expected.

1. Run the `sanity_check` task with
     ```bash
     mlcube run --task=sanity_check
     ```

If everything went well, you shouldn't see anything happen after executing this task. This is because **The `sanity_check` task is only expected to fail and exit if there's an anomaly with the data**. Else, the `sanity_check` task will complete successfully. You can try this out too! We have provided an invalid version of the prepared data

2. Check the contents of the corrupted prepared data:
   ```bash
   cat workspace/invalid_data/names.csv
   ```
   ```title="invalid_data/names.csv"
   --8<-- "examples/HelloWorld/data_preparator/mlcube/workspace/invalid_data/names.csv"
   ```

   You may notice this version now contains an unexpected column. This is something the `sanity_check` task doesn't expect, and it should react accordingly

3. Run the `sanity_check` task with the invalid data
   ```bash
   mlcube run --task=sanity_check data_path=invalid_data
   ```

   You should see the command failing. This is exactly what we expect from the Sanity Check Task.

### Running the Statistics Task
Lastly, the data preparator contains a `statistics` task for obtaining and sharing insights on the prepared dataset. These insights are to be shared under the approval of the Data Owner, and as such they should not disclose private data, but instead general and aggregated statistics.

1. Run the `statistics` task using
    ```bash
    mlcube run --task=statistics
    ```

By running the `statistics` task, a new file is created (`workspace/statistics.yaml`). This file contains the computed statistics.

2.  Check the resulting statistics using the following command:
    ```bash
    cat workspace/statistics.yaml
    ```
    ```title="statistics.yaml"
    --8<-- "examples/HelloWorld/data_preparator/mlcube/workspace/statistics.yaml"
    ```
    Here we can see some simple statistics computed on the `names.csv` file we inspected before. These statistics may serve Benchmark and Model Owners to understand the nature and the challenges that this specific dataset posseses. 

That's it! You just built and ran a hello-world data preparator mlcube! Now it's time to build one from scratch!

{% include "mlcubes/shared/build.md" %}

    ```bash
    $ medperf mlcube create data_preparator                      
    MedPerf 0.0.0
    project_name [Data Preparator MLCube]: Hello World Data Preparator MLCube # (1)!
    project_slug [hello_world_data_preparator_mlcube]: data_preparator_mlcube # (2)!
    description [Data Preparator MLCube Template. Provided by MLCommons]: Hello World Data Preparator implementation from scratch # (3)!
    author_name [John Smith]: John Smith # (4)!
    accelerator_count [0]: 0 # (5)!
    docker_image_name [docker/image:latest]: johnsmith/hello_world_dataprep:0.0.1 # (6)!
    use_separate_output_labels [n]: n # (7)!
    ```
    1. Gives a Human-readable name to the MLCube Project.
    2. Determines how the Data Preparator root folder will be named.
    3. Gives a Human-readable description to the MLCube Project.
    4. Documents the MLCube implementation by specifying the author. Please use your own name here.
    5. Indicates how many GPUs should be visible by the MLCube. Useful for Model MLCubes.
    6. MLCubes use containers under the hood. Medperf supports both Docker and Singularity. Here, you can provide an image tag to the image that will be created by this MLCube. **It's recommended to use a naming convention that allows you to upload it to Docker Hub.**
    7. Data Preparators can output the labels in a separate location to the data. This is useful in situations where you can't trust Model Owners to access the labels during inference (For example during challenges). For this example, we will output data and labels on the same folder

{% include "mlcubes/shared/cookiecutter.md" %}

After running the previous command, you should be able to see a folder created, named after the `project_slug` provided during configuration.

### Contents
Let's have a look at what the previous command generated. First, lets look at the whole folder structure:
```bash
tree 
```
```bash
.
└── data_preparator_mlcube
    ├── mlcube
    │   ├── mlcube.yaml # (1)!
    │   └── workspace # (2)!
    │       ├── data # (3)!
    │       ├── input_data # (4)!
    │       ├── input_labels # (5)!
    │       └── parameters.yaml # (6)!
    └── project # (7)!
        ├── Dockerfile # (8)!
        └── mlcube.py # (9)!

7 directories, 4 files
```

1. The `mlcube.yaml` file contains metadata about your data preparation procedure, including its interface. For MedPerf, we require three tasks: `prepare`, `sanity_check`, and `statistics`.
2. The `workspace` contains all the files and paths that can be used by the MLCube, as long as those paths are specified inside the `mlcube.yaml`.
3. The `data` folder is where the prepared data will be contained after running the `prepare` task.
4. The `input_data` is where the MLCube will look for raw data by default.
5. The `input_labels` is where the MLCube will look for labels by default.
6. This file provides ways to parameterize the data preparation process. You can set any key-value pairs that should be easily modifiable to adjust your mlcube's behavior. This file is mandatory but can be left blank if parametrization is unnecessary, like in this example.
7. Contains the actual implementation code of the mlcube.
8. A default `Dockerfile` which by default installs `python3.6` and any requirements for this MLCube to work.
9. `mlcube.py` provides a bare-bones command-line interface for a Data Preparator MLcube to run. The logic inside each command is intentionally left blank.

Let's go through each of the created files and modify them as needed to create a Hello World {{ page.meta.name }}.

#### `mlcube/mlcube.yaml`
The `mlcube.yaml` file contains metadata about the data preparation procedure. 

```yaml title="mlcube.yaml"
name: Hello World Data Preparator MLCube # (1)!
description: Hello World Data Preparator implementation from scratch # (2)!
authors:
 - {name: John Smith} # (3)!

platform:
  accelerator_count: 0 # (4)!

docker:
  # Image name
  image: johnsmith/hello_world_dataprep:0.0.1 # (5)!
  # Docker build context relative to $MLCUBE_ROOT. Default is `build`.
  build_context: "../project"
  # Docker file name within docker build context, default is `Dockerfile`.
  build_file: "Dockerfile"

tasks:
  prepare: # (6)!
    parameters:
      inputs: {
        data_path: input_data, # (7)!
        labels_path: input_labels, # (8)!
        parameters_file: parameters.yaml # (9)!
      }
      outputs: {
        output_path: data/, # (10)!
        # output_labels_path: labels/ # (11)!
      }
  sanity_check: # (12)!
    parameters:
      inputs: {
        data_path: data/,
        parameters_file: parameters.yaml
      }
  statistics: # (13)!
    parameters:
      inputs: {
        data_path: data/,
        parameters_file: parameters.yaml
      }
      outputs: {
        output_path: {type: file, default: statistics.yaml} # (14)!
      }
```

1. This is the name we gave during the configuration procedure.
2. This is the description we provided before.
3. Here you can see a list of authors. It is populated with the given author during setup.
4. The accelerator count defines the number of GPUS that should be visible by the MLCube. This was filled during setup.
5. This is the docker image name we provided before.
6. The prepare task transforms the input data and labels into the format expected by model cubes
7. **Required**. Where to find the input raw data. **MUST** be a folder.
8. **Required**. Where to find the input labels. **MUST** be a folder.
9. **Required**. Helper file to provide additional arguments. Value **MUST** be `parameters.yaml`
10. **Required**. Where to store prepared data and labels. **MUST** be a folder
11. If we had answered yes when asked for separated output labels, this would not be commented.
12. **Required**. The sanity_check task verifies the quality of the transformed data. If the data does not pass quality check, the `sanity_check` task **MUST** throw an error.
13. **Required**. The statistics task computes general statistics over the prepared data. This serves as a brief description of the data being prepared.
14. **Required**. Where to store the statistics value. **MUST** be `statistics.yaml`

In most cases, the `mlcube.yaml` file provided by our template will suffice. For now, let's leave it as is and continue with the other files.

---

#### `mlcube/workspace/parameters.yaml`

This file provides ways to parameterize the data preparation process. You can set any key-value pairs that should be easily modifiable to adjust your mlcube's behavior. This file is mandatory but can be left blank if parametrization is unnecessary, like in this example. We will see how to work with this file in the Model MLCube.

---

#### `project/mlcube.py`
   
MLCube expects an entry point to the project to run each specified tasks. It expects this entry point to behave like a CLI, in which each MLCube task (e.g., `prepare`) is executed as a subcommand – and each input/output parameter is passed as a CLI argument. 

``` bash
python3 project/mlcube.py prepare --data_path=<DATA_PATH>  --labels_path=<LABELS_PATH> --parameters_file=<PARAMETERS_FILE> --output_path=<OUTPUT_PATH>
```

!!! note 
    You can implement such a CLI interface with any language or tool as long as you follow the command structure demonstrated above.

```python title="mlcube.py"
"""MLCube handler file"""
import typer


app = typer.Typer()


@app.command("prepare")
def prepare(
    data_path: str = typer.Option(..., "--data_path"),
    labels_path: str = typer.Option(..., "--labels_path"),
    parameters_file: str = typer.Option(..., "--parameters_file"),
    output_path: str = typer.Option(..., "--output_path"),
    
):
    # Modify the prepare command as needed
    raise NotImplementedError("The prepare method is not yet implemented")


@app.command("sanity_check")
def sanity_check(
    data_path: str = typer.Option(..., "--data_path"),
    parameters_file: str = typer.Option(..., "--parameters_file"),
):
    # Modify the sanity_check command as needed
    raise NotImplementedError("The sanity check method is not yet implemented")


@app.command("statistics")
def sanity_check(
    data_path: str = typer.Option(..., "--data_path"),
    parameters_file: str = typer.Option(..., "--parameters_file"),
    out_path: str = typer.Option(..., "--output_path"),
):
    # Modify the statistics command as needed
    raise NotImplementedError("The statistics method is not yet implemented")


if __name__ == "__main__":
    app()
```

The `mlcube.py` generated by the template will already define the CLI interface, but no implementation of each task is provided. This must be filled by us. Let's do a quick implementation for each of the tasks. If you want to see a more detailed implementation, check the [Hello World Data Preparator MLCube](https://github.com/mlcommons/medperf/examples/HelloWorld/data_preparator) we executed previously.

---

##### Implement the Prepare task
The Prepare task should take paths for data and labels, and output a transformed and standardized version of the raw data. The Prepare task defines how it expects to consume the raw data. For real medical tasks, it should ideally provide methods for handling different data formats and standards for the same task.

For this example, we want to consume `.txt` names and `.csv` labels. The output data should be a `.csv` file that contains only the `First Name` and `Last Name` of the subject. To make it simple, we will assume the raw data contains the name in the following format `{first_name} {last_name}`. As for the labels, we assume they are already well formatted, so we only need to place it in the output path. Here's a short implementation of the `prepare` task given the previous assumptions.

```python
import os # (1)!
import shutil

@app.command("prepare") 
def prepare(
    data_path: str = typer.Option(..., "--data_path"),
    labels_path: str = typer.Option(..., "--labels_path"),
    parameters_file: str = typer.Option(..., "--parameters_file"),
    output_path: str = typer.Option(..., "--output_path"),
):
    input_data_file = os.path.join(data_path, "names.txt") # (2)!
    input_labels_file = os.path.join(labels_path, "labels.csv") # (3)!
    output_data_file = os.path.join(output_path, "names.csv") # (4)!
    output_labels_file = os.path.join(output_path, "labels.csv") # (5)!

    # Create the data.csv file
    with open(output_data_file, "w") as output_data:
        output_data.write("First Name,Last Name\n")
        with open(input_data_file, "r") as input_data:
            for line in input_data:
                first_name, last_name = line.split() # (6)!
                output_data.write(f"{first_name},{last_name}\n")
    
    # Copy the labels to its corresponding output file
    shutil.copyfile(input_labels_file, output_labels_file) # (7)!
```

1. The necessary imports for this function are provided here, but please place them at the top of your file.
2. Here we're specifying how the input data is expected to look like. For this example we expect all data to contain a `names.txt` file, but for real-world tasks, this should be more robust, and support many different ways of handling data.
3. Here we're specifying how the labels are expected to look like. In some cases, labels are obtained automatically from the data. In such cases, the labels path can be left unused on the implementation. A labels path will still need to be passed by Data Owners, so they must be informed of these decisions.
4. Here we're specifying how the output data will look like. This is left to the MLCube author to decide what's the best way to store the data for Model MLCubes to ingest. 
5. Here we're specifying how the labels will look like. The labels are to be handled later on by the Evaluator MLCube.
6. We assume the input data only contains names in the format of `{first_name} {last_name}`.
7. We assume the labels are already well formatted.

---

##### Implement the Sanity Check task
The `sanity_check` task ensures the prepared data looks as expected. It should check for anomalies, like values outside the expected range, `nan`s, and other undesired outcomes that could occur during the preparation step. **Running this command should raise an error and exit if any of the tests fail.**

For this example we can check that the columns are named appropiately, and that no field is empty.
```python
import os # (1)!
import pandas

@app.command("sanity_check")
def sanity_check(
    data_path: str = typer.Option(..., "--data_path"), # (2)!
    parameters_file: str = typer.Option(..., "--parameters_file"),
):
    data_file = os.path.join(data_path, "names.csv")
    data = pandas.read_csv(data_file)
    assert data.columns.tolist() == ["First Name", "Last Name"], "Column mismatch"
    assert data["First Name"].isna().sum() == 0, "There are empty fields"
    assert data["Last Name"].isna().sum() == 0, "There are empty fields"
```

1. The necessary imports for this function are provided here, but please place them at the top of your file.
2. You may notice only the `data_path` is being used here. We still need to provide the other arguments so the command signature matches the expected signature by MLCube.
3. Using `assert` is a very natural way to implement the sanity check. If the assert is False, the program raises an `AssertionError` and exits.

This provides a vary basic sanity check for our prepared data. For real-world tasks, checks should be more robust and comprehensive.

---

##### Implement the Statistics task
The `statistics` task computes general statistics from the prepared data. This is meant to inform other users about the nature of the data, and as such it should not expose private information. For our example, we will compute statistics about the length of the data. Those statistics should be stored in YAML format at the provided output file.

```python
import os # (1)!
import pandas
import yaml

@app.command("statistics")
def sanity_check(
    data_path: str = typer.Option(..., "--data_path"),
    parameters_file: str = typer.Option(..., "--parameters_file"),
    out_path: str = typer.Option(..., "--output_path"),
):
    data_file = os.path.join(data_path, "names.csv")
    data = pandas.read_csv(data_file)

    fname_len = data["First Name"].str.len()
    lname_len = data["Last Name"].str.len()

    stats = { # (2)!
        "First Name": {
            "length mean": float(fname_len.mean()),
            "length min": int(fname_len.min()),
            "length max": int(fname_len.max())
        },
        "Last Name": {
            "length mean": float(lname_len.mean()),
            "length min": int(lname_len.min()),
            "length max": int(lname_len.max())
        },
        "size": len(data)
    }

    with open(out_path, "w") as f: # (3)!
        yaml.dump(stats, f)
```

1. The necessary imports for this function are provided here, but please place them at the top of your file.
2. The computed statistics are stored as a dictionary. The structure can be arbitrary
3. This is one of the few cases in which a MedPerf MLCube parameter points diretly to a file instead of a folder. This is true for all statistics tasks across all Data Preparator MLCubes.

---
##### The complete implementation
Here you can find the whole `mlcube.py` file after all the implementation changes we made:

```python title="project/mlcube.py"
"""MLCube handler file"""
import typer
import os
import shutil
import pandas
import yaml


app = typer.Typer()


@app.command("prepare")
def prepare(
    data_path: str = typer.Option(..., "--data_path"),
    labels_path: str = typer.Option(..., "--labels_path"),
    parameters_file: str = typer.Option(..., "--parameters_file"),
    output_path: str = typer.Option(..., "--output_path"),
):
    input_data_file = os.path.join(data_path, "names.txt")
    input_labels_file = os.path.join(labels_path, "labels.csv")
    output_data_file = os.path.join(output_path, "names.csv")
    output_labels_file = os.path.join(output_path, "labels.csv")

    # Create the data.csv file
    with open(output_data_file, "w") as output_data:
        output_data.write("First Name,Last Name\n")
        with open(input_data_file, "r") as input_data:
            for line in input_data:
                first_name, last_name = line.split()
                output_data.write(f"{first_name},{last_name}\n")

    # Copy the labels to its corresponding output file
    shutil.copyfile(input_labels_file, output_labels_file)


@app.command("sanity_check")
def sanity_check(
    data_path: str = typer.Option(..., "--data_path"),
    parameters_file: str = typer.Option(..., "--parameters_file"),
):
    data_file = os.path.join(data_path, "names.csv")
    data = pandas.read_csv(data_file)
    assert data.columns.tolist() == ["First Name", "Last Name"], "Column mismatch"
    assert data["First Name"].isna().sum() == 0, "There are empty fields"
    assert data["Last Name"].isna().sum() == 0, "There are empty fields"


@app.command("statistics")
def sanity_check(
    data_path: str = typer.Option(..., "--data_path"),
    parameters_file: str = typer.Option(..., "--parameters_file"),
    out_path: str = typer.Option(..., "--output_path"),
):
    data_file = os.path.join(data_path, "names.csv")
    data = pandas.read_csv(data_file)

    fname_len = data["First Name"].str.len()
    lname_len = data["Last Name"].str.len()

    stats = {
        "First Name": {
            "length mean": float(fname_len.mean()),
            "length min": int(fname_len.min()),
            "length max": int(fname_len.max()),
        },
        "Last Name": {
            "length mean": float(lname_len.mean()),
            "length min": int(lname_len.min()),
            "length max": int(lname_len.max()),
        },
        "size": len(data),
    }

    with open(out_path, "w") as f:
        yaml.dump(stats, f)


if __name__ == "__main__":
    app()
```

---

{% include "mlcubes/shared/docker_file.md" %}

---

{% include "mlcubes/shared/requirements.md" %}

```python title="project/requirements.txt"
# Add your dependencies here
typer
PyYAML
pandas
```

---
That's it! With that we should have a functional Data Preparator MLCube, that has been implemented for our Hello World Task. Congratulations!

{% include "mlcubes/shared/execute.md" %}

3. Create input data
    We need to provide input data to our dataset. In this case, we can easily create sample data for our task. Assuming you're in the `tutorial/data_preparator_mlcube/mlcube` path, you can run:
    ```bash
    echo "John Smith" >> workspace/input_data/names.txt
    echo "id,greeting\n0,\"Hello, John smith\"\n1,\"Howdy, John Smith\"\n2,\"Greetings, John Smith\"\n3,\"Bonjour, John Smith\"" >> workspace/input_labels/labels.csv
    ```
    This will create the raw data and labels files, you can inspect them with
    ```bash
    cat workspace/input_data/names.txt
    cat workspace/input_labels/labels.csv
    ```
    ```bash title="workspace/input_data/names.txt"
    John Smith
    ```
    ```bash title="workspace/input_labels/labels.csv"
    id,greeting
    0,"Hello, John smith"
    1,"Howdy, John Smith"
    2,"Greetings, John Smith"
    3,"Bonjour, John Smith"
    ```
4. Run the pipeline
    You should now be able to run the whole pipeline as we did before for our example! If you want details of what is going on at each step, look for the [How to run](#how-to-run) section.
    ```bash
    mlcube run --task=prepare
    mlcube run --task=sanity_check
    mlcube run --task=statistics
    ```
That's it! You should now be able to write your own Data Preparator MLCubes from scratch!