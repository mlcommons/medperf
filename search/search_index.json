{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"cli_reference/","title":"In Progress","text":"<p>TODO: the page is hidden now. If implemented, find all usages and uncomment them.</p>"},{"location":"medperf_components/","title":"MedPerf Components","text":""},{"location":"medperf_components/#medperf-server","title":"MedPerf Server","text":"<p>The server contains all the metadata necessary to coordinate and execute experiments. No code assets or datasets are stored on the server.</p> <p>The backend server is implemented in Django, and it can be found in the server folder in the MedPerf Github repository.</p>"},{"location":"medperf_components/#medperf-client","title":"MedPerf Client","text":"<p>The MedPerf client contains all the necessary tools to interact with the server, preparing datasets for benchmarks and running experiments on the local machine. It can be found in this folder in the MedPerf Github repository.</p> <p>The client communicates to the server through the API to, for example, authenticate a user, retrieve benchmarks/containers and send results.</p> <p>The client is currently available to the user through a command-line interface (CLI). </p>"},{"location":"medperf_components/#auth-provider","title":"Auth Provider","text":"<p>The auth provider manages MedPerf users identities, authentication, and authorization to access the MedPerf server. Users will authenticate with the auth provider and authorize their MedPerf client to access the MedPerf server. Upon authorization, the MedPerf client will use access tokens issued by the auth provider in every request to the MedPerf server. The MedPerf server is configured to processes only requests authorized by the auth provider.</p> <p>Currently, MedPerf uses Auth0 as the auth provider.</p>"},{"location":"roles/","title":"User Roles and Responsibilities","text":"<p>Here we introduce user roles at MedPerf. Depending on the objectives and expectations a user may have multiple roles.</p>"},{"location":"roles/#benchmark-committee","title":"Benchmark Committee","text":"<p>May include healthcare stakeholders (e.g., hospitals, clinicians, patient advocacy groups, payors, etc.), regulatory bodies, data providers and model owners wishing to drive the evaluation of AI models on real world data. While the Benchmark Committee does not have admin privileges on MedPerf, they have elevated permissions regarding benchmark assets (e.g., task, evaluation metrics, etc.) and policies (e.g., participation of model owners, data providers, anonymizations)</p> <p></p>"},{"location":"roles/#data-providers","title":"Data Providers","text":"<p>May include hospitals, medical practices, research organizations, and healthcare payors that own medical data, register medical data, and execute benchmarks.</p> <p></p>"},{"location":"roles/#model-owners","title":"Model Owners","text":"<p>May include ML researchers and software vendors that own a trained medical ML model and want to evaluate its performance against a benchmark.</p> <p></p>"},{"location":"roles/#platform-providers","title":"Platform Providers","text":"<p>Organizations like MLCommons that operate the MedPerf platform enabling benchmark committees to develop and run benchmarks.</p> <p></p>"},{"location":"what_is_medperf/","title":"What is Medperf?","text":"<p>MedPerf is an open-source framework for benchmarking medical ML models. It uses Federated Evaluation a method in which medical ML models are securely distributed to multiple global facilities for evaluation prioritizing patient privacy to mitigate legal and regulatory risks. The goal of Federated Evaluation is to make it simple and reliable to share ML models with many data providers, evaluate those ML models against their data in controlled settings, then aggregate and analyze the findings. </p> <p>The MedPerf approach empowers healthcare stakeholders through neutral governance to assess and verify the performance of ML models in an efficient and human-supervised process without sharing any patient data across facilities during the process.</p> Federated evaluation of medical AI model using MedPerf on a hypothetical example"},{"location":"what_is_medperf/#why-medperf","title":"Why MedPerf?","text":"<p>MedPerf aims to identify bias and generalizability issues of medical ML models by evaluating them on diverse medical data across the world. This process allows developers of medical ML to efficiently identify performance and reliability issues on their models while healthcare stakeholders (e.g., hospitals, practices, etc.) can validate such models against clinical efficacy.</p> <p>Importantly, MedPerf supports technology for neutral governance in order to enable full trust and transparency among participating parties (e.g., AI vendor, data provider, regulatory body, etc.). This is all encapsulated in the benchmark committee which is the overseeing body on a benchmark.</p> Benchmark committee in MedPerf"},{"location":"what_is_medperf/#benefits-to-healthcare-stakeholders","title":"Benefits to healthcare stakeholders","text":"<p>Anyone who joins our platform can get several benefits, regardless of the role they will assume.</p> Benefits to healthacare stakeholders using MedPerf <p>Our paper describes the design philosophy in detail.</p>"},{"location":"workflow/","title":"Benchmark Workflow","text":"<p>A benchmark in MedPerf is a collection of assets that are developed by the benchmark committee that aims to evaluate medical ML on decentralized data providers.</p> <p>The process is simple yet effective enabling scalability.</p>"},{"location":"workflow/#step-1-establish-benchmark-committee","title":"Step 1. Establish Benchmark Committee","text":"<p>The benchmarking process starts with establishing a benchmark committee of healthcare stakeholders (experts, committee), which will identify a clinical problem where an effective ML-based solution can have a significant clinical impact.</p>"},{"location":"workflow/#step-2-register-benchmark","title":"Step 2. Register Benchmark","text":"<p>A benchmark workflow is defined by three containers. A Data Preparator container, a Reference Model container, and a Metrics container, need to be submitted in order to define a benchmark workflow. After submitting the three containers, alongside with a sample reference dataset, the Benchmark Committee is capable of creating a benchmark. Once the benchmark is submitted, the Medperf admin must approve it before it can be seen by other users. Follow our Hands-on Tutorial for detailed step-by-step guidelines.</p>"},{"location":"workflow/#step-3-register-dataset","title":"Step 3. Register Dataset","text":"<p>Data Providers that want to be part of the benchmark can register their own datasets, prepare them, and associate them with the benchmark. A dataset will be prepared using the benchmark's Data Preparator container and the dataset's metadata is registered within the MedPerf server.</p> Data Preparation <p>The data provider then can request to participate in the benchmark with their dataset. Requesting the association will run the benchmark's reference workflow to assure the compatibility of the prepared dataset structure with the workflow. Once the association request is approved by the Benchmark Committee, then the dataset becomes a part of the benchmark.</p> <p></p>"},{"location":"workflow/#step-4-register-models","title":"Step 4. Register Models","text":"<p>Once a benchmark is submitted by the Benchmark Committee, any user can submit their own Model containers and request an association with the benchmark. This association request executes the benchmark locally with the given model on the benchmark's reference dataset to ensure workflow validity and compatibility. If the model successfully passes the compatibility test, and its association is approved by the Benchmark Committee, it becomes a part of the benchmark.</p> <p></p>"},{"location":"workflow/#step-5-execute-benchmark","title":"Step 5. Execute Benchmark","text":"<p>The Benchmark Committee may notify Data Providers that models are available for benchmarking. Data Providers can then run the benchmark models locally on their data.</p> <p>This procedure retrieves the model containers associated with the benchmark and runs them on the indicated prepared dataset to generate predictions. The Metrics container of the benchmark is then retrieved to evaluate the predictions. Once the evaluation results are generated, the data provider can submit them to the platform.</p> <p></p>"},{"location":"workflow/#step-6-aggregate-and-release-results","title":"Step 6. Aggregate and Release Results","text":"<p>The benchmarking platform aggregates the results of running the models against the datasets and shares them according to the Benchmark Committee's policy.</p> <p>The sharing policy controls how much of the data is shared, ranging from a single aggregated metric to a more detailed model-data cross product. A public leaderboard is available to Model Owners who produce the best performances.</p>"},{"location":"concepts/associations/","title":"In Progress","text":"<p>TODO: the page is hidden now. If implemented, find all usages and uncomment them.</p>"},{"location":"concepts/auth/","title":"Authentication","text":"<p>This guide helps you learn how to login and logout using the MedPerf client to access the main production MedPerf server. MedPerf uses passwordless authentication. This means that login will only require you to access your email in order complete the login process.</p>"},{"location":"concepts/auth/#login","title":"Login","text":"<p>Follow the steps below to login:</p> <ul> <li>Step1 Run the following command:</li> </ul> <pre><code>medperf auth login\n</code></pre> <p>You will be prompted to enter your email address.</p> <p>After entering your email address, you will be provided with a verification URL and a code. A text similar to the following will be printed in your terminal:</p> <p></p> <p>Tip</p> <p>If you are running the MedPerf client on a machine with no graphical interface, you can use the link on any other device, e.g. your cellphone. Make sure that you trust that device.</p> <ul> <li>Step2 Open the verification URL and confirm the code:</li> </ul> <p>Open the printed URL in your browser. You will be presented with a code, and you will be asked to confirm if that code is the same one printed in your terminal.</p> <p></p> <ul> <li>Step3 After confirmation, you will be asked to enter your email address. Enter your email address and press \"Continue\". You will see the following screen:</li> </ul> <p></p> <ul> <li>Step4 Check your inbox. You should receive an email similar to the following:</li> </ul> <p></p> <p>Enter the received code in the previous screen.</p> <ul> <li>Step5 If there is no problem with your account, the login will be successful, and you will see a screen similar to the following:</li> </ul> <p></p>"},{"location":"concepts/auth/#logout","title":"Logout","text":"<p>To disconnect the MedPerf client, simply run the following command:</p> <pre><code>medperf auth logout\n</code></pre>"},{"location":"concepts/auth/#checking-the-authentication-status","title":"Checking the authentication status","text":"<p>Note that when you log in, the MedPerf client will remember you as long as you are using the same <code>profile</code>. If you switch to another profile by running <code>medperf profile activate &lt;other-profile&gt;</code>, you may have to log in again. If you switch back again to a profile where you previously logged in, your login state will be restored.</p> <p>You can always check the current login status by the running the following command:</p> <pre><code>medperf auth status\n</code></pre>"},{"location":"concepts/container_assets/","title":"Container Assets: What to Host?","text":"<p>Once you have built a container ready for MedPerf, you need to host its assets somewhere on the cloud so that it can be identified and retrieved by the MedPerf client on other machines. This requires hosting the container components somewhere on the cloud. The following is a description of what needs to be hosted.</p>"},{"location":"concepts/container_assets/#hosting-your-container-image","title":"Hosting Your Container Image","text":"<p>The container image should be hosted in a container registry, like Docker Hub. For singularity, medperf provides the option of passing the URL of a .sif image file directly (i.e. having the image file hosted somewhere and providing MedPerf with the download link).</p> <p>Note</p> <p>While there is the option of hosting the singularity image directly, it is highly recommended to use a container registry for accessability and usability purposes. MedPerf also has mechanisms for converting containers for other container runners, like Docker to Singularity.</p> <p>Note</p> <p>Docker Images can be on any docker container registry, not necessarily on Docker Hub.</p>"},{"location":"concepts/container_assets/#files-to-be-hosted","title":"Files to be hosted","text":"<p>The following is the list of files that must be hosted separately so they can be used by MedPerf:</p>"},{"location":"concepts/container_assets/#container_configyaml","title":"<code>container_config.yaml</code>","text":"<p>Every container is defined by its <code>container_config.yaml</code> manifest file. As such, Medperf needs to have access to this file to use the container.</p>"},{"location":"concepts/container_assets/#parametersyaml-optional","title":"<code>parameters.yaml</code> (Optional)","text":"<p>If your container depends on a parameters file (see here), the <code>parameters.yaml</code> file needs to be hosted as well and its URL should be provided to the container submission command.</p>"},{"location":"concepts/container_assets/#additional_filestargz-optional","title":"<code>additional_files.tar.gz</code> (Optional)","text":"<p>If your container depends on additional files (see here), the <code>additional_files.tar.gz</code> file needs to be hosted as well and its URL should be provided to the container submission command. Here is how this file should be compressed in order for MedPerf to be able to decompress it and mount it to your container during runtime:</p> <p>Suppose you have your additional files inside a folder named <code>additional_files</code>. Then, compress this folder using the following command:</p> <pre><code>tar -czf additional_files.tar.gz -C &lt;path_to_the_additional_files_folder&gt; .\n</code></pre> <p>This will create the <code>additional_files.tar.gz</code> archive that can then be hosted and its URL should be provided to the container submission command.</p>"},{"location":"concepts/container_assets/#see-also","title":"See Also","text":"<ul> <li>File Hosting</li> </ul>"},{"location":"concepts/hosting_files/","title":"Hosting Files","text":"<p>MedPerf requires some assets to be hosted on the cloud when running machine learning pipelines. Submitting Containers to the MedPerf server means submitting their metadata, and not, for example, model weights or parameters files. Container assets such as model weights need to be hosted on the cloud, and the submitted container metadata will only contain URLs (or certain identifiers) for these assets. Another example would be benchmark submission, where demo datasets need to be hosted.</p> <p>The MedPerf client expects assets to be hosted in certain ways. Below are options of how assets can be hosted and how MedPerf identitfies them (e.g. a URL).</p>"},{"location":"concepts/hosting_files/#file-hosting","title":"File hosting","text":"<p>This can be done with any cloud hosting tool/provider you desire (such as GCP, AWS, Dropbox, Google Drive, Github). As long as your file can be accessed through a direct download link, it will work with medperf. Generating a direct download link for your hosted file can be straight-forward when using some providers (e.g. Amazon Web Services, Google Cloud Platform, Microsoft Azure) and can be a bit tricky when using others (e.g. Dropbox, GitHub, Google Drive).</p> <p>Note</p> <p>Direct download links must be permanent</p> <p>Tip</p> <p>You can make sure if a URL is a direct download link or not using tools like <code>wget</code> or <code>curl</code>. Running <code>wget &lt;URL&gt;</code> will download the file if the URL is a direct download link. Running <code>wget &lt;URL&gt;</code> may fail or may download an HTML page if the URL is not a direct download link.</p> <p>When your file is hosted with a direct download link, MedPerf will be able to identify this file using that direct download link. So for example, when you are submitting a container, you should pass your hosted container config file as follows:</p> <pre><code>--container-config &lt;the-direct-download-link-to-the-file&gt;\n</code></pre> <p>Warning</p> <p>Files in this case are supposed to have at least anonymous public read access permission.</p>"},{"location":"concepts/hosting_files/#direct-download-links-of-files-on-github","title":"Direct download links of files on GitHub","text":"<p>It was a common practice by the current MedPerf users to host files on GitHub. You can learn below how to find the direct download link of a file hosted on GitHub. You can check online for other storage providers.</p> <p>It's important though to make sure the files won't be modified after their URLs are submitted to medperf, which could happen due to future commits. Because of this, the URLs of the files hosted on GitHub must contain a reference to the corresponding commit hash. Below are the steps to get this URL for a specific file:</p> <ol> <li>Open the GitHub repository and ensure you are in the correct branch</li> <li>Click on \u201cCommits\u201d at the right top corner of the repository explorer.</li> <li>Locate the latest commit, it is the top most commit.</li> <li>If you are targeting previous versions of your file, make sure to consider the right commit.</li> <li>Click on this button \u201c&lt;&gt;\u201d corresponding to the commit (Browse the repository at this point in the history).</li> <li>Navigate to the file of interest.</li> <li>Click on \u201cRaw\u201d.</li> <li>Copy the url from your browser. It should be a UserContent GitHub URLs (domain raw.githubusercontent.com).</li> </ol>"},{"location":"concepts/hosting_files/#synapse-hosting","title":"Synapse hosting","text":"<p>You can choose the option of hosting with Synapse in cases where privacy is a concern. Please refer to this link for hosting files on the Synapse platform.</p> <p>When your file is hosted on Synapse, MedPerf will be able to identify this file using the Synapse ID corresponding to that file. So for example, when you are submitting a container, you would pass your hosted container config file as follows (note the prefix):</p> <pre><code>--container-config synapse:&lt;the-synapse-id-of-the-file&gt;\n</code></pre> <p>Note that you need to authenticate with your Synapse credentials if you plan to use a Synaspe file with MedPerf. To do so run <code>medperf auth synapse_login</code>.</p> <p>Note</p> <p>You must authenticate if using files on Synapse. If this is not necessary, this means the file has anonymous public access read permission. In this case, Synapse allows you to generate a permanent direct download link for your file and you can follow the previous section.</p>"},{"location":"concepts/import_export_data/","title":"Copying your Dataset","text":"<p>When you register your dataset with MedPerf, your dataset will stay local on your machine. Suppose you decided to continue your work on a new machine; your local dataset needs to be copied/moved to the new machine and the MedPerf client installed on the new machine should be able to detect it.</p> <p>This guide is for users who registered their dataset while working on a certain machine but they decide then to continue working on another machine. You can use MedPerf commands to easily copy your dataset to another machine.</p>"},{"location":"concepts/import_export_data/#step1-export-your-dataset","title":"Step1: Export your dataset","text":"<p>First, you need to export your dataset into a <code>.gz</code> file so that the MedPerf client in the new machine later will be able to detect it and place it in the correct storage location.</p> <p>Run the following command to export your dataset:</p> <pre><code>medperf dataset export --data_uid YOUR_DATASET_ID --output OUTPUT_FOLDER\n</code></pre> <p>Where:</p> <ul> <li><code>YOUR_DATASET_ID</code> is your dataset ID,</li> <li><code>OUTPUT_FOLDER</code> is a path to a folder where MedPerf will create the exported <code>.gz</code> file.</li> </ul> <p>For example, the following command will export a dataset whose ID is <code>71</code>. The exported dataset file will be created inside the folder <code>./my_exported_dataset</code> and will be named <code>71.gz</code>.</p> <pre><code>medperf dataset export --data_uid 71 --output ./my_exported_dataset\n</code></pre>"},{"location":"concepts/import_export_data/#step2-import-your-dataset-in-the-new-machine","title":"Step2: Import your dataset in the new Machine","text":"<p>On your new machine, make sure you have MedPerf setup and installed. Take the generated file from the previous step (i.e., the <code>.gz</code> file) and place it somewhere in your new machine.</p> <p>Now to import the dataset into the MedPerf storage of the new machine, it depends on whether your dataset is still in development (i.e., you haven't fully prepared your dataset and haven't set it as operational), or if your dataset is operational.</p>"},{"location":"concepts/import_export_data/#case1-your-dataset-is-in-development","title":"Case1: Your dataset is in Development","text":"<p>In this case, the exported dataset file contains your raw data, because you will need it in the new machine to continue preparing your dataset. Run the following command in your new machine to import your dataset:</p> <pre><code>medperf dataset import --data_uid YOUR_DATASET_ID --input PATH_TO_THE_GZ_FILE --raw_dataset_path PATH_TO_PLACE_YOUR_RAW_DATA\n</code></pre> <p>Where:</p> <ul> <li><code>YOUR_DATASET_ID</code> is your dataset ID,</li> <li><code>PATH_TO_THE_GZ_FILE</code> is the path to the <code>.gz</code> file you placed in the new machine,</li> <li><code>PATH_TO_PLACE_YOUR_RAW_DATA</code> is the path to where the raw data will be placed in the new machine. This path should not already exist; MedPerf will create it.</li> </ul>"},{"location":"concepts/import_export_data/#case2-your-dataset-is-operational","title":"Case2: Your dataset is Operational","text":"<p>In this case, only your prepared data will be copied to the new machine. The raw data will remain on the old machine, as MedPerf no longer requires it. If you wish to move or back up your raw data for other purposes, you can do so using standard methods, such as manually transferring it. MedPerf on the new machine will not request or require access to it.</p> <p>Run the following command in your new machine to import your dataset:</p> <pre><code>medperf dataset import --data_uid YOUR_DATASET_ID --input PATH_TO_THE_GZ_FILE\n</code></pre> <p>Where:</p> <ul> <li><code>YOUR_DATASET_ID</code> is your dataset ID,</li> <li><code>PATH_TO_THE_GZ_FILE</code> is the path to the <code>.gz</code> file you placed in the new machine,</li> </ul>"},{"location":"concepts/import_export_data/#considerations","title":"Considerations","text":"<p>When working on a new machine, keep in mind the following:</p> <ul> <li>Any MedPerf profile configurations you had on your old machine (e.g., container platform, gpus, ...) should be configured again in the new machine if you want them.</li> <li>It's advised to logout on your old machine if you are not planning to use it anymore.</li> <li>Your dataset still exists on the old machine. You just created a copy of it in the new machine.</li> </ul>"},{"location":"concepts/priorities/","title":"In Progress","text":"<p>TODO: the page is hidden now. If implemented, find all usages and uncomment them.</p>"},{"location":"concepts/profiles/","title":"In Progress","text":"<p>TODO: the page is hidden now. If implemented, find all usages and uncomment them.</p>"},{"location":"concepts/single_run/","title":"In Progress","text":"<p>TODO: the page is hidden now. If implemented, find all usages and uncomment them.</p>"},{"location":"containers/containers/","title":"Containers in MedPerf","text":"<p>A benchmark workflow is composed of three steps: data preparation, inference, and metrics evaluation. These three steps are defined as containers. This document explains how to create these containers so that MedPerf is able to execute them properly.</p>"},{"location":"containers/containers/#container-types","title":"Container types","text":""},{"location":"containers/containers/#data-preparator-container","title":"Data Preparator Container","text":"<p>The Data Preparator container is used to prepare the data for executing the benchmark. Ideally, it can receive different data standards for the task at hand, transforming them into a single, unified standard. Additionally, it ensures the quality and compatibility of the data and computes statistics and metadata for registration purposes.</p> <p>This container interface should expose the following tasks/entrypoints:</p> <ul> <li> <p>prepare: Transforms the input data into the expected output data standard. It receives as input the location of the original data, as well as the location of the labels, and outputs the prepared dataset and accompanying labels. By default, inside the container filesystem, the container should read the input data from <code>/mlcommons/volumes/raw_data</code> and the input labels from <code>/mlcommons/volumes/raw_labels</code>, and should write the prepared/preprocessed version of the data in <code>/mlcommons/volumes/data</code> and <code>mlcommons/volumes/labels</code>.</p> </li> <li> <p>sanity_check: Ensures the integrity of the prepared data. It may check for anomalies and data corruption (e.g. blank images, empty test cases). It constitutes a set of conditions the prepared data should comply with. By default, inside the container filesystem, the container should read the input prepared data from <code>/mlcommons/volumes/data</code> and the input prepared labels from <code>/mlcommons/volumes/labels</code>.</p> </li> <li> <p>statistics: Computes statistics on the prepared data. By default, inside the container filesystem, the container should read the input prepared data from <code>/mlcommons/volumes/data</code> and the input prepared labels from <code>/mlcommons/volumes/labels</code>, and should write the computed statistics in the following file <code>/mlcommons/volumes/statistics/statistics.yaml</code>.</p> </li> </ul>"},{"location":"containers/containers/#model-container","title":"Model Container","text":"<p>The model container contains a pre-trained machine learning model that is going to be evaluated by the benchmark. It's interface should expose the following task:</p> <ul> <li>infer: Runs inference and computes predictions on the prepared data. It receives as input the location of the prepared data and outputs the predictions. By default, inside the container filesystem, the container should read the input prepared data from <code>/mlcommons/volumes/data</code> and should write the predictions in <code>/mlcommons/volumes/predictions</code>.</li> </ul>"},{"location":"containers/containers/#metricsevaluator-container","title":"Metrics/Evaluator Container","text":"<p>The Metrics Container is used for computing metrics on the model predictions by comparing them against the ground truth labels. It's interface should expose the following task:</p> <ul> <li>evaluate: Computes the metrics. It receives as input the location of the predictions and the location of the prepared data labels and generates a yaml file containing the metrics. By default, inside the container filesystem, the container should read the input predictions from <code>/mlcommons/volumes/predictions</code> and the input prepared labels from <code>/mlcommons/volumes/labels</code>, and should write the computed metrics in the following file <code>/mlcommons/volumes/results/results.yaml</code>.</li> </ul>"},{"location":"containers/containers/#extra-mounts","title":"Extra mounts","text":"<p>When creating a container, you may want to separate some assets from the container main code (e.g., your model weights, your model hyperparameters, some model weights used for annotation during data preparation, ...). MedPerf has a notion of an <code>additional_files</code> folder and a <code>parameters</code> file. When you submit a container metadata to the MedPerf server, you can put these assets inside a compressed folder <code>additional_files.tar.gz</code> file, and/or you can write your parameters to a <code>parameters.yaml</code> file. Then host these files and provide their URLs when submitting the container metadata. Instead of being baked inside the docker image, the contents of the archive <code>additional_files.tar.gz</code> file and the contents of the <code>parameters.yaml</code> file will be mounted by MedPerf to your container during runtime when your container is executed, as follows:</p> <ul> <li>Additional files: The contents of the <code>additional_files.tar.gz</code> archive will be uncompressed and available in <code>/mlcommons/volumes/additional_files</code> inside the container.</li> <li>Parameters file: Your parameters file will be available as <code>/mlcommons/volumes/parameters/parameters.yaml</code> inside the container.</li> </ul>"},{"location":"containers/containers/#container-configuration-file","title":"Container configuration file","text":"<p>A container configuration file defines properties about your container:</p> <ul> <li>Container type (Docker or Singularity)</li> <li>Image identifier (e.g., Dockerhub identifier for docker images, a URL to a .sif file for singularity images)</li> <li>Defined tasks. For example, if you are building a model container, it should define a <code>infer</code> task.</li> <li>For each defined task, you can specify run arguments (e.g., <code>command</code>, <code>environment variables</code>, ...) similar to how <code>docker run</code> consumes run arguments.</li> <li>For each defined task, the volumes to be mounted inside the container filesystem are configured.</li> </ul> <p>You can get a template of the container config of each container type by using the <code>medperf container create</code> command, or by checking the examples in the repository.</p>"},{"location":"containers/containers/#customizing-mount-paths","title":"Customizing mount paths","text":"<p>By default, as mentioned in previous sections, MedPerf mounts input and output volumes to default paths inside the container (e.g., input data to a model container will be mounted at <code>/mlcommons/volumes/data</code>). You can customize where these mounts will appear inside the container by modifying the container configuration file. Note that these mount paths should be absolute paths.</p>"},{"location":"containers/containers/#restrictions","title":"Restrictions","text":"<ul> <li> <p>Containers will not have network access during runtime. Any download or upload attempt inside the container during running it will result in an error.</p> </li> <li> <p>Containers will be run as a non-root user. Creating files and folders in the container filesystem outside the mounted predictions folder and outside the <code>/tmp</code> folder will result in permission error by default (unless you change filesystem permissions when building the container).</p> </li> <li> <p>Input volumes are mounted as read-only. Attempting to modify or delete them will result in an error.</p> </li> </ul>"},{"location":"getting_started/benchmark_owner_demo/","title":"Bechmark Committee","text":""},{"location":"getting_started/benchmark_owner_demo/#hands-on-tutorial-for-bechmark-committee","title":"Hands-on Tutorial for Bechmark Committee","text":""},{"location":"getting_started/benchmark_owner_demo/#overview","title":"Overview","text":"<p>In this guide, you will learn how a user can use MedPerf to create a benchmark. The key tasks can be summarized as follows:</p> <ol> <li>Implement a valid workflow.</li> <li>Develop a demo dataset.</li> <li>Test your workflow.</li> <li>Submitting the Containers to the MedPerf server.</li> <li>Host the demo dataset.</li> <li>Submit the benchmark to the MedPerf server.</li> </ol> <p>It's assumed that you have already set up the general testing environment as explained in the installation and setup guide.</p>"},{"location":"getting_started/benchmark_owner_demo/#before-you-start","title":"Before You Start","text":""},{"location":"getting_started/benchmark_owner_demo/#first-steps","title":"First steps","text":""},{"location":"getting_started/benchmark_owner_demo/#running-in-cloud-via-github-codespaces","title":"Running in cloud via Github Codespaces","text":"<p>As the most easy way to play with the tutorials you can launch a preinstalled Codespace cloud environment for MedPerf by clicking this link:</p> <p></p>"},{"location":"getting_started/benchmark_owner_demo/#running-in-local-environment","title":"Running in local environment","text":"<p>To start experimenting with MedPerf through this tutorial on your local machine, you need to start by following these quick steps:</p> <ol> <li>Install Medperf</li> <li>Set up Medperf</li> </ol>"},{"location":"getting_started/benchmark_owner_demo/#prepare-the-local-medperf-server","title":"Prepare the Local MedPerf Server","text":"<p>For the purpose of the tutorial, you have to initialize a local MedPerf server with a fresh database and then create the necessary entities that you will be interacting with. To do so, run the following: (make sure you are in MedPerf's root folder)</p> <pre><code>cd server\nsh reset_db.sh\npython seed.py --demo benchmark\ncd ..\n</code></pre>"},{"location":"getting_started/benchmark_owner_demo/#download-the-necessary-files","title":"Download the Necessary files","text":"<p>A script is provided to download all the necessary files so that you follow the tutorial smoothly. Run the following: (make sure you are in MedPerf's root folder)</p> <pre><code>sh tutorials_scripts/setup_benchmark_tutorial.sh\n</code></pre> <p>This will create a workspace folder <code>medperf_tutorial</code> where all necessary files are downloaded. The folder contains the following content:</p> Toy content description <p>In this tutorial we will create a benchmark that classifies chest X-Ray images.</p> <p>In real life all the listed artifacts and files have to be created on your own. However, for tutorial's sake you may use this toy data.</p>"},{"location":"getting_started/benchmark_owner_demo/#demo-data","title":"Demo Data","text":"<p>The <code>medperf_tutorial/demo_data/</code> folder contains the demo dataset content.</p> <ul> <li><code>images/</code> folder includes sample images.</li> <li><code>labels/labels.csv</code> provides a basic ground truth markup, indicating the class each image belongs to.</li> </ul> <p>The demo dataset is a sample dataset used for the development of your benchmark and used by Model Owners for the development of their models. More details are available in the section below</p>"},{"location":"getting_started/benchmark_owner_demo/#data-preparator-container","title":"Data Preparator Container","text":"<p>The <code>medperf_tutorial/data_preparator/</code> contains a DataPreparator container that you must implement. This container:</p> <ul> <li>Transforms raw data into a format convenient for model consumption, such as converting DICOM images into numpy tensors, cropping patches, normalizing columns, etc. It's up to you to define the format that is handy for future models.</li> <li>Ensures its output is in a standardized format, allowing Model Owners/Developers to rely on its consistency.</li> </ul>"},{"location":"getting_started/benchmark_owner_demo/#model-container","title":"Model Container","text":"<p>The <code>medperf_tutorial/model_custom_cnn/</code> is an example of a Model Container. You need to implement a reference model which will be used by data owners to test the compatibility of their data with your pipeline. Also, Model Developers joining your benchmark will follow the input/output specifications of this model when building their own models.</p>"},{"location":"getting_started/benchmark_owner_demo/#metrics-container","title":"Metrics Container","text":"<p>The <code>medperf_tutorial/metrics/</code> houses a Metrics Container that processes ground truth data, model predictions, and computes performance metrics - such as classification accuracy, loss, etc. After a Dataset Owner runs the benchmark pipeline on their data, these final metric values will be shared with you as the Benchmark Owner.</p>"},{"location":"getting_started/benchmark_owner_demo/#login-to-the-local-medperf-server","title":"Login to the Local MedPerf Server","text":"<p>The local MedPerf server is pre-configured with a dummy local authentication system. Remember that when you are communicating with the real MedPerf server, you should follow the steps in this guide to login. For the tutorials, you should run the following:</p> <pre><code>medperf auth login -e testbo@example.com\n</code></pre> <p>You are now ready to start!</p>"},{"location":"getting_started/benchmark_owner_demo/#1-implement-a-valid-workflow","title":"1. Implement a Valid Workflow","text":"<p> The implementation of a valid workflow is accomplished by implementing three Containers:</p> <ol> <li> <p>Data Preparator Container: This Container will transform raw data into a dataset ready for the AI model execution. All data owners willing to participate in this benchmark will have their data prepared using this Container. A guide on how to implement MedPerf-compatible data preparation Containers can be found here.</p> </li> <li> <p>Reference Model Container: This Container will contain an example model implementation for the desired AI task. It should be compatible with the data preparation Container (i.e., the outputs of the data preparation Container can be directly fed as inputs to this Container). A guide on how to implement MedPerf-compatible model Containers can be found here.</p> </li> <li> <p>Metrics Container: This Container will be responsible for evaluating the performance of a model. It should be compatible with the reference model Container (i.e., the outputs of the reference model Container can be directly fed as inputs to this Container). A guide on how to implement MedPerf-compatible metrics Containers can be found here.</p> </li> </ol> <p>For this tutorial, you are provided with following three already implemented containers for the task of chest X-ray classification. The implementations can be found in the following links: Data Preparator, Reference Model, Metrics. These containers are setup locally for you and can be found in your workspace folder under <code>data_preparator</code>, <code>model_custom_cnn</code>, and <code>metrics</code>.</p>"},{"location":"getting_started/benchmark_owner_demo/#2-develop-a-demo-dataset","title":"2. Develop a Demo Dataset","text":"<p> A demo dataset is a small reference dataset. It contains a few data records and their labels, which will be used to test the benchmark's workflow in two scenarios:</p> <ol> <li> <p>It is used for testing the benchmark's default workflow. The MedPerf client automatically runs a compatibility test of the benchmark's three containers prior to its submission. The test is run using the benchmark's demo dataset as input.</p> </li> <li> <p>When a model owner wants to participate in the benchmark, the MedPerf client tests the compatibility of their model with the benchmark's data preparation container and metrics container. The test is run using the benchmark's demo dataset as input.</p> </li> </ol> <p>For this tutorial, you are provided with a demo dataset for the chest X-ray classification workflow. The dataset can be found in your workspace folder under <code>demo_data</code>. It is a small dataset comprising two chest X-ray images and corresponding thoracic disease labels.</p> <p>You can test the workflow now that you have the three containers and the demo data. Testing the workflow before submitting any asset to the MedPerf server is usually recommended.</p>"},{"location":"getting_started/benchmark_owner_demo/#3-test-your-workflow","title":"3. Test your Workflow","text":"<p>MedPerf provides a single command to test an inference workflow. To test your workflow with local containers and local data, the following need to be passed to the command:</p> <ol> <li>Path to the data preparation container config file: <code>medperf_tutorial/data_preparator/container_config.yaml</code>.</li> <li>Path to the model container config file: <code>medperf_tutorial/model_custom_cnn/container_config.yaml</code>.</li> <li>Path to the metrics container config file: <code>medperf_tutorial/metrics/container_config.yaml</code>.</li> <li>Path to the demo dataset data records: <code>medperf_tutorial/demo_data/images</code>.</li> <li>Path to the demo dataset data labels. <code>medperf_tutorial/demo_data/labels</code>.</li> </ol> <p>Run the following command to execute the test ensuring you are in MedPerf's root folder:</p> <pre><code>medperf test run \\\n--data_preparator \"medperf_tutorial/data_preparator/container_config.yaml\" \\\n--model \"medperf_tutorial/model_custom_cnn/container_config.yaml\" \\\n--evaluator \"medperf_tutorial/metrics/container_config.yaml\" \\\n--data_path \"medperf_tutorial/demo_data/images\" \\\n--labels_path \"medperf_tutorial/demo_data/labels\"\n</code></pre> <p>Assuming the test passes successfully, you are ready to host the benchmark assets.</p>"},{"location":"getting_started/benchmark_owner_demo/#4-host-the-demo-dataset","title":"4. Host the Demo Dataset","text":"<p>The demo dataset should be packaged in a specific way as a compressed tarball file. The folder stucture in the workspace currently looks like the following:</p> <pre><code>.\n\u2514\u2500\u2500 medperf_tutorial\n    \u251c\u2500\u2500 demo_data\n    \u2502   \u251c\u2500\u2500 images\n    \u2502   \u2514\u2500\u2500 labels\n    \u2502\n    ...\n</code></pre> <p>The goal is to package the folder <code>demo_data</code>. You must first create a file called <code>paths.yaml</code>. This file will provide instructions on how to locate the data records path and the labels path. The <code>paths.yaml</code> file should specify both the data records path and the labels path.</p> <p>In your workspace directory (<code>medperf_tutorial</code>), create a file <code>paths.yaml</code> and fill it with the following:</p> <pre><code>data_path: demo_data/images\nlabels_path: demo_data/labels\n</code></pre> <p>Note</p> <p>The paths are determined by the Data Preparator container's expected input path.</p> <p>After that, the workspace should look like the following:</p> <pre><code>.\n\u2514\u2500\u2500 medperf_tutorial\n    \u251c\u2500\u2500 demo_data\n    \u2502   \u251c\u2500\u2500 images\n    \u2502   \u2514\u2500\u2500 labels\n    \u251c\u2500\u2500 paths.yaml\n    \u2502\n    ...\n</code></pre> <p>Finally, compress the required assets (<code>demo_data</code> and <code>paths.yaml</code>) into a tarball file by running the following command:</p> <pre><code>cd medperf_tutorial\ntar -czf demo_data.tar.gz demo_data paths.yaml\ncd ..\n</code></pre> <p>And that's it! Now you have to host the tarball file (<code>demo_data.tar.gz</code>) on the internet.</p> <p>For the tutorial to run smoothly, the file is already hosted at the following URL:</p> <pre><code>https://storage.googleapis.com/medperf-storage/chestxray_tutorial/demo_data.tar.gz\n</code></pre> <p>If you wish to host it by yourself, you can find the list of supported options and details about hosting files in this page.</p>"},{"location":"getting_started/benchmark_owner_demo/#5-submitting-the-containers","title":"5. Submitting the Containers","text":""},{"location":"getting_started/benchmark_owner_demo/#how-does-medperf-recognize-a-container","title":"How does MedPerf Recognize a Container?","text":"<p>The MedPerf server registers a container as metadata comprised of a set of assets that can be retrieved from the internet. This means that before submitting a container you have to host its assets on the internet. You can refer to this page if you want to understand what the assets are.</p>"},{"location":"getting_started/benchmark_owner_demo/#host-the-files","title":"Host the Files","text":"<p>For the tutorial to run smoothly, the assets are already hosted. If you wish to host them by yourself, you can find the list of supported options and details about hosting files in this page.</p>"},{"location":"getting_started/benchmark_owner_demo/#submit-the-containers","title":"Submit the Containers","text":""},{"location":"getting_started/benchmark_owner_demo/#data-preparator-container_1","title":"Data Preparator Container","text":"<p> In this tutorial, for the Data Preparator container, the submission should include:</p> <ul> <li> <p>The URL to the hosted container configuration file, which is:</p> <pre><code>https://raw.githubusercontent.com/mlcommons/medperf/main/examples/chestxray_tutorial/data_preparator/container_config.yaml\n</code></pre> </li> <li> <p>The URL to the hosted parameters file, which is:</p> <pre><code>https://raw.githubusercontent.com/mlcommons/medperf/main/examples/chestxray_tutorial/data_preparator/workspace/parameters.yaml\n</code></pre> </li> </ul> <p>Use the following command to submit:</p> <pre><code>medperf container submit \\\n--name my-prep \\\n--container-config-file \"https://raw.githubusercontent.com/mlcommons/medperf/main/examples/chestxray_tutorial/data_preparator/container_config.yaml\" \\\n--parameters-file \"https://raw.githubusercontent.com/mlcommons/medperf/main/examples/chestxray_tutorial/data_preparator/workspace/parameters.yaml\" \\\n--operational\n</code></pre>"},{"location":"getting_started/benchmark_owner_demo/#reference-model-container","title":"Reference Model Container","text":"<p> In this tutorial, for the Reference Model container, the submission should include:</p> <ul> <li> <p>The URL to the hosted container configuration file:</p> <pre><code>https://raw.githubusercontent.com/mlcommons/medperf/main/examples/chestxray_tutorial/model_custom_cnn/container_config.yaml\n</code></pre> </li> <li> <p>The URL to the hosted parameters file:</p> <pre><code>https://raw.githubusercontent.com/mlcommons/medperf/main/examples/chestxray_tutorial/model_custom_cnn/workspace/parameters.yaml\n</code></pre> </li> <li> <p>The URL to the hosted additional files tarball file:</p> <pre><code>https://storage.googleapis.com/medperf-storage/chestxray_tutorial/cnn_weights.tar.gz\n</code></pre> </li> </ul> <p>Use the following command to submit:</p> <pre><code>medperf container submit \\\n--name my-refmodel \\\n--container-config-file \"https://raw.githubusercontent.com/mlcommons/medperf/main/examples/chestxray_tutorial/model_custom_cnn/container_config.yaml\" \\\n--parameters-file \"https://raw.githubusercontent.com/mlcommons/medperf/main/examples/chestxray_tutorial/model_custom_cnn/workspace/parameters.yaml\" \\\n--additional-file \"https://storage.googleapis.com/medperf-storage/chestxray_tutorial/cnn_weights.tar.gz\" \\\n--operational\n</code></pre>"},{"location":"getting_started/benchmark_owner_demo/#metrics-container_1","title":"Metrics Container","text":"<p> In this tutorial, for the Metrics container, the submission should include:</p> <ul> <li> <p>The URL to the hosted container configuration file:</p> <pre><code>https://raw.githubusercontent.com/mlcommons/medperf/main/examples/chestxray_tutorial/metrics/container_config.yaml\n</code></pre> </li> <li> <p>The URL to the hosted parameters file:</p> <pre><code>https://raw.githubusercontent.com/mlcommons/medperf/main/examples/chestxray_tutorial/metrics/workspace/parameters.yaml\n</code></pre> </li> </ul> <p>Use the following command to submit:</p> <pre><code>medperf container submit \\\n--name my-metrics \\\n--container-config-file \"https://raw.githubusercontent.com/mlcommons/medperf/main/examples/chestxray_tutorial/metrics/container_config.yaml\" \\\n--parameters-file \"https://raw.githubusercontent.com/mlcommons/medperf/main/examples/chestxray_tutorial/metrics/workspace/parameters.yaml\" \\\n--operational\n</code></pre> <p>Each of the three containers will be assigned by a server UID. You can check the server UID for each container by running:</p> <pre><code>medperf container ls --mine\n</code></pre> <p>Finally, now after having the containers submitted and the demo dataset hosted, you can submit the benchmark to the MedPerf server.</p>"},{"location":"getting_started/benchmark_owner_demo/#6-submit-your-benchmark","title":"6. Submit your Benchmark","text":"<p> You need to keep at hand the following information:</p> <ul> <li>The Demo Dataset URL. Here, the URL will be:</li> </ul> <pre><code>https://storage.googleapis.com/medperf-storage/chestxray_tutorial/demo_data.tar.gz\n</code></pre> <ul> <li>The server UIDs of the three containers can be found by running:</li> </ul> <pre><code> medperf container ls\n</code></pre> <ul> <li>For this tutorial, the UIDs are as follows:</li> <li>Data preparator UID: <code>1</code></li> <li>Reference model UID: <code>2</code></li> <li>Evaluator UID: <code>3</code></li> </ul> <p>You can create and submit your benchmark using the following command:</p> <pre><code>medperf benchmark submit \\\n--name tutorial_bmk \\\n--description \"MedPerf demo bmk\" \\\n--demo-url \"https://storage.googleapis.com/medperf-storage/chestxray_tutorial/demo_data.tar.gz\" \\\n--data-preparation-container 1 \\\n--reference-model-container 2 \\\n--evaluator-container 3 \\\n--operational\n</code></pre> <p>The MedPerf client will first automatically run a compatibility test between the containers using the demo dataset. If the test is successful, the benchmark will be submitted along with the compatibility test results.</p> <p>Note</p> <p>The benchmark will stay inactive until the MedPerf server admin approves your submission.</p> <p>That's it! You can check your benchmark's server UID by running:</p> <pre><code>medperf benchmark ls --mine\n</code></pre> <p></p>"},{"location":"getting_started/benchmark_owner_demo/#cleanup-optional","title":"Cleanup (Optional)","text":"<p>You have reached the end of the tutorial! If you are planning to rerun any of the tutorials, don't forget to cleanup:</p> <ul> <li> <p>To shut down the local MedPerf server: press <code>CTRL</code>+<code>C</code> in the terminal where the server is running.</p> </li> <li> <p>To cleanup the downloaded files workspace (make sure you are in the MedPerf's root directory):</p> </li> </ul> <pre><code>rm -fr medperf_tutorial\n</code></pre> <ul> <li>To cleanup the local MedPerf server database: (make sure you are in the MedPerf's root directory)</li> </ul> <pre><code>cd server\nsh reset_db.sh\n</code></pre> <ul> <li>To cleanup the test storage:</li> </ul> <pre><code>rm -fr ~/.medperf/localhost_8000\n</code></pre>"},{"location":"getting_started/data_owner_demo/","title":"Data Owners","text":""},{"location":"getting_started/data_owner_demo/#hands-on-tutorial-for-data-owners","title":"Hands-on Tutorial for Data Owners","text":""},{"location":"getting_started/data_owner_demo/#overview","title":"Overview","text":"<p>As a data owner, you plan to run a benchmark on your own dataset. Using MedPerf, you will prepare your (raw) dataset and submit information about it to the MedPerf server. You may have to consult the benchmark committee to make sure that your raw dataset aligns with the benchmark's expected input format.</p> <p>Note</p> <p>A key concept of MedPerf is the stringent confidentiality of your data. It remains exclusively on your machine. Only minimal information about your dataset, such as the hash of its contents, is submitted. Once your Dataset is submitted and associated with a benchmark, you can run all benchmark models on your data within your own infrastructure and see the results / predictions.</p> <p>This guide provides you with the necessary steps to use MedPerf as a Data Owner. The key tasks can be summarized as follows:</p> <ol> <li>Register your data information.</li> <li>Prepare your data.</li> <li>Mark your data as operational.</li> <li>Request participation in a benchmark.</li> <li>Execute the benchmark models on your dataset.</li> <li>Submit a result.</li> </ol> <p>It is assumed that you have the general testing environment set up.</p>"},{"location":"getting_started/data_owner_demo/#before-you-start","title":"Before You Start","text":""},{"location":"getting_started/data_owner_demo/#first-steps","title":"First steps","text":""},{"location":"getting_started/data_owner_demo/#running-in-cloud-via-github-codespaces","title":"Running in cloud via Github Codespaces","text":"<p>As the most easy way to play with the tutorials you can launch a preinstalled Codespace cloud environment for MedPerf by clicking this link:</p> <p></p>"},{"location":"getting_started/data_owner_demo/#running-in-local-environment","title":"Running in local environment","text":"<p>To start experimenting with MedPerf through this tutorial on your local machine, you need to start by following these quick steps:</p> <ol> <li>Install Medperf</li> <li>Set up Medperf</li> </ol>"},{"location":"getting_started/data_owner_demo/#prepare-the-local-medperf-server","title":"Prepare the Local MedPerf Server","text":"<p>For the purpose of the tutorial, you have to initialize a local MedPerf server with a fresh database and then create the necessary entities that you will be interacting with. To do so, run the following: (make sure you are in MedPerf's root folder)</p> <pre><code>cd server\nsh reset_db.sh\npython seed.py --demo data\ncd ..\n</code></pre>"},{"location":"getting_started/data_owner_demo/#download-the-necessary-files","title":"Download the Necessary files","text":"<p>A script is provided to download all the necessary files so that you follow the tutorial smoothly. Run the following: (make sure you are in MedPerf's root folder)</p> <pre><code>sh tutorials_scripts/setup_data_tutorial.sh\n</code></pre> <p>This will create a workspace folder <code>medperf_tutorial</code> where all necessary files are downloaded. The folder contains the following content:</p> Toy content description <p>In real life all the listed artifacts and files have to be created on your own. However, for tutorial's sake you may use this toy data.</p>"},{"location":"getting_started/data_owner_demo/#tutorials-dataset-example","title":"Tutorial's Dataset Example","text":"<p>The <code>medperf_tutorial/sample_raw_data/</code> folder contains your data for the specified Benchmark. In this tutorial, where the benchmark involves classifying chest X-Ray images, your data comprises:</p> <ul> <li><code>images/</code> folder contains your images</li> <li><code>labels/labels.csv</code>, which provides the ground truth markup, specifying the class of each image.</li> </ul> <p>The format of this data is dictated by the Benchmark Owner, as it must be compatible with the benchmark's Data Preparation container. In a real-world scenario, the expected data format would differ from this toy example. Refer to the Benchmark Owner to get a format specifications and details for your practical case.</p> <p>As previously mentioned, your data itself never leaves your machine. During the dataset submission, only basic metadata is transferred, for which you will be prompted to confirm.</p>"},{"location":"getting_started/data_owner_demo/#login-to-the-local-medperf-server","title":"Login to the Local MedPerf Server","text":"<p>The local MedPerf server is pre-configured with a dummy local authentication system. Remember that when you are communicating with the real MedPerf server, you should follow the steps in this guide to login. For the tutorials, you should run the following:</p> <pre><code>medperf auth login -e testdo@example.com\n</code></pre> <p>You are now ready to start!</p>"},{"location":"getting_started/data_owner_demo/#1-register-your-data-information","title":"1. Register your Data Information","text":"<p> To register your dataset, you need to collect the following information:</p> <ul> <li>A name you wish to have for your dataset.</li> <li>A small description of the dataset.</li> <li>The source location of your data (e.g., hospital name).</li> <li>The path to the data records (here, it is <code>medperf_tutorial/sample_raw_data/images</code>).</li> <li>The path to the labels of the data (here, it is <code>medperf_tutorial/sample_raw_data/labels</code>)</li> <li>The benchmark ID that you wish to participate in. This ensures your data in the next step will be prepared using the benchmark's data preparation container.</li> </ul> <p>Note</p> <p>The <code>data_path</code> and <code>labels_path</code> are determined according to the input path requirements of the data preparation container. To ensure that your data is structured correctly, it is recommended to check with the Benchmark Committee for specific details or instructions.</p> <p>In order to find the benchmark ID, you can execute the following command to view the list of available benchmarks.</p> <pre><code>medperf benchmark ls\n</code></pre> <p>The target benchmark ID here is <code>1</code>.</p> <p>Note</p> <p>You will be submitting general information about the data, not the data itself. The data never leaves your machine.</p> <p>Run the following command to register your data (make sure you are in MedPerf's root folder):</p> <pre><code>medperf dataset submit \\\n--name \"mytestdata\" \\\n--description \"A tutorial dataset\" \\\n--location \"My machine\" \\\n--data_path \"medperf_tutorial/sample_raw_data/images\" \\\n--labels_path \"medperf_tutorial/sample_raw_data/labels\" \\\n--benchmark 1\n</code></pre> <p>Once you run this command, the information to be submitted will be displayed on the screen and you will be asked to confirm your submission. Once you confirm, your dataset will be successfully registered!</p>"},{"location":"getting_started/data_owner_demo/#2-prepare-your-data","title":"2. Prepare your Data","text":"<p>To prepare and preprocess your dataset, you need to know the server UID of your registered dataset. You can check your datasets information by running:</p> <pre><code>medperf dataset ls --mine\n</code></pre> <p>In our tutorial, your dataset ID will be <code>1</code>. Run the following command to prepare your dataset:</p> <pre><code>medperf dataset prepare --data_uid 1\n</code></pre> <p>This command will also calculate statistics on your data; statistics defined by the benchmark owner. These will be submitted to the MedPerf server in the next step upon your approval.</p>"},{"location":"getting_started/data_owner_demo/#3-mark-your-dataset-as-operational","title":"3. Mark your Dataset as Operational","text":"<p>After successfully preparing your dataset, you can mark it as ready so that it can be associated with benchmarks you want. During preparation, your dataset is considered in the <code>Development</code> stage, and now you will mark it as operational.</p> <p>Note</p> <p>Once marked as operational, it can never be marked as in-development anymore.</p> <p>Run the following command to mark your dataset as operational:</p> <pre><code>medperf dataset set_operational --data_uid 1\n</code></pre> <p>Once you run this command, you will see on your screen the updated information of your dataset along with the statistics mentioned in the previous step. You will be asked to confirm submission of the displayed information. Once you confirm, your dataset will be successfully marked as operational!</p> <p>Next, you can proceed to request participation in the benchmark by initiating an association request.</p>"},{"location":"getting_started/data_owner_demo/#4-request-participation","title":"4. Request Participation","text":"<p> For submitting the results of executing the benchmark models on your data in the future, you must associate your data with the benchmark.</p> <p>Once you have submitted your dataset to the MedPerf server, it will be assigned a server UID, which you can find by running <code>medperf dataset ls --mine</code>. Your dataset's server UID is also <code>1</code>.</p> <p>Run the following command to request associating your dataset with the benchmark:</p> <pre><code>medperf dataset associate --benchmark_uid 1 --data_uid 1\n</code></pre> <p>This command will first run the benchmark's reference model on your dataset to ensure your dataset is compatible with the benchmark workflow. Then, the association request information is printed on the screen, which includes an executive summary of the test mentioned. You will be prompted to confirm sending this information and initiating this association request.</p>"},{"location":"getting_started/data_owner_demo/#how-to-proceed-after-requesting-association","title":"How to proceed after requesting association","text":"<p> When participating with a real benchmark, you must wait for the Benchmark Committee to approve the association request. You can check the status of your association requests by running <code>medperf association ls -bd</code>. The association is identified by the server UIDs of your dataset and the benchmark with which you are requesting association.</p> <p>For the sake of continuing the tutorial only, run the following to simulate the benchmark committee approving your association (make sure you are in the MedPerf's root directory):</p> <pre><code>sh tutorials_scripts/simulate_data_association_approval.sh\n</code></pre> <p>You can verify if your association request has been approved by running <code>medperf association ls -bd</code>.</p>"},{"location":"getting_started/data_owner_demo/#5-execute-the-benchmark","title":"5. Execute the Benchmark","text":"<p> MedPerf provides a command that runs all the models of a benchmark effortlessly. You only need to provide two parameters:</p> <ul> <li>The benchmark ID you want to run, which is <code>1</code>.</li> <li>The server UID of your data, which is <code>1</code>.</li> </ul> <p>For that, run the following command:</p> <pre><code>medperf benchmark run --benchmark 1 --data_uid 1\n</code></pre> <p>After running the command, you will receive a summary of the executions. You will see something similar to the following:</p> <pre><code>  model    Execution UID  partial result    from cache    error\n-------  ---------------  ----------------  ------------  -------\n      5                2  False             False\n      3                1  False             True\nTotal number of models: 2\n        1 were skipped (already executed), of which 0 have partial results\n        0 failed\n        1 ran successfully, of which 0 have partial results\n\n\u2705 Done!\n</code></pre> <p>This means that the benchmark has two models:</p> <ul> <li>A model that you already ran when you requested the association. This explains why it was skipped.</li> <li>Another model that ran successfully. Its result UID is <code>2</code>.</li> </ul> <p>You can view the results by running the following command with the specific result UID. For example:</p> <pre><code>medperf result show_local_results 2\n</code></pre> <p>For now, your results are only local. Next, you will learn how to submit the results.</p>"},{"location":"getting_started/data_owner_demo/#6-submit-a-result","title":"6. Submit a Result","text":"<p> After executing the benchmark, you will submit the results to the MedPerf server. To do so, you have to find the target result ID.</p> <p>As an example, you will be submitting the result of UID <code>2</code>. To do this, run the following command:</p> <pre><code>medperf result submit --result 2\n</code></pre> <p>The information that is going to be submitted will be printed to the screen and you will be prompted to confirm that you want to submit.</p> <p></p>"},{"location":"getting_started/data_owner_demo/#cleanup-optional","title":"Cleanup (Optional)","text":"<p>You have reached the end of the tutorial! If you are planning to rerun any of the tutorials, don't forget to cleanup:</p> <ul> <li> <p>To shut down the local MedPerf server: press <code>CTRL</code>+<code>C</code> in the terminal where the server is running.</p> </li> <li> <p>To cleanup the downloaded files workspace (make sure you are in the MedPerf's root directory):</p> </li> </ul> <pre><code>rm -fr medperf_tutorial\n</code></pre> <ul> <li>To cleanup the local MedPerf server database: (make sure you are in the MedPerf's root directory)</li> </ul> <pre><code>cd server\nsh reset_db.sh\n</code></pre> <ul> <li>To cleanup the test storage:</li> </ul> <pre><code>rm -fr ~/.medperf/localhost_8000\n</code></pre>"},{"location":"getting_started/installation/","title":"Installation","text":""},{"location":"getting_started/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"getting_started/installation/#python","title":"Python","text":"<p>Make sure you have Python 3.9 installed along with pip. To check if they are installed, run:</p> <pre><code>python --version\npip --version\n</code></pre> <p>or, depending on you machine configuration:</p> <pre><code>python3 --version\npip3 --version\n</code></pre> <p>We will assume the commands' names are <code>pip</code> and <code>python</code>. Use <code>pip3</code> and <code>python3</code> if your machine is configured differently.</p>"},{"location":"getting_started/installation/#docker-or-singularity","title":"Docker or Singularity","text":"<p>Make sure you have the latest version of Docker or Singularity 3.10 installed.</p> <p>To verify docker is installed, run:</p> <pre><code>docker --version\n</code></pre> <p>To verify singularity is installed, run:</p> <pre><code>singularity --version\n</code></pre> <p>If using Docker, make sure you can run Docker as a non-root user.</p>"},{"location":"getting_started/installation/#install-medperf","title":"Install MedPerf","text":"<ol> <li> <p>(Optional) MedPerf is better to be installed in a virtual environment. We recommend using Anaconda. Having anaconda installed, create a virtual environment <code>medperf-env</code> with the following command:</p> <pre><code>conda create -n medperf-env python=3.9\n</code></pre> <p>Then, activate your environment:</p> <pre><code>conda activate medperf-env\n</code></pre> </li> <li> <p>Clone the MedPerf repository:</p> <pre><code>git clone https://github.com/mlcommons/medperf.git\ncd medperf\n</code></pre> </li> <li> <p>Install MedPerf from source:</p> <pre><code>pip install -e ./cli\n</code></pre> </li> <li> <p>Verify the installation:</p> <pre><code>medperf --version\n</code></pre> </li> </ol>"},{"location":"getting_started/installation/#whats-next","title":"What's Next?","text":"<ul> <li>Get familiar with the MedPerf client by following the hands-on tutorials.</li> <li>Understand and learn how to build MedPerf-compatible containers.</li> </ul>"},{"location":"getting_started/model_owner_demo/","title":"Model Owners","text":""},{"location":"getting_started/model_owner_demo/#hands-on-tutorial-for-model-owners","title":"Hands-on Tutorial for Model Owners","text":""},{"location":"getting_started/model_owner_demo/#overview","title":"Overview","text":"<p>In this guide, you will learn how a Model Owner can use MedPerf to take part in a benchmark. Usuall as a model owner you may be also interested in how to build a MedPerf-compatible model container. But this guide provides an already implemented container if you want to directly proceed to learn how to interact with MedPerf.</p> <p>The main tasks of this guide are:</p> <ol> <li>Testing container compatibility with the benchmark.</li> <li>Submitting the container.</li> <li>Requesting participation in a benchmark.</li> </ol> <p>It's assumed that you have already set up the general testing environment as explained in the setup guide.</p>"},{"location":"getting_started/model_owner_demo/#before-you-start","title":"Before You Start","text":""},{"location":"getting_started/model_owner_demo/#first-steps","title":"First steps","text":""},{"location":"getting_started/model_owner_demo/#running-in-cloud-via-github-codespaces","title":"Running in cloud via Github Codespaces","text":"<p>As the most easy way to play with the tutorials you can launch a preinstalled Codespace cloud environment for MedPerf by clicking this link:</p> <p></p>"},{"location":"getting_started/model_owner_demo/#running-in-local-environment","title":"Running in local environment","text":"<p>To start experimenting with MedPerf through this tutorial on your local machine, you need to start by following these quick steps:</p> <ol> <li>Install Medperf</li> <li>Set up Medperf</li> </ol>"},{"location":"getting_started/model_owner_demo/#prepare-the-local-medperf-server","title":"Prepare the Local MedPerf Server","text":"<p>For the purpose of the tutorial, you have to initialize a local MedPerf server with a fresh database and then create the necessary entities that you will be interacting with. To do so, run the following: (make sure you are in MedPerf's root folder)</p> <pre><code>cd server\nsh reset_db.sh\npython seed.py --demo model\ncd ..\n</code></pre>"},{"location":"getting_started/model_owner_demo/#download-the-necessary-files","title":"Download the Necessary files","text":"<p>A script is provided to download all the necessary files so that you follow the tutorial smoothly. Run the following: (make sure you are in MedPerf's root folder)</p> <pre><code>sh tutorials_scripts/setup_model_tutorial.sh\n</code></pre> <p>This will create a workspace folder <code>medperf_tutorial</code> where all necessary files are downloaded. The folder contains the following content:</p> Toy content description <p>In real life all the listed artifacts and files have to be created on your own. However, for tutorial's sake you may use this toy data.</p>"},{"location":"getting_started/model_owner_demo/#model-mlcube","title":"Model MLCube","text":"<p>The <code>medperf_tutorial/model_mobilenetv2/</code> is a toy Model container. Once you submit your model to the benchmark, all participating Data Owners would be able to run the model within the benchmark pipeline. Therefore, your container must support the specific input/output formats defined by the Benchmark Owners.</p> <p>For the purposes of this tutorial, you will work with a pre-prepared toy benchmark. In a real-world scenario,  you should refer to your Benchmark Owner to get a format specifications and details for your practical case.</p>"},{"location":"getting_started/model_owner_demo/#login-to-the-local-medperf-server","title":"Login to the Local MedPerf Server","text":"<p>The local MedPerf server is pre-configured with a dummy local authentication system. Remember that when you are communicating with the real MedPerf server, you should follow the steps in this guide to login. For the tutorials, you should run the following:</p> <pre><code>medperf auth login -e testmo@example.com\n</code></pre> <p>You are now ready to start!</p>"},{"location":"getting_started/model_owner_demo/#1-test-your-container-compatibility","title":"1. Test your Container Compatibility","text":"<p> Before submitting your container, it is highly recommended that you test your container compatibility with the benchmarks of interest to avoid later edits and multiple submissions. Your container should be compatible with the benchmark workflow in two main ways:</p> <ol> <li>It should expect a specific data input structure</li> <li>Its outputs should follow a particular structure expected by the benchmark's metrics evaluator container</li> </ol> <p>These details should usually be acquired by contacting the Benchmark Committee and following their instructions.</p> <p>To test your container validity with the benchmark, first run <code>medperf benchmark ls</code> to identify the benchmark's server UID. In this tutorial, it is going to be <code>1</code>.</p> <p>Next, locate the container. Unless you implemented your own container, the container provided for this tutorial is located in your workspace: <code>medperf_tutorial/model_mobilenetv2/container_config.yaml</code>.</p> <p>After that, run the compatibility test:</p> <pre><code>medperf test run \\\n--benchmark 1 \\\n--model \"medperf_tutorial/model_mobilenetv2/container_config.yaml\"\n</code></pre> <p>Assuming the test passes successfuly, you are ready to submit the container to the MedPerf server.</p>"},{"location":"getting_started/model_owner_demo/#2-submit-the-container","title":"2. Submit the Container","text":""},{"location":"getting_started/model_owner_demo/#how-does-medperf-recognize-an-container","title":"How does MedPerf Recognize an Container?","text":"<p>The MedPerf server registers a container as metadata comprised of a set of assets that can be retrieved from the internet. This means that before submitting a container you have to host its assets on the internet. You can refer to this page if you want to understand what the assets are.</p>"},{"location":"getting_started/model_owner_demo/#host-the-files","title":"Host the Files","text":"<p>For the tutorial to run smoothly, the assets are already hosted. If you wish to host them by yourself, you can find the list of supported options and details about hosting files in this page.</p>"},{"location":"getting_started/model_owner_demo/#submit-the-container","title":"Submit the Container","text":"<p>The submission should include the URLs of all the hosted assets. For the Container provided for the tutorial:</p> <ul> <li>The URL to the hosted container configuration file is</li> </ul> <pre><code>https://raw.githubusercontent.com/mlcommons/medperf/main/examples/chestxray_tutorial/model_mobilenetv2/container_config.yaml\n</code></pre> <ul> <li>The URL to the hosted parameters file is</li> </ul> <pre><code>https://raw.githubusercontent.com/mlcommons/medperf/main/examples/chestxray_tutorial/model_mobilenetv2/workspace/parameters.yaml\n</code></pre> <ul> <li>The URL to the hosted additional files tarball file is</li> </ul> <pre><code>https://storage.googleapis.com/medperf-storage/chestxray_tutorial/mobilenetv2_weights.tar.gz\n</code></pre> <p>Use the following command to submit:</p> <pre><code>medperf container submit \\\n--name my-model \\\n--container-config-file \"https://raw.githubusercontent.com/mlcommons/medperf/main/examples/chestxray_tutorial/model_mobilenetv2/container_config.yaml\" \\\n--parameters-file \"https://raw.githubusercontent.com/mlcommons/medperf/main/examples/chestxray_tutorial/model_mobilenetv2/workspace/parameters.yaml\" \\\n--additional-file \"https://storage.googleapis.com/medperf-storage/chestxray_tutorial/mobilenetv2_weights.tar.gz\" \\\n--operational\n</code></pre> <p>The container will be assigned by a server UID. You can check it by running:</p> <pre><code>medperf container ls --mine\n</code></pre>"},{"location":"getting_started/model_owner_demo/#3-request-participation","title":"3. Request Participation","text":"<p> Benchmark workflows are run by Data Owners, who will get notified when a new model is added to a benchmark. You must request the association for your model to be part of the benchmark.</p> <p>To initiate an association request, you need to collect the following information:</p> <ul> <li>The target benchmark ID, which is <code>1</code></li> <li>The server UID of your container, which is <code>4</code>.</li> </ul> <p>Run the following command to request associating your container with the benchmark:</p> <pre><code>medperf container associate --benchmark 1 --model_uid 4\n</code></pre> <p>This command will first run the benchmark's workflow on your model to ensure your model is compatible with the benchmark workflow. Then, the association request information is printed on the screen, which includes an executive summary of the test mentioned. You will be prompted to confirm sending this information and initiating this association request.</p>"},{"location":"getting_started/model_owner_demo/#what-happens-after-requesting-the-association","title":"What Happens After Requesting the Association?","text":"<p> When participating with a real benchmark, you must wait for the Benchmark Committee to approve the association request. You can check the status of your association requests by running <code>medperf association ls -bm</code>. The association is identified by the server UIDs of your container and the benchmark with which you are requesting association.</p> <p></p>"},{"location":"getting_started/model_owner_demo/#cleanup-optional","title":"Cleanup (Optional)","text":"<p>You have reached the end of the tutorial! If you are planning to rerun any of the tutorials, don't forget to cleanup:</p> <ul> <li> <p>To shut down the local MedPerf server: press <code>CTRL</code>+<code>C</code> in the terminal where the server is running.</p> </li> <li> <p>To cleanup the downloaded files workspace (make sure you are in the MedPerf's root directory):</p> </li> </ul> <pre><code>rm -fr medperf_tutorial\n</code></pre> <ul> <li>To cleanup the local MedPerf server database: (make sure you are in the MedPerf's root directory)</li> </ul> <pre><code>cd server\nsh reset_db.sh\n</code></pre> <ul> <li>To cleanup the test storage:</li> </ul> <pre><code>rm -fr ~/.medperf/localhost_8000\n</code></pre>"},{"location":"getting_started/overview/","title":"Overview","text":"<p>The MedPerf client provides all the necessary tools to run a complete benchmark experiment. Below, you will find a comprehensive breakdown of user roles and the corresponding functionalities they can access and perform using the MedPerf client:</p> <ul> <li>Benchmark Committee: The Benchmark Commitee can define and create a benchmark, as well as manage experiments (e.g., approving which datasets and models will be allowed to participate)</li> <li>Model Owner: The Model Owner can submit a model to the MedPerf server and request participation in a benchmark.</li> <li>Data Owner: The Data Owner can prepare their raw medical data, register the metadata of their prepared dataset, request participation in a benchmark, execute a benchmark's models on their data, and submit the results of the execution.</li> </ul>"},{"location":"getting_started/setup/","title":"Setup","text":"<p>This setup is only for running the tutorials. If you are using MedPerf with a real benchmark and real experiments, skip to this section to optionally change your container runner. Then, follow the tutorials as a general guidance for your real experiments.</p>"},{"location":"getting_started/setup/#install-the-medperf-client","title":"Install the MedPerf Client","text":"<p>If this is your first time using MedPerf, install the MedPerf client library as described here.  </p>"},{"location":"getting_started/setup/#run-a-local-medperf-server","title":"Run a Local MedPerf Server","text":"<p>For this tutorial, you should spawn a local MedPerf server for the MedPerf client to communicate with. Note that this server will be hosted on your <code>localhost</code> and not on the internet.</p> <ol> <li> <p>Install the server requirements ensuring you are in MedPerf's root folder:</p> <pre><code>pip install -r server/requirements.txt\npip install -r server/test-requirements.txt\n</code></pre> </li> <li> <p>Run the local MedPerf server using the following command:</p> <pre><code>cd server\ncp .env.local.local-auth.sqlite .env\nsh setup-dev-server.sh\n</code></pre> </li> </ol> <p>The local MedPerf server now is ready to recieve requests. You can always stop the server by pressing <code>CTRL</code>+<code>C</code> in the terminal where you ran the server.</p> <p>After that, you will be configuring the MedPerf client to communicate with the local MedPerf server. Make sure you continue following the instructions in a new terminal.</p>"},{"location":"getting_started/setup/#configure-the-medperf-client","title":"Configure the MedPerf Client","text":"<p>The MedPerf client can be configured by creating or modifying \"<code>profiles</code>\". A profile is a set of configuration parameters used by the client during runtime. By default, the profile named <code>default</code> will be active.</p> <p>The <code>default</code> profile is preconfigured so that the client communicates with the main MedPerf server (api.medperf.org). For the purposes of the tutorial, you will be using the <code>local</code> profile as it is preconfigured so that the client communicates with the local MedPerf server.</p> <p>To activate the <code>local</code> profile, run the following command:</p> <pre><code>medperf profile activate local\n</code></pre> <p>You can always check which profile is active by running:</p> <pre><code>medperf profile ls\n</code></pre> <p>To view the current active profile's configured parameters, you can run the following:</p> <pre><code>medperf profile view\n</code></pre>"},{"location":"getting_started/setup/#choose-the-container-runner","title":"Choose the Container Runner","text":"<p>You can configure the MedPerf client to use either Docker or Singularity. The <code>local</code> profile is configured to use Docker. If you want to use MedPerf with Singularity, modify the <code>local</code> profile configured parameters by running the following:</p> <pre><code>medperf profile set --platform singularity\n</code></pre> <p>This command will modify the <code>platform</code> parameter of the currently activated profile.</p>"},{"location":"getting_started/setup/#whats-next","title":"What's Next?","text":"<p>The local MedPerf server now is ready to recieve requests, and the MedPerf client is ready to communicate. Depending on your role, you can follow these hands-on tutorials:</p> <ul> <li> <p>How a benchmark committee can create and submit a benchmark.</p> </li> <li> <p>How a model owner can submit a model.</p> </li> <li> <p>How a data owner can prepare their data and execute a benchmark.</p> </li> </ul>"},{"location":"getting_started/signup/","title":"Create your MedPerf Account","text":"<p>MedPerf uses passwordless authentication. This means that there will be no need for a password, and you have to access your email in order complete the signup process.</p> <p>Automatic signups are currently disabled. Please contact the MedPerf team in order to provision an account.</p> <p>Tip</p> <p>You don't need an account to run the tutorials and learn how to use the MedPerf client.</p>"},{"location":"getting_started/signup/#whats-next","title":"What's Next?","text":"<ul> <li>Install the MedPerf client</li> </ul>"},{"location":"getting_started/tutorials_overview/","title":"Overview","text":"<p>The tutorials simulate a benchmarking example for the task of detecting thoracic diseases from chest X-ray scans. You can find the description of the used data here. Throughout the tutorials, you will be interacting with a temporary local MedPerf server as described in the setup page. This allows you to freely experiment with the MedPerf client and rerun the tutorials as many times as you want, providing you with an immersive learning experience. Please note that these tutorials also serve as a general guidance to be followed when using the MedPerf client in a real scenario.</p> <p>Before proceeding to the tutorials, make sure you have the general tutorial environment set up.</p> <p>To ensure users have the best experience in learning the fundamentals of MedPerf and utilizing the MedPerf client, the following set of tutorials are provided:</p> Benchmark Committee <p>Click here to see the documentation specifically for benchmark owners.</p> Model Owner <p>Click here to see the documentation specifically for model owners.</p> Data Owner <p>Click here to see the documentation specifically for data owners.</p>"},{"location":"getting_started/shared/before_we_start/","title":"Macro Rendering Error","text":"<p>File: <code>getting_started/shared/before_we_start.md</code></p> <p>UndefinedError: 'dict object' has no attribute 'tutorial_id'</p> <pre><code>Traceback (most recent call last):\n  File \"/opt/hostedtoolcache/Python/3.9.23/x64/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 527, in render\n    return md_template.render(**page_variables)\n  File \"/opt/hostedtoolcache/Python/3.9.23/x64/lib/python3.9/site-packages/jinja2/environment.py\", line 1295, in render\n    self.environment.handle_exception()\n  File \"/opt/hostedtoolcache/Python/3.9.23/x64/lib/python3.9/site-packages/jinja2/environment.py\", line 942, in handle_exception\n    raise rewrite_traceback_stack(source=source)\n  File \"&lt;template&gt;\", line 41, in top-level template code\njinja2.exceptions.UndefinedError: 'dict object' has no attribute 'tutorial_id'\n</code></pre>"},{"location":"getting_started/shared/cleanup/","title":"Cleanup","text":""},{"location":"getting_started/shared/cleanup/#cleanup-optional","title":"Cleanup (Optional)","text":"<p>You have reached the end of the tutorial! If you are planning to rerun any of the tutorials, don't forget to cleanup:</p> <ul> <li> <p>To shut down the local MedPerf server: press <code>CTRL</code>+<code>C</code> in the terminal where the server is running.</p> </li> <li> <p>To cleanup the downloaded files workspace (make sure you are in the MedPerf's root directory):</p> </li> </ul> <pre><code>rm -fr medperf_tutorial\n</code></pre> <ul> <li>To cleanup the local MedPerf server database: (make sure you are in the MedPerf's root directory)</li> </ul> <pre><code>cd server\nsh reset_db.sh\n</code></pre> <ul> <li>To cleanup the test storage:</li> </ul> <pre><code>rm -fr ~/.medperf/localhost_8000\n</code></pre>"},{"location":"getting_started/shared/container_submission_overview/","title":"Container submission overview","text":"<p>The MedPerf server registers a container as metadata comprised of a set of assets that can be retrieved from the internet. This means that before submitting a container you have to host its assets on the internet. You can refer to this page if you want to understand what the assets are.</p>"},{"location":"getting_started/shared/redirect_to_hosting_files/","title":"Redirect to hosting files","text":""},{"location":"getting_started/shared/redirect_to_hosting_files/#host-the-files","title":"Host the Files","text":"<p>For the tutorial to run smoothly, the assets are already hosted. If you wish to host them by yourself, you can find the list of supported options and details about hosting files in this page.</p>"},{"location":"getting_started/shared/tutorials_content_overview/benchmark/","title":"Benchmark","text":"<p>In this tutorial we will create a benchmark that classifies chest X-Ray images.</p>"},{"location":"getting_started/shared/tutorials_content_overview/benchmark/#demo-data","title":"Demo Data","text":"<p>The <code>medperf_tutorial/demo_data/</code> folder contains the demo dataset content.</p> <ul> <li><code>images/</code> folder includes sample images.</li> <li><code>labels/labels.csv</code> provides a basic ground truth markup, indicating the class each image belongs to.</li> </ul> <p>The demo dataset is a sample dataset used for the development of your benchmark and used by Model Owners for the development of their models. More details are available in the section below</p>"},{"location":"getting_started/shared/tutorials_content_overview/benchmark/#data-preparator-container","title":"Data Preparator Container","text":"<p>The <code>medperf_tutorial/data_preparator/</code> contains a DataPreparator container that you must implement. This container:</p> <ul> <li>Transforms raw data into a format convenient for model consumption, such as converting DICOM images into numpy tensors, cropping patches, normalizing columns, etc. It's up to you to define the format that is handy for future models.</li> <li>Ensures its output is in a standardized format, allowing Model Owners/Developers to rely on its consistency.</li> </ul>"},{"location":"getting_started/shared/tutorials_content_overview/benchmark/#model-container","title":"Model Container","text":"<p>The <code>medperf_tutorial/model_custom_cnn/</code> is an example of a Model Container. You need to implement a reference model which will be used by data owners to test the compatibility of their data with your pipeline. Also, Model Developers joining your benchmark will follow the input/output specifications of this model when building their own models.</p>"},{"location":"getting_started/shared/tutorials_content_overview/benchmark/#metrics-container","title":"Metrics Container","text":"<p>The <code>medperf_tutorial/metrics/</code> houses a Metrics Container that processes ground truth data, model predictions, and computes performance metrics - such as classification accuracy, loss, etc. After a Dataset Owner runs the benchmark pipeline on their data, these final metric values will be shared with you as the Benchmark Owner.</p>"},{"location":"getting_started/shared/tutorials_content_overview/data/","title":"Data","text":""},{"location":"getting_started/shared/tutorials_content_overview/data/#tutorials-dataset-example","title":"Tutorial's Dataset Example","text":"<p>The <code>medperf_tutorial/sample_raw_data/</code> folder contains your data for the specified Benchmark. In this tutorial, where the benchmark involves classifying chest X-Ray images, your data comprises:</p> <ul> <li><code>images/</code> folder contains your images</li> <li><code>labels/labels.csv</code>, which provides the ground truth markup, specifying the class of each image.</li> </ul> <p>The format of this data is dictated by the Benchmark Owner, as it must be compatible with the benchmark's Data Preparation container. In a real-world scenario, the expected data format would differ from this toy example. Refer to the Benchmark Owner to get a format specifications and details for your practical case.</p> <p>As previously mentioned, your data itself never leaves your machine. During the dataset submission, only basic metadata is transferred, for which you will be prompted to confirm.</p>"},{"location":"getting_started/shared/tutorials_content_overview/model/","title":"Model","text":""},{"location":"getting_started/shared/tutorials_content_overview/model/#model-mlcube","title":"Model MLCube","text":"<p>The <code>medperf_tutorial/model_mobilenetv2/</code> is a toy Model container. Once you submit your model to the benchmark, all participating Data Owners would be able to run the model within the benchmark pipeline. Therefore, your container must support the specific input/output formats defined by the Benchmark Owners.</p> <p>For the purposes of this tutorial, you will work with a pre-prepared toy benchmark. In a real-world scenario,  you should refer to your Benchmark Owner to get a format specifications and details for your practical case.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>_version</li> <li>account_management<ul> <li>account_management</li> <li>token_storage<ul> <li>filesystem</li> <li>keyring_</li> </ul> </li> </ul> </li> <li>certificates</li> <li>cli</li> <li>commands<ul> <li>aggregator<ul> <li>aggregator</li> <li>associate</li> <li>run</li> <li>submit</li> </ul> </li> <li>association<ul> <li>approval</li> <li>association</li> <li>list</li> <li>priority</li> <li>utils</li> </ul> </li> <li>auth<ul> <li>auth</li> <li>login</li> <li>logout</li> <li>status</li> <li>synapse_login</li> </ul> </li> <li>benchmark<ul> <li>associate</li> <li>benchmark</li> <li>submit</li> <li>update_associations_poilcy</li> </ul> </li> <li>ca<ul> <li>associate</li> <li>ca</li> <li>submit</li> </ul> </li> <li>certificate<ul> <li>certificate</li> <li>client_certificate</li> <li>server_certificate</li> </ul> </li> <li>compatibility_test<ul> <li>compatibility_test</li> <li>run</li> <li>utils</li> <li>validate_params</li> </ul> </li> <li>dataset<ul> <li>associate</li> <li>associate_benchmark</li> <li>associate_training</li> <li>dataset</li> <li>export_dataset</li> <li>import_dataset</li> <li>prepare</li> <li>set_operational</li> <li>submit</li> <li>train</li> </ul> </li> <li>execution<ul> <li>create</li> <li>execution</li> <li>execution_flow</li> <li>show_local_results</li> <li>submit</li> <li>utils</li> </ul> </li> <li>list</li> <li>mlcube<ul> <li>associate</li> <li>create</li> <li>mlcube</li> <li>run_test</li> <li>submit</li> </ul> </li> <li>profile</li> <li>storage</li> <li>training<ul> <li>close_event</li> <li>get_experiment_status</li> <li>set_plan</li> <li>start_event</li> <li>submit</li> <li>training</li> <li>update_plan</li> </ul> </li> <li>view</li> </ul> </li> <li>comms<ul> <li>auth<ul> <li>auth0</li> <li>interface</li> <li>local</li> <li>token_verifier</li> </ul> </li> <li>entity_resources<ul> <li>resources</li> <li>sources<ul> <li>direct</li> <li>source</li> <li>synapse</li> </ul> </li> <li>utils</li> </ul> </li> <li>factory</li> <li>interface</li> <li>rest</li> </ul> </li> <li>config</li> <li>config_management<ul> <li>config_management</li> </ul> </li> <li>containers<ul> <li>parsers<ul> <li>factory</li> <li>mlcube</li> <li>parser</li> <li>simple_container</li> </ul> </li> <li>runners<ul> <li>docker_runner</li> <li>docker_utils</li> <li>factory</li> <li>runner</li> <li>singularity_runner</li> <li>singularity_utils</li> <li>utils</li> </ul> </li> </ul> </li> <li>decorators</li> <li>entities<ul> <li>aggregator</li> <li>benchmark</li> <li>ca</li> <li>cube</li> <li>dataset</li> <li>event</li> <li>execution</li> <li>interface</li> <li>report</li> <li>schemas</li> <li>training_exp</li> </ul> </li> <li>enums</li> <li>exceptions</li> <li>init</li> <li>storage<ul> <li>utils</li> </ul> </li> <li>ui<ul> <li>cli</li> <li>factory</li> <li>interface</li> <li>stdin</li> <li>web_ui</li> </ul> </li> <li>utils</li> <li>web_ui<ul> <li>api<ul> <li>routes</li> </ul> </li> <li>app</li> <li>auth</li> <li>benchmarks<ul> <li>routes</li> </ul> </li> <li>common</li> <li>containers<ul> <li>routes</li> </ul> </li> <li>datasets<ul> <li>routes</li> </ul> </li> <li>events</li> <li>medperf_login</li> <li>profiles</li> <li>security_check</li> <li>yaml_fetch<ul> <li>routes</li> </ul> </li> </ul> </li> <li>webui_main</li> </ul>"},{"location":"reference/_version/","title":"version","text":""},{"location":"reference/certificates/","title":"Certificates","text":""},{"location":"reference/certificates/#certificates.get_client_cert","title":"<code>get_client_cert(ca, email, output_path)</code>","text":"<p>Responsible for getting a user cert</p> Source code in <code>cli/medperf/certificates.py</code> <pre><code>def get_client_cert(ca: CA, email: str, output_path: str):\n\"\"\"Responsible for getting a user cert\"\"\"\ncommon_name = email\nca.prepare_config()\nmounts = {\n\"ca_config\": ca.config_path,\n\"pki_assets\": output_path,\n}\nenv = {\"MEDPERF_INPUT_CN\": common_name}\nmlcube = Cube.get(ca.client_mlcube)\nmlcube.download_run_files()\nmlcube.run(task=\"get_client_cert\", mounts=mounts, env=env, disable_network=False)\n</code></pre>"},{"location":"reference/certificates/#certificates.get_server_cert","title":"<code>get_server_cert(ca, address, output_path)</code>","text":"<p>Responsible for getting a server cert</p> Source code in <code>cli/medperf/certificates.py</code> <pre><code>def get_server_cert(ca: CA, address: str, output_path: str):\n\"\"\"Responsible for getting a server cert\"\"\"\ncommon_name = address\nca.prepare_config()\nmounts = {\n\"ca_config\": ca.config_path,\n\"pki_assets\": output_path,\n}\nenv = {\"MEDPERF_INPUT_CN\": common_name}\nmlcube = Cube.get(ca.server_mlcube)\nmlcube.download_run_files()\nmlcube.run(\ntask=\"get_server_cert\",\nmounts=mounts,\nenv=env,\nports=[\"0.0.0.0:80:80\"],\ndisable_network=False,\n)\n</code></pre>"},{"location":"reference/certificates/#certificates.trust","title":"<code>trust(ca)</code>","text":"<p>Verifies the CA cert fingerprint and writes it to the MedPerf storage. This is needed when running a workload, either by the users or by the aggregator</p> Source code in <code>cli/medperf/certificates.py</code> <pre><code>def trust(ca: CA):\n\"\"\"Verifies the CA cert fingerprint and writes it to the MedPerf storage.\n    This is needed when running a workload, either by the users or\n    by the aggregator\n    \"\"\"\nca.prepare_config()\nmounts = {\n\"ca_config\": ca.config_path,\n\"pki_assets\": ca.pki_assets,\n}\nmlcube = Cube.get(ca.ca_mlcube)\nmlcube.download_run_files()\nmlcube.run(task=\"trust\", mounts=mounts, disable_network=False)\n</code></pre>"},{"location":"reference/cli/","title":"Cli","text":""},{"location":"reference/cli/#cli.execute","title":"<code>execute(benchmark_uid=typer.Option(..., '--benchmark', '-b', help='UID of the desired benchmark'), data_uid=typer.Option(..., '--data_uid', '-d', help='Registered Dataset UID'), model_uid=typer.Option(..., '--model_uid', '-m', help='UID of model to execute'), approval=typer.Option(False, '-y', help='Skip approval step'), ignore_model_errors=typer.Option(False, '--ignore-model-errors', help='Ignore failing models, allowing for possibly submitting partial results'), no_cache=typer.Option(False, '--no-cache', help='Ignore existing results. The experiment then will be rerun'), new_result=typer.Option(False, '--new-result', help='Works if the result of the execution was already uploaded.This will rerun and create a new record.'))</code>","text":"<p>Runs the benchmark execution step for a given benchmark, prepared dataset and model</p> Source code in <code>cli/medperf/cli.py</code> <pre><code>@app.command(\"run\")\n@clean_except\ndef execute(\nbenchmark_uid: int = typer.Option(\n..., \"--benchmark\", \"-b\", help=\"UID of the desired benchmark\"\n),\ndata_uid: int = typer.Option(\n..., \"--data_uid\", \"-d\", help=\"Registered Dataset UID\"\n),\nmodel_uid: int = typer.Option(\n..., \"--model_uid\", \"-m\", help=\"UID of model to execute\"\n),\napproval: bool = typer.Option(False, \"-y\", help=\"Skip approval step\"),\nignore_model_errors: bool = typer.Option(\nFalse,\n\"--ignore-model-errors\",\nhelp=\"Ignore failing models, allowing for possibly submitting partial results\",\n),\nno_cache: bool = typer.Option(\nFalse,\n\"--no-cache\",\nhelp=\"Ignore existing results. The experiment then will be rerun\",\n),\nnew_result: bool = typer.Option(\nFalse,\n\"--new-result\",\nhelp=(\n\"Works if the result of the execution was already uploaded.\"\n\"This will rerun and create a new record.\"\n),\n),\n):\n\"\"\"Runs the benchmark execution step for a given benchmark, prepared dataset and model\"\"\"\nexecution = BenchmarkExecution.run(\nbenchmark_uid,\ndata_uid,\n[model_uid],\nignore_model_errors=ignore_model_errors,\nno_cache=no_cache,\nrerun_finalized_executions=new_result,\n)[0]\nResultSubmission.run(execution.id, approved=approval)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/cli/#cli.get_webui_props","title":"<code>get_webui_props()</code>","text":"<p>Prints necessary information to access an already-running medperf webui</p> Source code in <code>cli/medperf/cli.py</code> <pre><code>@app.command(\"get_webui_properties\")\n@clean_except\ndef get_webui_props():\n\"\"\"Prints necessary information to access an already-running medperf webui\"\"\"\nget_webui_properties()\n</code></pre>"},{"location":"reference/config/","title":"Config","text":""},{"location":"reference/decorators/","title":"Decorators","text":""},{"location":"reference/decorators/#decorators.add_inline_parameters","title":"<code>add_inline_parameters(func)</code>","text":"<p>Decorator that adds common configuration options to a typer command</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>function to be decorated</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>decorated function</p> Source code in <code>cli/medperf/decorators.py</code> <pre><code>def add_inline_parameters(func: Callable) -&gt; Callable:\n\"\"\"Decorator that adds common configuration options to a typer command\n    Args:\n        func (Callable): function to be decorated\n    Returns:\n        Callable: decorated function\n    \"\"\"\n# NOTE: changing parameters here should be accompanied\n#       by changing config.inline_parameters\n@merge_args(func)\ndef wrapper(\n*args,\nloglevel: str = typer.Option(\nconfig.loglevel,\n\"--loglevel\",\nhelp=\"Logging level [debug | info | warning | error]\",\n),\nprepare_timeout: int = typer.Option(\nconfig.prepare_timeout,\n\"--prepare_timeout\",\nhelp=\"Maximum time in seconds before interrupting prepare task\",\n),\nsanity_check_timeout: int = typer.Option(\nconfig.sanity_check_timeout,\n\"--sanity_check_timeout\",\nhelp=\"Maximum time in seconds before interrupting sanity_check task\",\n),\nstatistics_timeout: int = typer.Option(\nconfig.statistics_timeout,\n\"--statistics_timeout\",\nhelp=\"Maximum time in seconds before interrupting statistics task\",\n),\ninfer_timeout: int = typer.Option(\nconfig.infer_timeout,\n\"--infer_timeout\",\nhelp=\"Maximum time in seconds before interrupting infer task\",\n),\nevaluate_timeout: int = typer.Option(\nconfig.evaluate_timeout,\n\"--evaluate_timeout\",\nhelp=\"Maximum time in seconds before interrupting evaluate task\",\n),\ncontainer_loglevel: str = typer.Option(\nconfig.container_loglevel,\n\"--container-loglevel\",\nhelp=\"Logging level for containers to be run [debug | info | warning | error]\",\n),\nplatform: str = typer.Option(\nconfig.platform,\n\"--platform\",\nhelp=\"Platform to use for MLCube. [docker | singularity]\",\n),\ngpus: str = typer.Option(\nconfig.gpus,\n\"--gpus\",\nhelp=\"\"\"\n            What GPUs to expose to MLCube.\n            Accepted Values are:\\n\n            - \"\" or 0: to expose no GPUs (e.g.: --gpus=\"\")\\n\n            - \"all\": to expose all GPUs. (e.g.: --gpus=all)\\n\n            - an integer: to expose a certain number of GPUs. ONLY AVAILABLE FOR DOCKER\n            (e.g., --gpus=2 to expose 2 GPUs)\\n\n            - Form \"device=&lt;id1&gt;,&lt;id2&gt;\": to expose specific GPUs.\n            (e.g., --gpus=\"device=0,2\")\\n\"\"\",\n),\nshm_size: str = typer.Option(\nconfig.shm_size,\n\"--shm-size\",\nhelp=\"\"\"\n            Only for Docker. See --shm-size argument\n            in docker run: https://docs.docker.com/engine/containers/run/\"\"\",\n),\ncleanup: bool = typer.Option(\nconfig.cleanup,\n\"--cleanup/--no-cleanup\",\nhelp=\"Whether to clean up temporary medperf storage after execution\",\n),\n**kwargs,\n):\nreturn func(*args, **kwargs)\nreturn wrapper\n</code></pre>"},{"location":"reference/decorators/#decorators.clean_except","title":"<code>clean_except(func)</code>","text":"<p>Decorator for handling errors. It allows logging and cleaning the project's directory before throwing the error.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>Function to handle for errors</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>Decorated function</p> Source code in <code>cli/medperf/decorators.py</code> <pre><code>def clean_except(func: Callable) -&gt; Callable:\n\"\"\"Decorator for handling errors. It allows logging\n    and cleaning the project's directory before throwing the error.\n    Args:\n        func (Callable): Function to handle for errors\n    Returns:\n        Callable: Decorated function\n    \"\"\"\n@functools.wraps(func)\ndef wrapper(*args, **kwargs):\ntry:\nlogging.info(f\"Running function '{func.__name__}'\")\nfunc(*args, **kwargs)\nexcept CleanExit as e:\nlogging.info(str(e))\nconfig.ui.print(str(e))\nsys.exit(e.medperf_status_code)\nexcept MedperfException as e:\nlogging.exception(e)\npretty_error(str(e))\nsys.exit(1)\nexcept Exception as e:\nlogging.error(\"An unexpected error occured. Terminating.\")\nlogging.exception(e)\nraise e\nfinally:\npackage_logs()\ncleanup()\nreturn wrapper\n</code></pre>"},{"location":"reference/decorators/#decorators.configurable","title":"<code>configurable(func)</code>","text":"<p>Decorator that adds common configuration options to a typer command</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>function to be decorated</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>decorated function</p> Source code in <code>cli/medperf/decorators.py</code> <pre><code>def configurable(func: Callable) -&gt; Callable:\n\"\"\"Decorator that adds common configuration options to a typer command\n    Args:\n        func (Callable): function to be decorated\n    Returns:\n        Callable: decorated function\n    \"\"\"\n# NOTE: changing parameters here should be accompanied\n#       by changing configurable_parameters\n@merge_args(func)\ndef wrapper(\n*args,\nserver: str = typer.Option(\nconfig.server, \"--server\", help=\"URL of a hosted MedPerf API instance\"\n),\nauth_class: str = typer.Option(\nconfig.auth_class,\n\"--auth_class\",\nhelp=\"Authentication interface to use [Auth0]\",\n),\nauth_domain: str = typer.Option(\nconfig.auth_domain, \"--auth_domain\", help=\"Auth0 domain name\"\n),\nauth_jwks_url: str = typer.Option(\nconfig.auth_jwks_url, \"--auth_jwks_url\", help=\"Auth0 Json Web Key set URL\"\n),\nauth_idtoken_issuer: str = typer.Option(\nconfig.auth_idtoken_issuer,\n\"--auth_idtoken_issuer\",\nhelp=\"Auth0 ID token issuer\",\n),\nauth_client_id: str = typer.Option(\nconfig.auth_client_id, \"--auth_client_id\", help=\"Auth0 client ID\"\n),\nauth_audience: str = typer.Option(\nconfig.auth_audience,\n\"--auth_audience\",\nhelp=\"Server's Auth0 API identifier\",\n),\ncertificate: str = typer.Option(\nconfig.certificate, \"--certificate\", help=\"path to a valid SSL certificate\"\n),\nloglevel: str = typer.Option(\nconfig.loglevel,\n\"--loglevel\",\nhelp=\"Logging level [debug | info | warning | error]\",\n),\nprepare_timeout: int = typer.Option(\nconfig.prepare_timeout,\n\"--prepare_timeout\",\nhelp=\"Maximum time in seconds before interrupting prepare task\",\n),\nsanity_check_timeout: int = typer.Option(\nconfig.sanity_check_timeout,\n\"--sanity_check_timeout\",\nhelp=\"Maximum time in seconds before interrupting sanity_check task\",\n),\nstatistics_timeout: int = typer.Option(\nconfig.statistics_timeout,\n\"--statistics_timeout\",\nhelp=\"Maximum time in seconds before interrupting statistics task\",\n),\ninfer_timeout: int = typer.Option(\nconfig.infer_timeout,\n\"--infer_timeout\",\nhelp=\"Maximum time in seconds before interrupting infer task\",\n),\nevaluate_timeout: int = typer.Option(\nconfig.evaluate_timeout,\n\"--evaluate_timeout\",\nhelp=\"Maximum time in seconds before interrupting evaluate task\",\n),\ncontainer_loglevel: str = typer.Option(\nconfig.container_loglevel,\n\"--container-loglevel\",\nhelp=\"Logging level for containers to be run [debug | info | warning | error]\",\n),\nplatform: str = typer.Option(\nconfig.platform,\n\"--platform\",\nhelp=\"Platform to use for MLCube. [docker | singularity]\",\n),\ngpus: str = typer.Option(\nconfig.gpus,\n\"--gpus\",\nhelp=\"\"\"\n            What GPUs to expose to MLCube.\n            Accepted Values are comma separated GPU IDs (e.g \"1,2\"), or \\\"all\\\".\n            MLCubes that aren't configured to use GPUs won't be affected by this.\n            Defaults to all available GPUs\"\"\",\n),\ncleanup: bool = typer.Option(\nconfig.cleanup,\n\"--cleanup/--no-cleanup\",\nhelp=\"Wether to clean up temporary medperf storage after execution\",\n),\n**kwargs,\n):\nreturn func(*args, **kwargs)\nreturn wrapper\n</code></pre>"},{"location":"reference/enums/","title":"Enums","text":""},{"location":"reference/exceptions/","title":"Exceptions","text":""},{"location":"reference/exceptions/#exceptions.AuthenticationError","title":"<code>AuthenticationError</code>","text":"<p>             Bases: <code>MedperfException</code></p> <p>Raised when authentication can't be processed</p> Source code in <code>cli/medperf/exceptions.py</code> <pre><code>class AuthenticationError(MedperfException):\n\"\"\"Raised when authentication can't be processed\"\"\"\n</code></pre>"},{"location":"reference/exceptions/#exceptions.CleanExit","title":"<code>CleanExit</code>","text":"<p>             Bases: <code>MedperfException</code></p> <p>Raised when Medperf needs to stop for non erroneous reasons</p> Source code in <code>cli/medperf/exceptions.py</code> <pre><code>class CleanExit(MedperfException):\n\"\"\"Raised when Medperf needs to stop for non erroneous reasons\"\"\"\ndef __init__(self, *args, medperf_status_code=0) -&gt; None:\nsuper().__init__(*args)\nself.medperf_status_code = medperf_status_code\n</code></pre>"},{"location":"reference/exceptions/#exceptions.CommunicationAuthenticationError","title":"<code>CommunicationAuthenticationError</code>","text":"<p>             Bases: <code>CommunicationError</code></p> <p>Raised when the communication interface can't handle an authentication request</p> Source code in <code>cli/medperf/exceptions.py</code> <pre><code>class CommunicationAuthenticationError(CommunicationError):\n\"\"\"Raised when the communication interface can't handle an authentication request\"\"\"\n</code></pre>"},{"location":"reference/exceptions/#exceptions.CommunicationError","title":"<code>CommunicationError</code>","text":"<p>             Bases: <code>MedperfException</code></p> <p>Raised when an error happens due to the communication interface</p> Source code in <code>cli/medperf/exceptions.py</code> <pre><code>class CommunicationError(MedperfException):\n\"\"\"Raised when an error happens due to the communication interface\"\"\"\n</code></pre>"},{"location":"reference/exceptions/#exceptions.CommunicationRequestError","title":"<code>CommunicationRequestError</code>","text":"<p>             Bases: <code>CommunicationError</code></p> <p>Raised when the communication interface can't handle a request appropiately</p> Source code in <code>cli/medperf/exceptions.py</code> <pre><code>class CommunicationRequestError(CommunicationError):\n\"\"\"Raised when the communication interface can't handle a request appropiately\"\"\"\n</code></pre>"},{"location":"reference/exceptions/#exceptions.CommunicationRetrievalError","title":"<code>CommunicationRetrievalError</code>","text":"<p>             Bases: <code>CommunicationError</code></p> <p>Raised when the communication interface can't retrieve an element</p> Source code in <code>cli/medperf/exceptions.py</code> <pre><code>class CommunicationRetrievalError(CommunicationError):\n\"\"\"Raised when the communication interface can't retrieve an element\"\"\"\n</code></pre>"},{"location":"reference/exceptions/#exceptions.ExecutionError","title":"<code>ExecutionError</code>","text":"<p>             Bases: <code>MedperfException</code></p> <p>Raised when an execution component fails</p> Source code in <code>cli/medperf/exceptions.py</code> <pre><code>class ExecutionError(MedperfException):\n\"\"\"Raised when an execution component fails\"\"\"\n</code></pre>"},{"location":"reference/exceptions/#exceptions.InvalidArgumentError","title":"<code>InvalidArgumentError</code>","text":"<p>             Bases: <code>MedperfException</code></p> <p>Raised when an argument or set of arguments are consided invalid</p> Source code in <code>cli/medperf/exceptions.py</code> <pre><code>class InvalidArgumentError(MedperfException):\n\"\"\"Raised when an argument or set of arguments are consided invalid\"\"\"\n</code></pre>"},{"location":"reference/exceptions/#exceptions.InvalidContainerSpec","title":"<code>InvalidContainerSpec</code>","text":"<p>             Bases: <code>MedperfException</code></p> <p>Raised when container config file is invalid</p> Source code in <code>cli/medperf/exceptions.py</code> <pre><code>class InvalidContainerSpec(MedperfException):\n\"\"\"Raised when container config file is invalid\"\"\"\n</code></pre>"},{"location":"reference/exceptions/#exceptions.InvalidEntityError","title":"<code>InvalidEntityError</code>","text":"<p>             Bases: <code>MedperfException</code></p> <p>Raised when an entity is considered invalid</p> Source code in <code>cli/medperf/exceptions.py</code> <pre><code>class InvalidEntityError(MedperfException):\n\"\"\"Raised when an entity is considered invalid\"\"\"\n</code></pre>"},{"location":"reference/exceptions/#exceptions.MedperfException","title":"<code>MedperfException</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Medperf base exception</p> Source code in <code>cli/medperf/exceptions.py</code> <pre><code>class MedperfException(Exception):\n\"\"\"Medperf base exception\"\"\"\n</code></pre>"},{"location":"reference/init/","title":"Init","text":""},{"location":"reference/utils/","title":"Utils","text":""},{"location":"reference/utils/#utils.approval_prompt","title":"<code>approval_prompt(msg)</code>","text":"<p>Helper function for prompting the user for things they have to explicitly approve.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>What message to ask the user for approval.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Wether the user explicitly approved or not.</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def approval_prompt(msg: str) -&gt; bool:\n\"\"\"Helper function for prompting the user for things they have to explicitly approve.\n    Args:\n        msg (str): What message to ask the user for approval.\n    Returns:\n        bool: Wether the user explicitly approved or not.\n    \"\"\"\nlogging.info(\"Prompting for user's approval\")\nui = config.ui\napproval = None\nwhile approval is None or approval not in \"yn\":\napproval = ui.prompt(msg.strip() + \" \").lower()\nlogging.info(f\"User answered approval with {approval}\")\nreturn approval == \"y\"\n</code></pre>"},{"location":"reference/utils/#utils.check_for_updates","title":"<code>check_for_updates()</code>","text":"<p>Check if the current branch is up-to-date with its remote counterpart using GitPython.</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def check_for_updates() -&gt; None:\n\"\"\"Check if the current branch is up-to-date with its remote counterpart using GitPython.\"\"\"\nrepo = Repo(config.BASE_DIR)\nif repo.bare:\nlogging.debug(\"Repo is bare\")\nreturn\nlogging.debug(f\"Current git commit: {repo.head.commit.hexsha}\")\ntry:\nfor remote in repo.remotes:\nremote.fetch()\nif repo.head.is_detached:\nlogging.debug(\"Repo is in detached state\")\nreturn\ncurrent_branch = repo.active_branch\ntracking_branch = current_branch.tracking_branch()\nif tracking_branch is None:\nlogging.debug(\"Current branch does not track a remote branch.\")\nreturn\nif current_branch.commit.hexsha == tracking_branch.commit.hexsha:\nlogging.debug(\"No git branch updates.\")\nreturn\nlogging.debug(\nf\"Git branch updates found: {current_branch.commit.hexsha} -&gt; {tracking_branch.commit.hexsha}\"\n)\nconfig.ui.print_warning(\n\"MedPerf client updates found. Please, update your MedPerf installation.\"\n)\nexcept GitCommandError as e:\nlogging.debug(\n\"Exception raised during updates check. Maybe user checked out repo with git@ and private key\"\n\" or repo is in detached / non-tracked state?\"\n)\nlogging.debug(e)\n</code></pre>"},{"location":"reference/utils/#utils.cleanup","title":"<code>cleanup()</code>","text":"<p>Removes clutter and unused files from the medperf folder structure.</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def cleanup():\n\"\"\"Removes clutter and unused files from the medperf folder structure.\"\"\"\nif not config.cleanup:\nlogging.info(\"Cleanup disabled\")\nreturn\nfor path in config.tmp_paths:\nremove_path(path)\ntrash_folder = config.trash_folder\nif os.path.exists(trash_folder) and os.listdir(trash_folder):\nmsg = \"WARNING: Failed to premanently cleanup some files. Consider deleting\"\nmsg += f\" '{trash_folder}' manually to avoid unnecessary storage.\"\nconfig.ui.print_warning(msg)\n</code></pre>"},{"location":"reference/utils/#utils.combine_proc_sp_text","title":"<code>combine_proc_sp_text(proc)</code>","text":"<p>Combines the output of a process and the spinner. Joins any string captured from the process with the spinner current text. Any strings ending with any other character from the subprocess will be returned later.</p> <p>Parameters:</p> Name Type Description Default <code>proc</code> <code>spawn</code> <p>a pexpect spawned child</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>all non-carriage-return-ending string captured from proc</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def combine_proc_sp_text(proc: spawn) -&gt; str:\n\"\"\"Combines the output of a process and the spinner.\n    Joins any string captured from the process with the\n    spinner current text. Any strings ending with any other\n    character from the subprocess will be returned later.\n    Args:\n        proc (spawn): a pexpect spawned child\n    Returns:\n        str: all non-carriage-return-ending string captured from proc\n    \"\"\"\nui = config.ui\nproc_out = \"\"\nbreak_ = False\nlog_filter = _MLCubeOutputFilter(proc.pid)\nwhile not break_:\nif not proc.isalive():\nbreak_ = True\ntry:\nline = proc.readline()\nexcept TIMEOUT:\nlogging.error(\"Process timed out\")\nlogging.debug(proc_out)\nraise ExecutionError(\"Process timed out\")\nline = line.decode(\"utf-8\", \"ignore\")\nif not line:\ncontinue\n# Always log each line just in case the final proc_out\n# wasn't logged for some reason\nlogging.debug(line)\nproc_out += line\nif not log_filter.check_line(line):\nui.print(f\"{Fore.WHITE}{Style.DIM}{line.strip()}{Style.RESET_ALL}\")\nlogging.debug(\"Container process finished\")\nlogging.debug(proc_out)\nreturn proc_out\n</code></pre>"},{"location":"reference/utils/#utils.dict_pretty_print","title":"<code>dict_pretty_print(in_dict, skip_none_values=True)</code>","text":"<p>Helper function for distinctively printing dictionaries with yaml format.</p> <p>Parameters:</p> Name Type Description Default <code>in_dict</code> <code>dict</code> <p>dictionary to print</p> required <code>skip_none_values</code> <code>bool</code> <p>if fields with <code>None</code> values should be omitted</p> <code>True</code> Source code in <code>cli/medperf/utils.py</code> <pre><code>def dict_pretty_print(in_dict: dict, skip_none_values: bool = True):\n\"\"\"Helper function for distinctively printing dictionaries with yaml format.\n    Args:\n        in_dict (dict): dictionary to print\n        skip_none_values (bool): if fields with `None` values should be omitted\n    \"\"\"\nlogging.debug(f\"Printing dictionary to the user: {in_dict}\")\nyaml_dict = make_pretty_dict(in_dict, skip_none_values)\nconfig.ui.print_yaml(yaml_dict)\nlogging.debug(f\"Dictionary printed to the user: {in_dict}\")\n</code></pre>"},{"location":"reference/utils/#utils.format_errors_dict","title":"<code>format_errors_dict(errors_dict)</code>","text":"<p>Reformats the error details from a field-error(s) dictionary into a human-readable string for printing</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def format_errors_dict(errors_dict: dict):\n\"\"\"Reformats the error details from a field-error(s) dictionary into a human-readable string for printing\"\"\"\nerror_msg = \"\"\nfor field, errors in errors_dict.items():\nerror_msg += \"\\n\"\nif isinstance(field, tuple):\nfield = field[0]\nerror_msg += f\"- {field}: \"\nif isinstance(errors, str):\nerror_msg += errors\nelif len(errors) == 1:\n# If a single error for a field is given, don't create a sublist\nerror_msg += str(errors[0])\nelse:\n# Create a sublist otherwise\nfor e_msg in errors:\nerror_msg += \"\\n\"\nerror_msg += f\"\\t- {e_msg}\"\nreturn error_msg\n</code></pre>"},{"location":"reference/utils/#utils.generate_tmp_path","title":"<code>generate_tmp_path()</code>","text":"<p>Generates a temporary path by means of getting the current timestamp with a random salt</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>generated temporary path</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def generate_tmp_path() -&gt; str:\n\"\"\"Generates a temporary path by means of getting the current timestamp\n    with a random salt\n    Returns:\n        str: generated temporary path\n    \"\"\"\ntmp_path = os.path.join(config.tmp_folder, generate_tmp_uid())\nconfig.tmp_paths.append(tmp_path)\nreturn tmp_path\n</code></pre>"},{"location":"reference/utils/#utils.generate_tmp_uid","title":"<code>generate_tmp_uid()</code>","text":"<p>Generates a temporary uid by means of getting the current timestamp with a random salt</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>generated temporary uid</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def generate_tmp_uid() -&gt; str:\n\"\"\"Generates a temporary uid by means of getting the current timestamp\n    with a random salt\n    Returns:\n        str: generated temporary uid\n    \"\"\"\ndt = datetime.utcnow()\nts_int = int(datetime.timestamp(dt))\nsalt = random.randint(-ts_int, ts_int)\nts = str(ts_int + salt)\nreturn ts\n</code></pre>"},{"location":"reference/utils/#utils.get_file_hash","title":"<code>get_file_hash(path)</code>","text":"<p>Calculates the sha256 hash for a given file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Location of the file of interest.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Calculated hash</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def get_file_hash(path: str) -&gt; str:\n\"\"\"Calculates the sha256 hash for a given file.\n    Args:\n        path (str): Location of the file of interest.\n    Returns:\n        str: Calculated hash\n    \"\"\"\nlogging.debug(\"Calculating hash for file {}\".format(path))\nBUF_SIZE = 65536\nsha = hashlib.sha256()\nwith open(path, \"rb\") as f:\nwhile True:\ndata = f.read(BUF_SIZE)\nif not data:\nbreak\nsha.update(data)\nsha_val = sha.hexdigest()\nlogging.debug(f\"Hash for file {path}: {sha_val}\")\nreturn sha_val\n</code></pre>"},{"location":"reference/utils/#utils.get_folders_hash","title":"<code>get_folders_hash(paths)</code>","text":"<p>Generates a hash for all the contents of the fiven folders. This procedure hashes all the files in all passed folders, sorts them and then hashes that list.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>List(str</code> <p>Folders to hash.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>sha256 hash that represents all the folders altogether</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def get_folders_hash(paths: List[str]) -&gt; str:\n\"\"\"Generates a hash for all the contents of the fiven folders. This procedure\n    hashes all the files in all passed folders, sorts them and then hashes that list.\n    Args:\n        paths List(str): Folders to hash.\n    Returns:\n        str: sha256 hash that represents all the folders altogether\n    \"\"\"\nhashes = []\n# The hash doesn't depend on the order of paths or folders, as the hashes get sorted after the fact\nfor path in paths:\nfor root, _, files in os.walk(path, topdown=False):\nfor file in files:\nlogging.debug(f\"Hashing file {file}\")\nfilepath = os.path.join(root, file)\nhashes.append(get_file_hash(filepath))\nhashes = sorted(hashes)\nsha = hashlib.sha256()\nfor hash in hashes:\nsha.update(hash.encode(\"utf-8\"))\nhash_val = sha.hexdigest()\nlogging.debug(f\"Folder hash: {hash_val}\")\nreturn hash_val\n</code></pre>"},{"location":"reference/utils/#utils.get_uids","title":"<code>get_uids(path)</code>","text":"<p>Retrieves the UID of all the elements in the specified path.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: UIDs of objects in path.</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def get_uids(path: str) -&gt; List[str]:\n\"\"\"Retrieves the UID of all the elements in the specified path.\n    Returns:\n        List[str]: UIDs of objects in path.\n    \"\"\"\nlogging.debug(\"Retrieving datasets\")\nuids = next(os.walk(path))[1]\nlogging.debug(f\"Found {len(uids)} datasets\")\nlogging.debug(f\"Datasets: {uids}\")\nreturn uids\n</code></pre>"},{"location":"reference/utils/#utils.make_pretty_dict","title":"<code>make_pretty_dict(in_dict, skip_none_values=True)</code>","text":"<p>Helper function for distinctively creating dict string with yaml format.</p> <p>Parameters:</p> Name Type Description Default <code>in_dict</code> <code>dict</code> <p>dictionary to convert to yaml string</p> required <code>skip_none_values</code> <code>bool</code> <p>if fields with <code>None</code> values should be omitted</p> <code>True</code> Source code in <code>cli/medperf/utils.py</code> <pre><code>def make_pretty_dict(in_dict: dict, skip_none_values: bool = True):\n\"\"\"Helper function for distinctively creating dict string with yaml format.\n    Args:\n        in_dict (dict): dictionary to convert to yaml string\n        skip_none_values (bool): if fields with `None` values should be omitted\n    \"\"\"\nif skip_none_values:\nin_dict = {k: v for (k, v) in in_dict.items() if v is not None}\nyaml_dict = yaml.dump(in_dict)\nreturn yaml_dict\n</code></pre>"},{"location":"reference/utils/#utils.move_folder","title":"<code>move_folder(src, dest)</code>","text":"<p>Recursively moves a folder from {src} to {dest}</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>str</code> <p>Path of the source folder to be moved</p> required <code>dest</code> <code>src</code> <p>Path of the destination that the folder will be moved to</p> required Source code in <code>cli/medperf/utils.py</code> <pre><code>def move_folder(src: str, dest: str) -&gt; None:\n\"\"\"Recursively moves a folder from {src} to {dest}\n    Args:\n        src (str): Path of the source folder to be moved\n        dest (src): Path of the destination that the folder will be moved to\n    \"\"\"\nshutil.move(src, dest)\nlogging.info(f\"Folder moved: {src} to {dest}\")\n</code></pre>"},{"location":"reference/utils/#utils.pretty_error","title":"<code>pretty_error(msg)</code>","text":"<p>Prints an error message with typer protocol</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>Error message to show to the user</p> required Source code in <code>cli/medperf/utils.py</code> <pre><code>def pretty_error(msg: str):\n\"\"\"Prints an error message with typer protocol\n    Args:\n        msg (str): Error message to show to the user\n    \"\"\"\nui = config.ui\nlogging.warning(\n\"MedPerf had to stop execution. See logs above for more information\"\n)\nif msg[-1] != \".\":\nmsg = msg + \".\"\nui.print_error(msg)\n</code></pre>"},{"location":"reference/utils/#utils.remove_path","title":"<code>remove_path(path)</code>","text":"<p>Cleans up a clutter object. In case of failure, it is moved to <code>.trash</code></p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def remove_path(path):\n\"\"\"Cleans up a clutter object. In case of failure, it is moved to `.trash`\"\"\"\n# NOTE: We assume medperf will always have permissions to unlink\n# and rename clutter paths, since for now they are expected to live\n# in folders owned by medperf\nif not os.path.exists(path):\nreturn\nlogging.info(f\"Removing clutter path: {path}\")\n# Don't delete symlinks\nif os.path.islink(path):\nos.unlink(path)\nreturn\ntry:\nif os.path.isfile(path):\nos.remove(path)\nelse:\nshutil.rmtree(path)\nexcept OSError as e:\nlogging.error(f\"Could not remove {path}: {str(e)}\")\nmove_to_trash(path)\n</code></pre>"},{"location":"reference/utils/#utils.sanitize_json","title":"<code>sanitize_json(data)</code>","text":"<p>Makes sure the input data is JSON compliant.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>dictionary containing data to be represented as JSON.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>sanitized dictionary</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def sanitize_json(data: dict) -&gt; dict:\n\"\"\"Makes sure the input data is JSON compliant.\n    Args:\n        data (dict): dictionary containing data to be represented as JSON.\n    Returns:\n        dict: sanitized dictionary\n    \"\"\"\njson_string = json.dumps(data)\njson_string = re.sub(r\"\\bNaN\\b\", '\"nan\"', json_string)\njson_string = re.sub(r\"(-?)\\bInfinity\\b\", r'\"\\1Infinity\"', json_string)\ndata = json.loads(json_string)\nreturn data\n</code></pre>"},{"location":"reference/utils/#utils.tar","title":"<code>tar(filepath, folders_paths, folder_prefix=None)</code>","text":"<p>Tars the tar.gz file</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path where the tar.gz file will be saved.</p> required <code>folder_path</code> <code>str</code> <p>Path of the data should be compressed.</p> required <code>folder_prefix</code> <code>str</code> <p>new folder name that will contain the</p> <code>None</code> Source code in <code>cli/medperf/utils.py</code> <pre><code>def tar(filepath: str, folders_paths: List[str], folder_prefix: str = None) -&gt; None:\n\"\"\"Tars the tar.gz file\n    Args:\n        filepath (str): Path where the tar.gz file will be saved.\n        folder_path (str): Path of the data should be compressed.\n        folder_prefix (str): new folder name that will contain the\n        compressed files in the tar file. (Default=\"\")\n    \"\"\"\nif os.path.exists(filepath):\nraise InvalidArgumentError(f\"{filepath} already exists.\")\nlogging.info(f\"Compressing tar.gz at {filepath}\")\ntar_arc = tarfile.open(filepath, \"w:gz\")\nfor folder in folders_paths:\nif folder_prefix:\narcname = os.path.join(folder_prefix, os.path.basename(folder))\nelse:\narcname = os.path.basename(folder)\ntar_arc.add(folder, arcname=arcname)\nlogging.info(f\"Compressing tar.gz at {filepath}: {folder} Added.\")\ntar_arc.close()\n</code></pre>"},{"location":"reference/utils/#utils.untar","title":"<code>untar(filepath, remove=True, extract_to=None)</code>","text":"<p>Untars and optionally removes the tar.gz file</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path where the tar.gz file can be found.</p> required <code>remove</code> <code>bool</code> <p>Whether to delete the tar.gz file. Defaults to True.</p> <code>True</code> <code>extract_to</code> <code>str</code> <p>Where to extract the tar.gz file. Defaults to parent directory</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>location where the untared files can be found.</p> Source code in <code>cli/medperf/utils.py</code> <pre><code>def untar(filepath: str, remove: bool = True, extract_to: str = None) -&gt; str:\n\"\"\"Untars and optionally removes the tar.gz file\n    Args:\n        filepath (str): Path where the tar.gz file can be found.\n        remove (bool): Whether to delete the tar.gz file. Defaults to True.\n        extract_to (str): Where to extract the tar.gz file. Defaults to parent directory\n    Returns:\n        str: location where the untared files can be found.\n    \"\"\"\nlogging.info(f\"Uncompressing tar.gz at {filepath}\")\naddpath = extract_to or str(Path(filepath).parent)\ntry:\ntar = tarfile.open(filepath)\ntar.extractall(addpath)\ntar.close()\nexcept tarfile.ReadError as e:\nraise ExecutionError(\"Cannot extract tar.gz file, \" + e.__str__())\n# OS Specific issue: Mac Creates superfluous files with tarfile library\n[\nremove_path(spurious_file)\nfor spurious_file in glob(addpath + \"/**/._*\", recursive=True)\n]\nif remove:\nlogging.info(f\"Deleting {filepath}\")\nremove_path(filepath)\nreturn addpath\n</code></pre>"},{"location":"reference/webui_main/","title":"Webui main","text":""},{"location":"reference/account_management/account_management/","title":"Account management","text":""},{"location":"reference/account_management/account_management/#account_management.account_management.get_medperf_user_data","title":"<code>get_medperf_user_data()</code>","text":"<p>Return cached medperf user data. Get from the server if not found</p> Source code in <code>cli/medperf/account_management/account_management.py</code> <pre><code>def get_medperf_user_data():\n\"\"\"Return cached medperf user data. Get from the server if not found\"\"\"\nconfig_p = read_config()\nif config.credentials_keyword not in config_p.active_profile:\nraise AuthenticationError(\"You are not logged in\")\nmedperf_user = config_p.active_profile[config.credentials_keyword].get(\n\"medperf_user\", None\n)\nif medperf_user is None:\nmedperf_user = set_medperf_user_data()\nreturn medperf_user\n</code></pre>"},{"location":"reference/account_management/account_management/#account_management.account_management.set_medperf_user_data","title":"<code>set_medperf_user_data()</code>","text":"<p>Get and cache user data from the MedPerf server</p> Source code in <code>cli/medperf/account_management/account_management.py</code> <pre><code>def set_medperf_user_data():\n\"\"\"Get and cache user data from the MedPerf server\"\"\"\nconfig_p = read_config()\nmedperf_user = config.comms.get_current_user()\nconfig_p.active_profile[config.credentials_keyword][\"medperf_user\"] = medperf_user\nwrite_config(config_p)\nreturn medperf_user\n</code></pre>"},{"location":"reference/account_management/token_storage/filesystem/","title":"Filesystem","text":""},{"location":"reference/account_management/token_storage/keyring_/","title":"Keyring","text":"<p>Keyring token storage is NOT used. We used it before this commit but users who connect to remote machines through passwordless SSH faced some issues.</p>"},{"location":"reference/commands/list/","title":"List","text":""},{"location":"reference/commands/list/#commands.list.EntityList","title":"<code>EntityList</code>","text":"Source code in <code>cli/medperf/commands/list.py</code> <pre><code>class EntityList:\n@staticmethod\ndef run(\nentity_class: Type[Entity],\nfields: List[str],\nunregistered: bool = False,\nmine_only: bool = False,\n**kwargs,\n):\n\"\"\"Lists all local datasets\n        Args:\n            entity_class: entity class to instantiate (Dataset, Model, etc.)\n            fields (list[str]): list of fields to display\n            unregistered (bool, optional): Display only local unregistered results. Defaults to False.\n            mine_only (bool, optional): Display all registered current-user results. Defaults to False.\n            kwargs (dict): Additional parameters for filtering entity lists. Keys with None will be filtered out.\n        \"\"\"\nkwargs = {k: v for k, v in kwargs.items() if v is not None}\nentity_list = EntityList(\nentity_class, fields, unregistered, mine_only, **kwargs\n)\nentity_list.prepare()\nentity_list.validate()\nentity_list.filter()\nentity_list.display()\ndef __init__(\nself,\nentity_class: Type[Entity],\nfields: List[str],\nunregistered: bool,\nmine_only: bool,\n**kwargs,\n):\nself.entity_class = entity_class\nself.fields = fields\nself.unregistered = unregistered\nself.mine_only = mine_only\nself.filters = kwargs\nself.data = []\ndef prepare(self):\nif self.mine_only:\nself.filters[\"owner\"] = get_medperf_user_data()[\"id\"]\nentities = self.entity_class.all(\nunregistered=self.unregistered, filters=self.filters\n)\nself.data = [entity.display_dict() for entity in entities]\ndef validate(self):\nif self.data:\nvalid_fields = set(self.data[0].keys())\nchosen_fields = set(self.fields)\nif not chosen_fields.issubset(valid_fields):\ninvalid_fields = chosen_fields.difference(valid_fields)\ninvalid_fields = \", \".join(invalid_fields)\nraise InvalidArgumentError(f\"Invalid field(s): {invalid_fields}\")\ndef filter(self):\nself.data = [\n{field: entity_dict[field] for field in self.fields}\nfor entity_dict in self.data\n]\ndef display(self):\nheaders = self.fields\ndata_lists = [list(entity_dict.values()) for entity_dict in self.data]\ntab = tabulate(data_lists, headers=headers)\nconfig.ui.print(tab)\n</code></pre>"},{"location":"reference/commands/list/#commands.list.EntityList.run","title":"<code>run(entity_class, fields, unregistered=False, mine_only=False, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Lists all local datasets</p> <p>Parameters:</p> Name Type Description Default <code>entity_class</code> <code>Type[Entity]</code> <p>entity class to instantiate (Dataset, Model, etc.)</p> required <code>fields</code> <code>list[str]</code> <p>list of fields to display</p> required <code>unregistered</code> <code>bool</code> <p>Display only local unregistered results. Defaults to False.</p> <code>False</code> <code>mine_only</code> <code>bool</code> <p>Display all registered current-user results. Defaults to False.</p> <code>False</code> <code>kwargs</code> <code>dict</code> <p>Additional parameters for filtering entity lists. Keys with None will be filtered out.</p> <code>{}</code> Source code in <code>cli/medperf/commands/list.py</code> <pre><code>@staticmethod\ndef run(\nentity_class: Type[Entity],\nfields: List[str],\nunregistered: bool = False,\nmine_only: bool = False,\n**kwargs,\n):\n\"\"\"Lists all local datasets\n    Args:\n        entity_class: entity class to instantiate (Dataset, Model, etc.)\n        fields (list[str]): list of fields to display\n        unregistered (bool, optional): Display only local unregistered results. Defaults to False.\n        mine_only (bool, optional): Display all registered current-user results. Defaults to False.\n        kwargs (dict): Additional parameters for filtering entity lists. Keys with None will be filtered out.\n    \"\"\"\nkwargs = {k: v for k, v in kwargs.items() if v is not None}\nentity_list = EntityList(\nentity_class, fields, unregistered, mine_only, **kwargs\n)\nentity_list.prepare()\nentity_list.validate()\nentity_list.filter()\nentity_list.display()\n</code></pre>"},{"location":"reference/commands/profile/","title":"Profile","text":""},{"location":"reference/commands/profile/#commands.profile.activate","title":"<code>activate(profile)</code>","text":"<p>Assigns the active profile, which is used by default</p> <p>Parameters:</p> Name Type Description Default <code>profile</code> <code>str</code> <p>Name of the profile to be used.</p> required Source code in <code>cli/medperf/commands/profile.py</code> <pre><code>@app.command(\"activate\")\n@clean_except\ndef activate(profile: str):\n\"\"\"Assigns the active profile, which is used by default\n    Args:\n        profile (str): Name of the profile to be used.\n    \"\"\"\nconfig_p = read_config()\nif profile not in config_p:\nraise InvalidArgumentError(\"The provided profile does not exist\")\nconfig_p.activate(profile)\nwrite_config(config_p)\n</code></pre>"},{"location":"reference/commands/profile/#commands.profile.create","title":"<code>create(ctx, name=typer.Option(..., '--name', '-n', help=\"Profile's name\"))</code>","text":"<p>Creates a new profile for managing and customizing configuration</p> Source code in <code>cli/medperf/commands/profile.py</code> <pre><code>@app.command(\"create\")\n@clean_except\n@configurable\ndef create(\nctx: typer.Context,\nname: str = typer.Option(..., \"--name\", \"-n\", help=\"Profile's name\"),\n):\n\"\"\"Creates a new profile for managing and customizing configuration\"\"\"\nargs = ctx.params\nargs.pop(\"name\")\nconfig_p = read_config()\nif name in config_p:\nraise InvalidArgumentError(\"A profile with the same name already exists\")\nconfig_p[name] = args\nwrite_config(config_p)\n</code></pre>"},{"location":"reference/commands/profile/#commands.profile.delete","title":"<code>delete(profile)</code>","text":"<p>Deletes a profile's configuration.</p> <p>Parameters:</p> Name Type Description Default <code>profile</code> <code>str</code> <p>Profile to delete.</p> required Source code in <code>cli/medperf/commands/profile.py</code> <pre><code>@app.command(\"delete\")\n@clean_except\ndef delete(profile: str):\n\"\"\"Deletes a profile's configuration.\n    Args:\n        profile (str): Profile to delete.\n    \"\"\"\nconfig_p = read_config()\nif profile not in config_p.profiles:\nraise InvalidArgumentError(\"The provided profile does not exist\")\nif profile in [\nconfig.default_profile_name,\nconfig.testauth_profile_name,\nconfig.test_profile_name,\n]:\nraise InvalidArgumentError(\"Cannot delete reserved profiles\")\nif config_p.is_profile_active(profile):\nraise InvalidArgumentError(\"Cannot delete a currently activated profile\")\ndel config_p[profile]\nwrite_config(config_p)\n</code></pre>"},{"location":"reference/commands/profile/#commands.profile.list","title":"<code>list()</code>","text":"<p>Lists all available profiles</p> Source code in <code>cli/medperf/commands/profile.py</code> <pre><code>@app.command(\"ls\")\n@clean_except\ndef list():\n\"\"\"Lists all available profiles\"\"\"\nui = config.ui\nconfig_p = read_config()\nfor profile in config_p:\nif config_p.is_profile_active(profile):\nui.print_highlight(\"* \" + profile)\nelse:\nui.print(\"  \" + profile)\n</code></pre>"},{"location":"reference/commands/profile/#commands.profile.set_args","title":"<code>set_args(ctx)</code>","text":"<p>Assign key-value configuration pairs to the current profile.</p> Source code in <code>cli/medperf/commands/profile.py</code> <pre><code>@app.command(\"set\")\n@clean_except\n@configurable\ndef set_args(ctx: typer.Context):\n\"\"\"Assign key-value configuration pairs to the current profile.\"\"\"\nargs = ctx.params\nconfig_p = read_config()\nconfig_p.active_profile.update(args)\nwrite_config(config_p)\n</code></pre>"},{"location":"reference/commands/profile/#commands.profile.view","title":"<code>view(profile=typer.Argument(None))</code>","text":"<p>Displays a profile's configuration.</p> <p>Parameters:</p> Name Type Description Default <code>profile</code> <code>str</code> <p>Profile to display information from. Defaults to active profile.</p> <code>typer.Argument(None)</code> Source code in <code>cli/medperf/commands/profile.py</code> <pre><code>@app.command(\"view\")\n@clean_except\ndef view(profile: str = typer.Argument(None)):\n\"\"\"Displays a profile's configuration.\n    Args:\n        profile (str, optional): Profile to display information from. Defaults to active profile.\n    \"\"\"\nconfig_p = read_config()\nprofile_config = config_p.active_profile\nif profile:\nif profile not in config_p:\nraise InvalidArgumentError(\"The provided profile does not exist\")\nprofile_config = config_p[profile]\nprofile_config.pop(config.credentials_keyword, None)\nprofile_name = profile if profile else config_p.active_profile_name\nconfig.ui.print(f\"\\nProfile '{profile_name}':\")\ndict_pretty_print(profile_config, skip_none_values=False)\n</code></pre>"},{"location":"reference/commands/storage/","title":"Storage","text":""},{"location":"reference/commands/storage/#commands.storage.clean","title":"<code>clean()</code>","text":"<p>Cleans up clutter paths</p> Source code in <code>cli/medperf/commands/storage.py</code> <pre><code>@app.command(\"cleanup\")\ndef clean():\n\"\"\"Cleans up clutter paths\"\"\"\n# Force cleanup to be true\nconfig.cleanup = True\ncleanup()\n</code></pre>"},{"location":"reference/commands/storage/#commands.storage.ls","title":"<code>ls()</code>","text":"<p>Show the location of the current medperf assets</p> Source code in <code>cli/medperf/commands/storage.py</code> <pre><code>@app.command(\"ls\")\n@clean_except\ndef ls():\n\"\"\"Show the location of the current medperf assets\"\"\"\nheaders = [\"Asset\", \"Location\"]\ninfo = []\nfor folder in config.storage:\ninfo.append((folder, config.storage[folder][\"base\"]))\ntab = tabulate(info, headers=headers)\nconfig.ui.print(tab)\n</code></pre>"},{"location":"reference/commands/storage/#commands.storage.move","title":"<code>move(path=typer.Option(..., '--target', '-t', help='Target path'))</code>","text":"<p>Moves all storage folders to a target base path. Folders include: Benchmarks, datasets, containers, results, tests, ...</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>target path</p> <code>typer.Option(..., '--target', '-t', help='Target path')</code> Source code in <code>cli/medperf/commands/storage.py</code> <pre><code>@app.command(\"move\")\n@clean_except\ndef move(path: str = typer.Option(..., \"--target\", \"-t\", help=\"Target path\")):\n\"\"\"Moves all storage folders to a target base path. Folders include:\n    Benchmarks, datasets, containers, results, tests, ...\n    Args:\n        path (str): target path\n    \"\"\"\nmove_storage(path)\n</code></pre>"},{"location":"reference/commands/view/","title":"View","text":""},{"location":"reference/commands/view/#commands.view.EntityView","title":"<code>EntityView</code>","text":"Source code in <code>cli/medperf/commands/view.py</code> <pre><code>class EntityView:\n@staticmethod\ndef run(\nentity_id: Union[int, str],\nentity_class: Type[Entity],\nformat: str = \"yaml\",\nunregistered: bool = False,\nmine_only: bool = False,\noutput: str = None,\n**kwargs,\n):\n\"\"\"Displays the contents of a single or multiple entities of a given type\n        Args:\n            entity_id (Union[int, str]): Entity identifies\n            entity_class (Entity): Entity type\n            unregistered (bool, optional): Display only local unregistered entities. Defaults to False.\n            mine_only (bool, optional): Display all current-user entities. Defaults to False.\n            format (str, optional): What format to use to display the contents. Valid formats: [yaml, json]. Defaults to yaml.\n            output (str, optional): Path to a file for storing the entity contents. If not provided, the contents are printed.\n            kwargs (dict): Additional parameters for filtering entity lists.\n        \"\"\"\nentity_view = EntityView(\nentity_id, entity_class, format, unregistered, mine_only, output, **kwargs\n)\nentity_view.validate()\nentity_view.prepare()\nif output is None:\nentity_view.display()\nelse:\nentity_view.store()\ndef __init__(\nself,\nentity_id: Union[int, str],\nentity_class: Type[Entity],\nformat: str,\nunregistered: bool,\nmine_only: bool,\noutput: str,\n**kwargs,\n):\nself.entity_id = entity_id\nself.entity_class = entity_class\nself.format = format\nself.unregistered = unregistered\nself.mine_only = mine_only\nself.output = output\nself.filters = kwargs\nself.data = []\ndef validate(self):\nvalid_formats = set([\"yaml\", \"json\"])\nif self.format not in valid_formats:\nraise InvalidArgumentError(\"The provided format is not supported\")\ndef prepare(self):\nif self.entity_id is not None:\nentities = [self.entity_class.get(self.entity_id)]\nelse:\nif self.mine_only:\nself.filters[\"owner\"] = get_medperf_user_data()[\"id\"]\nentities = self.entity_class.all(\nunregistered=self.unregistered, filters=self.filters\n)\nself.data = [entity.todict() for entity in entities]\nif self.entity_id is not None:\n# User expects a single entity if id provided\n# Don't output the view as a list of entities\nself.data = self.data[0]\ndef display(self):\nif self.format == \"json\":\nformatter = json.dumps\nif self.format == \"yaml\":\nformatter = yaml.dump\nformatted_data = formatter(self.data)\nconfig.ui.print(formatted_data)\ndef store(self):\nif self.format == \"json\":\nformatter = json.dump\nif self.format == \"yaml\":\nformatter = yaml.dump\nwith open(self.output, \"w\") as f:\nformatter(self.data, f)\n</code></pre>"},{"location":"reference/commands/view/#commands.view.EntityView.run","title":"<code>run(entity_id, entity_class, format='yaml', unregistered=False, mine_only=False, output=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Displays the contents of a single or multiple entities of a given type</p> <p>Parameters:</p> Name Type Description Default <code>entity_id</code> <code>Union[int, str]</code> <p>Entity identifies</p> required <code>entity_class</code> <code>Entity</code> <p>Entity type</p> required <code>unregistered</code> <code>bool</code> <p>Display only local unregistered entities. Defaults to False.</p> <code>False</code> <code>mine_only</code> <code>bool</code> <p>Display all current-user entities. Defaults to False.</p> <code>False</code> <code>format</code> <code>str</code> <p>What format to use to display the contents. Valid formats: [yaml, json]. Defaults to yaml.</p> <code>'yaml'</code> <code>output</code> <code>str</code> <p>Path to a file for storing the entity contents. If not provided, the contents are printed.</p> <code>None</code> <code>kwargs</code> <code>dict</code> <p>Additional parameters for filtering entity lists.</p> <code>{}</code> Source code in <code>cli/medperf/commands/view.py</code> <pre><code>@staticmethod\ndef run(\nentity_id: Union[int, str],\nentity_class: Type[Entity],\nformat: str = \"yaml\",\nunregistered: bool = False,\nmine_only: bool = False,\noutput: str = None,\n**kwargs,\n):\n\"\"\"Displays the contents of a single or multiple entities of a given type\n    Args:\n        entity_id (Union[int, str]): Entity identifies\n        entity_class (Entity): Entity type\n        unregistered (bool, optional): Display only local unregistered entities. Defaults to False.\n        mine_only (bool, optional): Display all current-user entities. Defaults to False.\n        format (str, optional): What format to use to display the contents. Valid formats: [yaml, json]. Defaults to yaml.\n        output (str, optional): Path to a file for storing the entity contents. If not provided, the contents are printed.\n        kwargs (dict): Additional parameters for filtering entity lists.\n    \"\"\"\nentity_view = EntityView(\nentity_id, entity_class, format, unregistered, mine_only, output, **kwargs\n)\nentity_view.validate()\nentity_view.prepare()\nif output is None:\nentity_view.display()\nelse:\nentity_view.store()\n</code></pre>"},{"location":"reference/commands/aggregator/aggregator/","title":"Aggregator","text":""},{"location":"reference/commands/aggregator/aggregator/#commands.aggregator.aggregator.associate","title":"<code>associate(aggregator_id=typer.Option(..., '--aggregator_id', '-a', help='UID of benchmark to associate with'), training_exp_id=typer.Option(..., '--training_exp_id', '-t', help='UID of benchmark to associate with'), approval=typer.Option(False, '-y', help='Skip approval step'))</code>","text":"<p>Associates an aggregator with a training experiment.</p> Source code in <code>cli/medperf/commands/aggregator/aggregator.py</code> <pre><code>@app.command(\"associate\")\n@clean_except\ndef associate(\naggregator_id: int = typer.Option(\n..., \"--aggregator_id\", \"-a\", help=\"UID of benchmark to associate with\"\n),\ntraining_exp_id: int = typer.Option(\n..., \"--training_exp_id\", \"-t\", help=\"UID of benchmark to associate with\"\n),\napproval: bool = typer.Option(False, \"-y\", help=\"Skip approval step\"),\n):\n\"\"\"Associates an aggregator with a training experiment.\"\"\"\nAssociateAggregator.run(aggregator_id, training_exp_id, approved=approval)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/aggregator/aggregator/#commands.aggregator.aggregator.list","title":"<code>list(unregistered=typer.Option(False, '--unregistered', help='Get unregistered aggregators'), mine=typer.Option(False, '--mine', help='Get current-user aggregators'))</code>","text":"<p>List aggregators</p> Source code in <code>cli/medperf/commands/aggregator/aggregator.py</code> <pre><code>@app.command(\"ls\")\n@clean_except\ndef list(\nunregistered: bool = typer.Option(\nFalse, \"--unregistered\", help=\"Get unregistered aggregators\"\n),\nmine: bool = typer.Option(False, \"--mine\", help=\"Get current-user aggregators\"),\n):\n\"\"\"List aggregators\"\"\"\nEntityList.run(\nAggregator,\nfields=[\"UID\", \"Name\", \"Address\", \"Port\"],\nunregistered=unregistered,\nmine_only=mine,\n)\n</code></pre>"},{"location":"reference/commands/aggregator/aggregator/#commands.aggregator.aggregator.run","title":"<code>run(training_exp_id=typer.Option(..., '--training_exp_id', '-t', help='UID of training experiment whose aggregator to be run'), publish_on=typer.Option('127.0.0.1', '--publish_on', '-p', help='Host network interface on which the aggregator will listen'), overwrite=typer.Option(False, '--overwrite', help='Overwrite outputs if present'))</code>","text":"<p>Starts the aggregation server of a training experiment</p> Source code in <code>cli/medperf/commands/aggregator/aggregator.py</code> <pre><code>@app.command(\"start\")\n@clean_except\ndef run(\ntraining_exp_id: int = typer.Option(\n...,\n\"--training_exp_id\",\n\"-t\",\nhelp=\"UID of training experiment whose aggregator to be run\",\n),\npublish_on: str = typer.Option(\n\"127.0.0.1\",\n\"--publish_on\",\n\"-p\",\nhelp=\"Host network interface on which the aggregator will listen\",\n),\noverwrite: bool = typer.Option(\nFalse, \"--overwrite\", help=\"Overwrite outputs if present\"\n),\n):\n\"\"\"Starts the aggregation server of a training experiment\"\"\"\nStartAggregator.run(training_exp_id, publish_on, overwrite)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/aggregator/aggregator/#commands.aggregator.aggregator.submit","title":"<code>submit(name=typer.Option(..., '--name', '-n', help='Name of the aggregator'), address=typer.Option(..., '--address', '-a', help='Address/domain of the aggregator'), port=typer.Option(..., '--port', '-p', help='The port which the aggregator will use'), aggregation_mlcube=typer.Option(..., '--aggregation-container', '-m', help='Aggregation container UID'))</code>","text":"<p>Submits an aggregator</p> Source code in <code>cli/medperf/commands/aggregator/aggregator.py</code> <pre><code>@app.command(\"submit\")\n@clean_except\ndef submit(\nname: str = typer.Option(..., \"--name\", \"-n\", help=\"Name of the aggregator\"),\naddress: str = typer.Option(\n..., \"--address\", \"-a\", help=\"Address/domain of the aggregator\"\n),\nport: int = typer.Option(\n..., \"--port\", \"-p\", help=\"The port which the aggregator will use\"\n),\naggregation_mlcube: int = typer.Option(\n..., \"--aggregation-container\", \"-m\", help=\"Aggregation container UID\"\n),\n):\n\"\"\"Submits an aggregator\"\"\"\nSubmitAggregator.run(name, address, port, aggregation_mlcube)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/aggregator/aggregator/#commands.aggregator.aggregator.view","title":"<code>view(entity_id=typer.Argument(None, help='Benchmark ID'), format=typer.Option('yaml', '-f', '--format', help='Format to display contents. Available formats: [yaml, json]'), unregistered=typer.Option(False, '--unregistered', help='Display unregistered benchmarks if benchmark ID is not provided'), mine=typer.Option(False, '--mine', help='Display current-user benchmarks if benchmark ID is not provided'), output=typer.Option(None, '--output', '-o', help='Output file to store contents. If not provided, the output will be displayed'))</code>","text":"<p>Displays the information of one or more aggregators</p> Source code in <code>cli/medperf/commands/aggregator/aggregator.py</code> <pre><code>@app.command(\"view\")\n@clean_except\ndef view(\nentity_id: Optional[int] = typer.Argument(None, help=\"Benchmark ID\"),\nformat: str = typer.Option(\n\"yaml\",\n\"-f\",\n\"--format\",\nhelp=\"Format to display contents. Available formats: [yaml, json]\",\n),\nunregistered: bool = typer.Option(\nFalse,\n\"--unregistered\",\nhelp=\"Display unregistered benchmarks if benchmark ID is not provided\",\n),\nmine: bool = typer.Option(\nFalse,\n\"--mine\",\nhelp=\"Display current-user benchmarks if benchmark ID is not provided\",\n),\noutput: str = typer.Option(\nNone,\n\"--output\",\n\"-o\",\nhelp=\"Output file to store contents. If not provided, the output will be displayed\",\n),\n):\n\"\"\"Displays the information of one or more aggregators\"\"\"\nEntityView.run(entity_id, Aggregator, format, unregistered, mine, output)\n</code></pre>"},{"location":"reference/commands/aggregator/associate/","title":"Associate","text":""},{"location":"reference/commands/aggregator/associate/#commands.aggregator.associate.AssociateAggregator","title":"<code>AssociateAggregator</code>","text":"Source code in <code>cli/medperf/commands/aggregator/associate.py</code> <pre><code>class AssociateAggregator:\n@staticmethod\ndef run(training_exp_id: int, agg_uid: int, approved=False):\n\"\"\"Associates an aggregator with a training experiment\n        Args:\n            agg_uid (int): UID of the registered aggregator to associate\n            benchmark_uid (int): UID of the benchmark to associate with\n        \"\"\"\ncomms = config.comms\nui = config.ui\nagg = Aggregator.get(agg_uid)\nif agg.id is None:\nmsg = \"The provided aggregator is not registered.\"\nraise InvalidArgumentError(msg)\ntraining_exp = TrainingExp.get(training_exp_id)\nmsg = \"Please confirm that you would like to associate\"\nmsg += f\" the aggregator {agg.name} with the training exp {training_exp.name}.\"\nmsg += \" [Y/n]\"\napproved = approved or approval_prompt(msg)\nif approved:\nui.print(\"Generating aggregator training association\")\ncomms.associate_training_aggregator(agg.id, training_exp_id)\nelse:\nui.print(\"Aggregator association operation cancelled.\")\n</code></pre>"},{"location":"reference/commands/aggregator/associate/#commands.aggregator.associate.AssociateAggregator.run","title":"<code>run(training_exp_id, agg_uid, approved=False)</code>  <code>staticmethod</code>","text":"<p>Associates an aggregator with a training experiment</p> <p>Parameters:</p> Name Type Description Default <code>agg_uid</code> <code>int</code> <p>UID of the registered aggregator to associate</p> required <code>benchmark_uid</code> <code>int</code> <p>UID of the benchmark to associate with</p> required Source code in <code>cli/medperf/commands/aggregator/associate.py</code> <pre><code>@staticmethod\ndef run(training_exp_id: int, agg_uid: int, approved=False):\n\"\"\"Associates an aggregator with a training experiment\n    Args:\n        agg_uid (int): UID of the registered aggregator to associate\n        benchmark_uid (int): UID of the benchmark to associate with\n    \"\"\"\ncomms = config.comms\nui = config.ui\nagg = Aggregator.get(agg_uid)\nif agg.id is None:\nmsg = \"The provided aggregator is not registered.\"\nraise InvalidArgumentError(msg)\ntraining_exp = TrainingExp.get(training_exp_id)\nmsg = \"Please confirm that you would like to associate\"\nmsg += f\" the aggregator {agg.name} with the training exp {training_exp.name}.\"\nmsg += \" [Y/n]\"\napproved = approved or approval_prompt(msg)\nif approved:\nui.print(\"Generating aggregator training association\")\ncomms.associate_training_aggregator(agg.id, training_exp_id)\nelse:\nui.print(\"Aggregator association operation cancelled.\")\n</code></pre>"},{"location":"reference/commands/aggregator/run/","title":"Run","text":""},{"location":"reference/commands/aggregator/run/#commands.aggregator.run.StartAggregator","title":"<code>StartAggregator</code>","text":"Source code in <code>cli/medperf/commands/aggregator/run.py</code> <pre><code>class StartAggregator:\n@classmethod\ndef run(\ncls,\ntraining_exp_id: int,\npublish_on: str = \"127.0.0.1\",\noverwrite: bool = False,\n):\n\"\"\"Starts the aggregation server of a training experiment\n        Args:\n            training_exp_id (int): Training experiment UID.\n        \"\"\"\nexecution = cls(training_exp_id, publish_on, overwrite)\nexecution.prepare()\nexecution.validate()\nexecution.check_existing_outputs()\nexecution.prepare_aggregator()\nexecution.prepare_participants_list()\nexecution.prepare_plan()\nexecution.prepare_pki_assets()\nwith config.ui.interactive():\nexecution.run_experiment()\ndef __init__(self, training_exp_id, publish_on, overwrite) -&gt; None:\nself.training_exp_id = training_exp_id\nself.overwrite = overwrite\nself.publish_on = publish_on\nself.ui = config.ui\ndef prepare(self):\nself.training_exp = TrainingExp.get(self.training_exp_id)\nself.ui.print(f\"Training Execution: {self.training_exp.name}\")\nself.event = TrainingEvent.from_experiment(self.training_exp_id)\ndef validate(self):\nif self.event.finished:\nmsg = \"The provided training experiment has to start a training event.\"\nraise InvalidArgumentError(msg)\ndef check_existing_outputs(self):\nmsg = (\n\"Outputs still exist from previous runs. Overwrite\"\n\" them by rerunning the command with --overwrite\"\n)\npaths = [\nself.event.agg_out_logs,\nself.event.out_weights,\nself.event.report_path,\n]\nfor path in paths:\nif os.path.exists(path):\nif not self.overwrite:\nraise MedperfException(msg)\nremove_path(path)\ndef prepare_aggregator(self):\nself.aggregator = Aggregator.from_experiment(self.training_exp_id)\nself.cube = self.__get_cube(self.aggregator.aggregation_mlcube, \"aggregation\")\ndef __get_cube(self, uid: int, name: str) -&gt; Cube:\nself.ui.text = f\"Retrieving container '{name}'\"\ncube = Cube.get(uid)\ncube.download_run_files()\nself.ui.print(f\"&gt; container '{name}' download complete\")\nreturn cube\ndef prepare_participants_list(self):\nself.event.prepare_participants_list()\ndef prepare_plan(self):\nself.training_exp.prepare_plan()\ndef prepare_pki_assets(self):\nca = CA.from_experiment(self.training_exp_id)\ntrust(ca)\nagg_address = self.aggregator.address\nself.aggregator_pki_assets = get_pki_assets_path(agg_address, ca.name)\nself.ca = ca\ndef run_experiment(self):\nmounts = {\n\"node_cert_folder\": self.aggregator_pki_assets,\n\"ca_cert_folder\": self.ca.pki_assets,\n\"plan_path\": self.training_exp.plan_path,\n\"collaborators\": self.event.participants_list_path,\n\"output_logs\": self.event.agg_out_logs,\n\"output_weights\": self.event.out_weights,\n\"report_path\": self.event.report_path,\n}\nself.ui.text = \"Running Aggregator\"\nport = self.aggregator.port\nself.cube.run(\ntask=\"start_aggregator\",\nmounts=mounts,\nports=[f\"{self.publish_on}:{port}:{port}\"],\ndisable_network=False,\n)\n</code></pre>"},{"location":"reference/commands/aggregator/run/#commands.aggregator.run.StartAggregator.run","title":"<code>run(training_exp_id, publish_on='127.0.0.1', overwrite=False)</code>  <code>classmethod</code>","text":"<p>Starts the aggregation server of a training experiment</p> <p>Parameters:</p> Name Type Description Default <code>training_exp_id</code> <code>int</code> <p>Training experiment UID.</p> required Source code in <code>cli/medperf/commands/aggregator/run.py</code> <pre><code>@classmethod\ndef run(\ncls,\ntraining_exp_id: int,\npublish_on: str = \"127.0.0.1\",\noverwrite: bool = False,\n):\n\"\"\"Starts the aggregation server of a training experiment\n    Args:\n        training_exp_id (int): Training experiment UID.\n    \"\"\"\nexecution = cls(training_exp_id, publish_on, overwrite)\nexecution.prepare()\nexecution.validate()\nexecution.check_existing_outputs()\nexecution.prepare_aggregator()\nexecution.prepare_participants_list()\nexecution.prepare_plan()\nexecution.prepare_pki_assets()\nwith config.ui.interactive():\nexecution.run_experiment()\n</code></pre>"},{"location":"reference/commands/aggregator/submit/","title":"Submit","text":""},{"location":"reference/commands/aggregator/submit/#commands.aggregator.submit.SubmitAggregator","title":"<code>SubmitAggregator</code>","text":"Source code in <code>cli/medperf/commands/aggregator/submit.py</code> <pre><code>class SubmitAggregator:\n@classmethod\ndef run(cls, name: str, address: str, port: int, aggregation_mlcube: int):\n\"\"\"Submits a new aggregator to the medperf platform\n        Args:\n            name (str): aggregator name\n            address (str): aggregator address/domain\n            port (int): port which the aggregator will use\n            aggregation_mlcube (int): aggregation mlcube uid\n        \"\"\"\nui = config.ui\nsubmission = cls(name, address, port, aggregation_mlcube)\nwith ui.interactive():\nui.text = \"Submitting Aggregator to MedPerf\"\nsubmission.validate_agg_cube()\nupdated_benchmark_body = submission.submit()\nui.print(\"Uploaded\")\nsubmission.write(updated_benchmark_body)\ndef __init__(self, name: str, address: str, port: int, aggregation_mlcube: int):\nself.ui = config.ui\nagg_config = {\"address\": address, \"port\": port}\nself.aggregator = Aggregator(\nname=name, config=agg_config, aggregation_mlcube=aggregation_mlcube\n)\nconfig.tmp_paths.append(self.aggregator.path)\ndef validate_agg_cube(self):\nCube.get(self.aggregator.aggregation_mlcube)\ndef submit(self):\nupdated_body = self.aggregator.upload()\nreturn updated_body\ndef write(self, updated_body):\nremove_path(self.aggregator.path)\naggregator = Aggregator(**updated_body)\naggregator.write()\n</code></pre>"},{"location":"reference/commands/aggregator/submit/#commands.aggregator.submit.SubmitAggregator.run","title":"<code>run(name, address, port, aggregation_mlcube)</code>  <code>classmethod</code>","text":"<p>Submits a new aggregator to the medperf platform</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>aggregator name</p> required <code>address</code> <code>str</code> <p>aggregator address/domain</p> required <code>port</code> <code>int</code> <p>port which the aggregator will use</p> required <code>aggregation_mlcube</code> <code>int</code> <p>aggregation mlcube uid</p> required Source code in <code>cli/medperf/commands/aggregator/submit.py</code> <pre><code>@classmethod\ndef run(cls, name: str, address: str, port: int, aggregation_mlcube: int):\n\"\"\"Submits a new aggregator to the medperf platform\n    Args:\n        name (str): aggregator name\n        address (str): aggregator address/domain\n        port (int): port which the aggregator will use\n        aggregation_mlcube (int): aggregation mlcube uid\n    \"\"\"\nui = config.ui\nsubmission = cls(name, address, port, aggregation_mlcube)\nwith ui.interactive():\nui.text = \"Submitting Aggregator to MedPerf\"\nsubmission.validate_agg_cube()\nupdated_benchmark_body = submission.submit()\nui.print(\"Uploaded\")\nsubmission.write(updated_benchmark_body)\n</code></pre>"},{"location":"reference/commands/association/approval/","title":"Approval","text":""},{"location":"reference/commands/association/approval/#commands.association.approval.Approval","title":"<code>Approval</code>","text":"Source code in <code>cli/medperf/commands/association/approval.py</code> <pre><code>class Approval:\n@staticmethod\ndef run(\napproval_status: str,\nbenchmark_uid: int = None,\ntraining_exp_uid: int = None,\ndataset_uid: int = None,\nmlcube_uid: int = None,\naggregator_uid: int = None,\nca_uid: int = None,\n):\n\"\"\"Sets approval status for an association between a benchmark and a dataset or mlcube\n        Args:\n            benchmark_uid (int): Benchmark UID.\n            approval_status (str): Desired approval status to set for the association.\n            comms (Comms): Instance of Comms interface.\n            ui (UI): Instance of UI interface.\n            dataset_uid (int, optional): Dataset UID. Defaults to None.\n            mlcube_uid (int, optional): MLCube UID. Defaults to None.\n        \"\"\"\ncomms = config.comms\nvalidate_args(\nbenchmark_uid,\ntraining_exp_uid,\ndataset_uid,\nmlcube_uid,\naggregator_uid,\nca_uid,\napproval_status.value,\n)\nupdate = {\"approval_status\": approval_status.value}\nif benchmark_uid:\nif dataset_uid:\ncomms.update_benchmark_dataset_association(\nbenchmark_uid, dataset_uid, update\n)\nif mlcube_uid:\ncomms.update_benchmark_model_association(\nbenchmark_uid, mlcube_uid, update\n)\nif training_exp_uid:\nif dataset_uid:\ncomms.update_training_dataset_association(\ntraining_exp_uid, dataset_uid, update\n)\nif aggregator_uid:\ncomms.update_training_aggregator_association(\ntraining_exp_uid, aggregator_uid, update\n)\nif ca_uid:\ncomms.update_training_ca_association(training_exp_uid, ca_uid, update)\n</code></pre>"},{"location":"reference/commands/association/approval/#commands.association.approval.Approval.run","title":"<code>run(approval_status, benchmark_uid=None, training_exp_uid=None, dataset_uid=None, mlcube_uid=None, aggregator_uid=None, ca_uid=None)</code>  <code>staticmethod</code>","text":"<p>Sets approval status for an association between a benchmark and a dataset or mlcube</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>Benchmark UID.</p> <code>None</code> <code>approval_status</code> <code>str</code> <p>Desired approval status to set for the association.</p> required <code>comms</code> <code>Comms</code> <p>Instance of Comms interface.</p> required <code>ui</code> <code>UI</code> <p>Instance of UI interface.</p> required <code>dataset_uid</code> <code>int</code> <p>Dataset UID. Defaults to None.</p> <code>None</code> <code>mlcube_uid</code> <code>int</code> <p>MLCube UID. Defaults to None.</p> <code>None</code> Source code in <code>cli/medperf/commands/association/approval.py</code> <pre><code>@staticmethod\ndef run(\napproval_status: str,\nbenchmark_uid: int = None,\ntraining_exp_uid: int = None,\ndataset_uid: int = None,\nmlcube_uid: int = None,\naggregator_uid: int = None,\nca_uid: int = None,\n):\n\"\"\"Sets approval status for an association between a benchmark and a dataset or mlcube\n    Args:\n        benchmark_uid (int): Benchmark UID.\n        approval_status (str): Desired approval status to set for the association.\n        comms (Comms): Instance of Comms interface.\n        ui (UI): Instance of UI interface.\n        dataset_uid (int, optional): Dataset UID. Defaults to None.\n        mlcube_uid (int, optional): MLCube UID. Defaults to None.\n    \"\"\"\ncomms = config.comms\nvalidate_args(\nbenchmark_uid,\ntraining_exp_uid,\ndataset_uid,\nmlcube_uid,\naggregator_uid,\nca_uid,\napproval_status.value,\n)\nupdate = {\"approval_status\": approval_status.value}\nif benchmark_uid:\nif dataset_uid:\ncomms.update_benchmark_dataset_association(\nbenchmark_uid, dataset_uid, update\n)\nif mlcube_uid:\ncomms.update_benchmark_model_association(\nbenchmark_uid, mlcube_uid, update\n)\nif training_exp_uid:\nif dataset_uid:\ncomms.update_training_dataset_association(\ntraining_exp_uid, dataset_uid, update\n)\nif aggregator_uid:\ncomms.update_training_aggregator_association(\ntraining_exp_uid, aggregator_uid, update\n)\nif ca_uid:\ncomms.update_training_ca_association(training_exp_uid, ca_uid, update)\n</code></pre>"},{"location":"reference/commands/association/association/","title":"Association","text":""},{"location":"reference/commands/association/association/#commands.association.association.approve","title":"<code>approve(benchmark_uid=typer.Option(None, '--benchmark', '-b', help='Benchmark UID'), training_exp_uid=typer.Option(None, '--training_exp', '-t', help='Training exp UID'), dataset_uid=typer.Option(None, '--dataset', '-d', help='Dataset UID'), model_uid=typer.Option(None, '--model', '-m', help='Model container UID'), aggregator_uid=typer.Option(None, '--aggregator', '-a', help='Aggregator UID'), ca_uid=typer.Option(None, '--ca', '-c', help='CA UID'))</code>","text":"<p>Approves an association between a benchmark and a dataset or model container</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>Benchmark UID.</p> <code>typer.Option(None, '--benchmark', '-b', help='Benchmark UID')</code> <code>dataset_uid</code> <code>int</code> <p>Dataset UID.</p> <code>typer.Option(None, '--dataset', '-d', help='Dataset UID')</code> <code>model_uid</code> <code>int</code> <p>Model container UID.</p> <code>typer.Option(None, '--model', '-m', help='Model container UID')</code> Source code in <code>cli/medperf/commands/association/association.py</code> <pre><code>@app.command(\"approve\")\n@clean_except\ndef approve(\nbenchmark_uid: int = typer.Option(None, \"--benchmark\", \"-b\", help=\"Benchmark UID\"),\ntraining_exp_uid: int = typer.Option(\nNone, \"--training_exp\", \"-t\", help=\"Training exp UID\"\n),\ndataset_uid: int = typer.Option(None, \"--dataset\", \"-d\", help=\"Dataset UID\"),\nmodel_uid: int = typer.Option(None, \"--model\", \"-m\", help=\"Model container UID\"),\naggregator_uid: int = typer.Option(\nNone, \"--aggregator\", \"-a\", help=\"Aggregator UID\"\n),\nca_uid: int = typer.Option(None, \"--ca\", \"-c\", help=\"CA UID\"),\n):\n\"\"\"Approves an association between a benchmark and a dataset or model container\n    Args:\n        benchmark_uid (int): Benchmark UID.\n        dataset_uid (int, optional): Dataset UID.\n        model_uid (int, optional): Model container UID.\n    \"\"\"\nApproval.run(\nStatus.APPROVED,\nbenchmark_uid,\ntraining_exp_uid,\ndataset_uid,\nmodel_uid,\naggregator_uid,\nca_uid,\n)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/association/association/#commands.association.association.list","title":"<code>list(benchmark=typer.Option(False, '-b', help='list benchmark associations'), training_exp=typer.Option(False, '-t', help='list training associations'), dataset=typer.Option(False, '-d', help='list dataset associations'), mlcube=typer.Option(False, '-m', help='list models associations'), aggregator=typer.Option(False, '-a', help='list aggregator associations'), ca=typer.Option(False, '-c', help='list ca associations'), approval_status=typer.Option(None, '--approval-status', help='Approval status'))</code>","text":"<p>Display all associations related to the current user.</p> <p>Parameters:</p> Name Type Description Default <code>filter</code> <code>str</code> <p>Filter associations by approval status. Defaults to displaying all user associations.</p> required Source code in <code>cli/medperf/commands/association/association.py</code> <pre><code>@app.command(\"ls\")\n@clean_except\ndef list(\nbenchmark: bool = typer.Option(False, \"-b\", help=\"list benchmark associations\"),\ntraining_exp: bool = typer.Option(False, \"-t\", help=\"list training associations\"),\ndataset: bool = typer.Option(False, \"-d\", help=\"list dataset associations\"),\nmlcube: bool = typer.Option(False, \"-m\", help=\"list models associations\"),\naggregator: bool = typer.Option(False, \"-a\", help=\"list aggregator associations\"),\nca: bool = typer.Option(False, \"-c\", help=\"list ca associations\"),\napproval_status: str = typer.Option(\nNone, \"--approval-status\", help=\"Approval status\"\n),\n):\n\"\"\"Display all associations related to the current user.\n    Args:\n        filter (str, optional): Filter associations by approval status.\n            Defaults to displaying all user associations.\n    \"\"\"\nListAssociations.run(\napproval_status,\nbenchmark,\ntraining_exp,\ndataset,\nmlcube,\naggregator,\nca,\n)\n</code></pre>"},{"location":"reference/commands/association/association/#commands.association.association.reject","title":"<code>reject(benchmark_uid=typer.Option(None, '--benchmark', '-b', help='Benchmark UID'), training_exp_uid=typer.Option(None, '--training_exp', '-t', help='Training exp UID'), dataset_uid=typer.Option(None, '--dataset', '-d', help='Dataset UID'), model_uid=typer.Option(None, '--model', '-m', help='Model container UID'), aggregator_uid=typer.Option(None, '--aggregator', '-a', help='Aggregator UID'), ca_uid=typer.Option(None, '--ca', '-c', help='CA UID'))</code>","text":"<p>Rejects an association between a benchmark and a dataset or model container</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>Benchmark UID.</p> <code>typer.Option(None, '--benchmark', '-b', help='Benchmark UID')</code> <code>dataset_uid</code> <code>int</code> <p>Dataset UID.</p> <code>typer.Option(None, '--dataset', '-d', help='Dataset UID')</code> <code>model_uid</code> <code>int</code> <p>Model container UID.</p> <code>typer.Option(None, '--model', '-m', help='Model container UID')</code> Source code in <code>cli/medperf/commands/association/association.py</code> <pre><code>@app.command(\"reject\")\n@clean_except\ndef reject(\nbenchmark_uid: int = typer.Option(None, \"--benchmark\", \"-b\", help=\"Benchmark UID\"),\ntraining_exp_uid: int = typer.Option(\nNone, \"--training_exp\", \"-t\", help=\"Training exp UID\"\n),\ndataset_uid: int = typer.Option(None, \"--dataset\", \"-d\", help=\"Dataset UID\"),\nmodel_uid: int = typer.Option(None, \"--model\", \"-m\", help=\"Model container UID\"),\naggregator_uid: int = typer.Option(\nNone, \"--aggregator\", \"-a\", help=\"Aggregator UID\"\n),\nca_uid: int = typer.Option(None, \"--ca\", \"-c\", help=\"CA UID\"),\n):\n\"\"\"Rejects an association between a benchmark and a dataset or model container\n    Args:\n        benchmark_uid (int): Benchmark UID.\n        dataset_uid (int, optional): Dataset UID.\n        model_uid (int, optional): Model container UID.\n    \"\"\"\nApproval.run(\nStatus.REJECTED,\nbenchmark_uid,\ntraining_exp_uid,\ndataset_uid,\nmodel_uid,\naggregator_uid,\nca_uid,\n)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/association/association/#commands.association.association.set_priority","title":"<code>set_priority(benchmark_uid=typer.Option(..., '--benchmark', '-b', help='Benchmark UID'), model_uid=typer.Option(..., '--model', '-m', help='Model container UID'), priority=typer.Option(..., '--priority', '-p', help='Priority, an integer'))</code>","text":"<p>Updates the priority of a benchmark-model association. Model priorities within a benchmark define which models need to be executed before others when this benchmark is run. A model with a higher priority is executed before a model with lower priority. The order of execution of models of the same priority is arbitrary.</p> <p>Examples:</p> <p>Assume there are three models of IDs (1,2,3), associated with a certain benchmark, all having priority = 0. - By setting the priority of model (2) to the value of 1, the client will make sure that model (2) is executed before models (1,3). - By setting the priority of model (1) to the value of -5, the client will make sure that models (2,3) are executed before model (1).</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>Benchmark UID.</p> <code>typer.Option(..., '--benchmark', '-b', help='Benchmark UID')</code> <code>model_uid</code> <code>int</code> <p>Model container UID.</p> <code>typer.Option(..., '--model', '-m', help='Model container UID')</code> <code>priority</code> <code>int</code> <p>Priority, an integer</p> <code>typer.Option(..., '--priority', '-p', help='Priority, an integer')</code> Source code in <code>cli/medperf/commands/association/association.py</code> <pre><code>@app.command(\"set_priority\")\n@clean_except\ndef set_priority(\nbenchmark_uid: int = typer.Option(..., \"--benchmark\", \"-b\", help=\"Benchmark UID\"),\nmodel_uid: int = typer.Option(..., \"--model\", \"-m\", help=\"Model container UID\"),\npriority: int = typer.Option(..., \"--priority\", \"-p\", help=\"Priority, an integer\"),\n):\n\"\"\"Updates the priority of a benchmark-model association. Model priorities within\n    a benchmark define which models need to be executed before others when\n    this benchmark is run. A model with a higher priority is executed before\n    a model with lower priority. The order of execution of models of the same priority\n    is arbitrary.\n    Examples:\n    Assume there are three models of IDs (1,2,3), associated with a certain benchmark,\n    all having priority = 0.\n    - By setting the priority of model (2) to the value of 1, the client will make\n    sure that model (2) is executed before models (1,3).\n    - By setting the priority of model (1) to the value of -5, the client will make\n    sure that models (2,3) are executed before model (1).\n    Args:\n        benchmark_uid (int): Benchmark UID.\n        model_uid (int): Model container UID.\n        priority (int): Priority, an integer\n    \"\"\"\nAssociationPriority.run(benchmark_uid, model_uid, priority)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/association/list/","title":"List","text":""},{"location":"reference/commands/association/list/#commands.association.list.ListAssociations","title":"<code>ListAssociations</code>","text":"Source code in <code>cli/medperf/commands/association/list.py</code> <pre><code>class ListAssociations:\n@staticmethod\ndef run(\napproval_status,\nbenchmark=False,\ntraining_exp=False,\ndataset=False,\nmlcube=False,\naggregator=False,\nca=False,\n):\n\"\"\"Get user association requests\"\"\"\nvalidate_args(\nbenchmark, training_exp, dataset, mlcube, aggregator, ca, approval_status\n)\nif training_exp:\nexperiment_type = \"training_exp\"\nelif benchmark:\nexperiment_type = \"benchmark\"\nif mlcube:\ncomponent_type = \"model_mlcube\"\nelif dataset:\ncomponent_type = \"dataset\"\nelif aggregator:\ncomponent_type = \"aggregator\"\nelif ca:\ncomponent_type = \"ca\"\nassocs = get_user_associations(experiment_type, component_type, approval_status)\nassocs_info = []\nfor assoc in assocs:\nassoc_info = (\nassoc[component_type],\nassoc[experiment_type],\nassoc[\"initiated_by\"],\nassoc[\"approval_status\"],\n)\nassocs_info.append(assoc_info)\nheaders = [\nf\"{component_type.replace('_', ' ').title()} UID\",\nf\"{experiment_type.replace('_', ' ').title()} UID\",\n\"Initiated by\",\n\"Status\",\n]\ntab = tabulate(assocs_info, headers=headers)\nconfig.ui.print(tab)\n</code></pre>"},{"location":"reference/commands/association/list/#commands.association.list.ListAssociations.run","title":"<code>run(approval_status, benchmark=False, training_exp=False, dataset=False, mlcube=False, aggregator=False, ca=False)</code>  <code>staticmethod</code>","text":"<p>Get user association requests</p> Source code in <code>cli/medperf/commands/association/list.py</code> <pre><code>@staticmethod\ndef run(\napproval_status,\nbenchmark=False,\ntraining_exp=False,\ndataset=False,\nmlcube=False,\naggregator=False,\nca=False,\n):\n\"\"\"Get user association requests\"\"\"\nvalidate_args(\nbenchmark, training_exp, dataset, mlcube, aggregator, ca, approval_status\n)\nif training_exp:\nexperiment_type = \"training_exp\"\nelif benchmark:\nexperiment_type = \"benchmark\"\nif mlcube:\ncomponent_type = \"model_mlcube\"\nelif dataset:\ncomponent_type = \"dataset\"\nelif aggregator:\ncomponent_type = \"aggregator\"\nelif ca:\ncomponent_type = \"ca\"\nassocs = get_user_associations(experiment_type, component_type, approval_status)\nassocs_info = []\nfor assoc in assocs:\nassoc_info = (\nassoc[component_type],\nassoc[experiment_type],\nassoc[\"initiated_by\"],\nassoc[\"approval_status\"],\n)\nassocs_info.append(assoc_info)\nheaders = [\nf\"{component_type.replace('_', ' ').title()} UID\",\nf\"{experiment_type.replace('_', ' ').title()} UID\",\n\"Initiated by\",\n\"Status\",\n]\ntab = tabulate(assocs_info, headers=headers)\nconfig.ui.print(tab)\n</code></pre>"},{"location":"reference/commands/association/priority/","title":"Priority","text":""},{"location":"reference/commands/association/priority/#commands.association.priority.AssociationPriority","title":"<code>AssociationPriority</code>","text":"Source code in <code>cli/medperf/commands/association/priority.py</code> <pre><code>class AssociationPriority:\n@staticmethod\ndef run(benchmark_uid: int, mlcube_uid: int, priority: int):\n\"\"\"Sets priority for an association between a benchmark and an mlcube\n        Args:\n            benchmark_uid (int): Benchmark UID.\n            mlcube_uid (int): MLCube UID.\n            priority (int): priority value\n        \"\"\"\nassociated_cubes = Benchmark.get_models_uids(benchmark_uid)\nif mlcube_uid not in associated_cubes:\nraise InvalidArgumentError(\n\"The given container doesn't exist or is not associated with the benchmark\"\n)\nconfig.comms.update_benchmark_model_association(\nbenchmark_uid, mlcube_uid, {\"priority\": priority}\n)\n</code></pre>"},{"location":"reference/commands/association/priority/#commands.association.priority.AssociationPriority.run","title":"<code>run(benchmark_uid, mlcube_uid, priority)</code>  <code>staticmethod</code>","text":"<p>Sets priority for an association between a benchmark and an mlcube</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>Benchmark UID.</p> required <code>mlcube_uid</code> <code>int</code> <p>MLCube UID.</p> required <code>priority</code> <code>int</code> <p>priority value</p> required Source code in <code>cli/medperf/commands/association/priority.py</code> <pre><code>@staticmethod\ndef run(benchmark_uid: int, mlcube_uid: int, priority: int):\n\"\"\"Sets priority for an association between a benchmark and an mlcube\n    Args:\n        benchmark_uid (int): Benchmark UID.\n        mlcube_uid (int): MLCube UID.\n        priority (int): priority value\n    \"\"\"\nassociated_cubes = Benchmark.get_models_uids(benchmark_uid)\nif mlcube_uid not in associated_cubes:\nraise InvalidArgumentError(\n\"The given container doesn't exist or is not associated with the benchmark\"\n)\nconfig.comms.update_benchmark_model_association(\nbenchmark_uid, mlcube_uid, {\"priority\": priority}\n)\n</code></pre>"},{"location":"reference/commands/association/utils/","title":"Utils","text":""},{"location":"reference/commands/association/utils/#commands.association.utils.filter_latest_associations","title":"<code>filter_latest_associations(associations, experiment_key, component_key)</code>","text":"<p>Given a list of component-experiment associations, this function retrieves a list containing the latest association of each experiment-component instance.</p> <p>Parameters:</p> Name Type Description Default <code>associations</code> <code>list[dict]</code> <p>the list of associations</p> required <code>experiment_key</code> <code>str</code> <p>experiment identifier field in the association</p> required <code>component_key</code> <code>str</code> <p>component identifier field in the association</p> required <p>Returns:</p> Type Description <p>list[dict]: the list containing the latest association of each         entity instance.</p> Source code in <code>cli/medperf/commands/association/utils.py</code> <pre><code>def filter_latest_associations(associations, experiment_key, component_key):\n\"\"\"Given a list of component-experiment associations, this function\n    retrieves a list containing the latest association of each\n    experiment-component instance.\n    Args:\n        associations (list[dict]): the list of associations\n        experiment_key (str): experiment identifier field in the association\n        component_key (str): component identifier field in the association\n    Returns:\n        list[dict]: the list containing the latest association of each\n                    entity instance.\n    \"\"\"\nassociations.sort(key=lambda assoc: parse_datetime(assoc[\"created_at\"]))\nlatest_associations = {}\nfor assoc in associations:\ncomponent_id = assoc[component_key]\nexperiment_id = assoc[experiment_key]\nlatest_associations[(component_id, experiment_id)] = assoc\nlatest_associations = list(latest_associations.values())\nreturn latest_associations\n</code></pre>"},{"location":"reference/commands/auth/auth/","title":"Auth","text":""},{"location":"reference/commands/auth/auth/#commands.auth.auth.login","title":"<code>login(email=typer.Option(None, '--email', '-e', help='The email associated with your account'))</code>","text":"<p>Authenticate to be able to access the MedPerf server. A verification link will be provided and should be open in a browser to complete the login process.</p> Source code in <code>cli/medperf/commands/auth/auth.py</code> <pre><code>@app.command(\"login\")\n@clean_except\ndef login(\nemail: str = typer.Option(\nNone, \"--email\", \"-e\", help=\"The email associated with your account\"\n)\n):\n\"\"\"Authenticate to be able to access the MedPerf server. A verification link will\n    be provided and should be open in a browser to complete the login process.\"\"\"\nLogin.run(email)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/auth/auth/#commands.auth.auth.logout","title":"<code>logout()</code>","text":"<p>Revoke the currently active login state.</p> Source code in <code>cli/medperf/commands/auth/auth.py</code> <pre><code>@app.command(\"logout\")\n@clean_except\ndef logout():\n\"\"\"Revoke the currently active login state.\"\"\"\nLogout.run()\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/auth/auth/#commands.auth.auth.status","title":"<code>status()</code>","text":"<p>Shows the currently logged in user.</p> Source code in <code>cli/medperf/commands/auth/auth.py</code> <pre><code>@app.command(\"status\")\n@clean_except\ndef status():\n\"\"\"Shows the currently logged in user.\"\"\"\nStatus.run()\n</code></pre>"},{"location":"reference/commands/auth/auth/#commands.auth.auth.synapse_login","title":"<code>synapse_login(token=typer.Option(None, '--token', '-t', help='Personal Access Token to login with'))</code>","text":"<p>Login to the synapse server. Provide either a username and a password, or a token</p> Source code in <code>cli/medperf/commands/auth/auth.py</code> <pre><code>@app.command(\"synapse_login\")\n@clean_except\ndef synapse_login(\ntoken: str = typer.Option(\nNone, \"--token\", \"-t\", help=\"Personal Access Token to login with\"\n),\n):\n\"\"\"Login to the synapse server.\n    Provide either a username and a password, or a token\n    \"\"\"\nSynapseLogin.run(token=token)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/auth/login/","title":"Login","text":""},{"location":"reference/commands/auth/login/#commands.auth.login.Login","title":"<code>Login</code>","text":"Source code in <code>cli/medperf/commands/auth/login.py</code> <pre><code>class Login:\n@staticmethod\ndef run(email: str = None):\n\"\"\"Authenticate to be able to access the MedPerf server. A verification link will\n        be provided and should be open in a browser to complete the login process.\"\"\"\nraise_if_logged_in()\nif not email:\nemail = config.ui.prompt(\"Please type your email: \")\ntry:\nvalidate_email(email, check_deliverability=False)\nexcept EmailNotValidError as e:\nraise InvalidArgumentError(str(e))\nconfig.auth.login(email)\n</code></pre>"},{"location":"reference/commands/auth/login/#commands.auth.login.Login.run","title":"<code>run(email=None)</code>  <code>staticmethod</code>","text":"<p>Authenticate to be able to access the MedPerf server. A verification link will be provided and should be open in a browser to complete the login process.</p> Source code in <code>cli/medperf/commands/auth/login.py</code> <pre><code>@staticmethod\ndef run(email: str = None):\n\"\"\"Authenticate to be able to access the MedPerf server. A verification link will\n    be provided and should be open in a browser to complete the login process.\"\"\"\nraise_if_logged_in()\nif not email:\nemail = config.ui.prompt(\"Please type your email: \")\ntry:\nvalidate_email(email, check_deliverability=False)\nexcept EmailNotValidError as e:\nraise InvalidArgumentError(str(e))\nconfig.auth.login(email)\n</code></pre>"},{"location":"reference/commands/auth/logout/","title":"Logout","text":""},{"location":"reference/commands/auth/logout/#commands.auth.logout.Logout","title":"<code>Logout</code>","text":"Source code in <code>cli/medperf/commands/auth/logout.py</code> <pre><code>class Logout:\n@staticmethod\ndef run():\n\"\"\"Revoke the currently active login state.\"\"\"\nconfig.auth.logout()\n</code></pre>"},{"location":"reference/commands/auth/logout/#commands.auth.logout.Logout.run","title":"<code>run()</code>  <code>staticmethod</code>","text":"<p>Revoke the currently active login state.</p> Source code in <code>cli/medperf/commands/auth/logout.py</code> <pre><code>@staticmethod\ndef run():\n\"\"\"Revoke the currently active login state.\"\"\"\nconfig.auth.logout()\n</code></pre>"},{"location":"reference/commands/auth/status/","title":"Status","text":""},{"location":"reference/commands/auth/status/#commands.auth.status.Status","title":"<code>Status</code>","text":"Source code in <code>cli/medperf/commands/auth/status.py</code> <pre><code>class Status:\n@staticmethod\ndef run():\n\"\"\"Shows the currently logged in user.\"\"\"\naccount_info = read_user_account()\nif account_info is None:\nconfig.ui.print(\"You are not logged in\")\nreturn\nemail = account_info[\"email\"]\nconfig.ui.print(f\"Logged in user email address: {email}\")\n</code></pre>"},{"location":"reference/commands/auth/status/#commands.auth.status.Status.run","title":"<code>run()</code>  <code>staticmethod</code>","text":"<p>Shows the currently logged in user.</p> Source code in <code>cli/medperf/commands/auth/status.py</code> <pre><code>@staticmethod\ndef run():\n\"\"\"Shows the currently logged in user.\"\"\"\naccount_info = read_user_account()\nif account_info is None:\nconfig.ui.print(\"You are not logged in\")\nreturn\nemail = account_info[\"email\"]\nconfig.ui.print(f\"Logged in user email address: {email}\")\n</code></pre>"},{"location":"reference/commands/auth/synapse_login/","title":"Synapse login","text":""},{"location":"reference/commands/auth/synapse_login/#commands.auth.synapse_login.SynapseLogin","title":"<code>SynapseLogin</code>","text":"Source code in <code>cli/medperf/commands/auth/synapse_login.py</code> <pre><code>class SynapseLogin:\n@classmethod\ndef run(cls, token: str = None):\n\"\"\"Login to the Synapse server. Must be done only once.\"\"\"\nif not token:\nmsg = (\n\"Please provide your Synapse Personal Access Token (PAT). \"\n\"You can generate a new PAT at \"\n\"https://www.synapse.org/#!PersonalAccessTokens:0\\n\"\n\"Synapse PAT: \"\n)\ntoken = config.ui.hidden_prompt(msg)\ncls.login_with_token(token)\n@classmethod\ndef login_with_token(cls, access_token=None):\n\"\"\"Login to the Synapse server. Must be done only once.\"\"\"\nsyn = synapseclient.Synapse()\ntry:\nsyn.login(authToken=access_token)\nexcept SynapseAuthenticationError as err:\nraise CommunicationAuthenticationError(\"Invalid Synapse credentials\") from err\n</code></pre>"},{"location":"reference/commands/auth/synapse_login/#commands.auth.synapse_login.SynapseLogin.login_with_token","title":"<code>login_with_token(access_token=None)</code>  <code>classmethod</code>","text":"<p>Login to the Synapse server. Must be done only once.</p> Source code in <code>cli/medperf/commands/auth/synapse_login.py</code> <pre><code>@classmethod\ndef login_with_token(cls, access_token=None):\n\"\"\"Login to the Synapse server. Must be done only once.\"\"\"\nsyn = synapseclient.Synapse()\ntry:\nsyn.login(authToken=access_token)\nexcept SynapseAuthenticationError as err:\nraise CommunicationAuthenticationError(\"Invalid Synapse credentials\") from err\n</code></pre>"},{"location":"reference/commands/auth/synapse_login/#commands.auth.synapse_login.SynapseLogin.run","title":"<code>run(token=None)</code>  <code>classmethod</code>","text":"<p>Login to the Synapse server. Must be done only once.</p> Source code in <code>cli/medperf/commands/auth/synapse_login.py</code> <pre><code>@classmethod\ndef run(cls, token: str = None):\n\"\"\"Login to the Synapse server. Must be done only once.\"\"\"\nif not token:\nmsg = (\n\"Please provide your Synapse Personal Access Token (PAT). \"\n\"You can generate a new PAT at \"\n\"https://www.synapse.org/#!PersonalAccessTokens:0\\n\"\n\"Synapse PAT: \"\n)\ntoken = config.ui.hidden_prompt(msg)\ncls.login_with_token(token)\n</code></pre>"},{"location":"reference/commands/benchmark/associate/","title":"Associate","text":""},{"location":"reference/commands/benchmark/associate/#commands.benchmark.associate.AssociateBenchmark","title":"<code>AssociateBenchmark</code>","text":"Source code in <code>cli/medperf/commands/benchmark/associate.py</code> <pre><code>class AssociateBenchmark:\n@classmethod\ndef run(\ncls,\nbenchmark_uid: int,\nmodel_uid: int,\ndata_uid: int,\napproved=False,\nno_cache=False,\n):\n\"\"\"Associates a dataset or model to the given benchmark\n        Args:\n            benchmark_uid (int): UID of benchmark to associate entities with\n            model_uid (int): UID of model to associate with benchmark\n            data_uid (int): UID of dataset to associate with benchmark\n            comms (Comms): Instance of Communications interface\n            ui (UI): Instance of UI interface\n            approved (bool): Skip approval step. Defaults to False\n        \"\"\"\ntoo_many_resources = data_uid and model_uid\nno_resource = data_uid is None and model_uid is None\nif no_resource or too_many_resources:\nraise InvalidArgumentError(\"Must provide either a dataset or a model\")\nif model_uid is not None:\nAssociateCube.run(\nmodel_uid, benchmark_uid, approved=approved, no_cache=no_cache\n)\nif data_uid is not None:\nAssociateDataset.run(\ndata_uid, benchmark_uid, approved=approved, no_cache=no_cache\n)\n</code></pre>"},{"location":"reference/commands/benchmark/associate/#commands.benchmark.associate.AssociateBenchmark.run","title":"<code>run(benchmark_uid, model_uid, data_uid, approved=False, no_cache=False)</code>  <code>classmethod</code>","text":"<p>Associates a dataset or model to the given benchmark</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>UID of benchmark to associate entities with</p> required <code>model_uid</code> <code>int</code> <p>UID of model to associate with benchmark</p> required <code>data_uid</code> <code>int</code> <p>UID of dataset to associate with benchmark</p> required <code>comms</code> <code>Comms</code> <p>Instance of Communications interface</p> required <code>ui</code> <code>UI</code> <p>Instance of UI interface</p> required <code>approved</code> <code>bool</code> <p>Skip approval step. Defaults to False</p> <code>False</code> Source code in <code>cli/medperf/commands/benchmark/associate.py</code> <pre><code>@classmethod\ndef run(\ncls,\nbenchmark_uid: int,\nmodel_uid: int,\ndata_uid: int,\napproved=False,\nno_cache=False,\n):\n\"\"\"Associates a dataset or model to the given benchmark\n    Args:\n        benchmark_uid (int): UID of benchmark to associate entities with\n        model_uid (int): UID of model to associate with benchmark\n        data_uid (int): UID of dataset to associate with benchmark\n        comms (Comms): Instance of Communications interface\n        ui (UI): Instance of UI interface\n        approved (bool): Skip approval step. Defaults to False\n    \"\"\"\ntoo_many_resources = data_uid and model_uid\nno_resource = data_uid is None and model_uid is None\nif no_resource or too_many_resources:\nraise InvalidArgumentError(\"Must provide either a dataset or a model\")\nif model_uid is not None:\nAssociateCube.run(\nmodel_uid, benchmark_uid, approved=approved, no_cache=no_cache\n)\nif data_uid is not None:\nAssociateDataset.run(\ndata_uid, benchmark_uid, approved=approved, no_cache=no_cache\n)\n</code></pre>"},{"location":"reference/commands/benchmark/benchmark/","title":"Benchmark","text":""},{"location":"reference/commands/benchmark/benchmark/#commands.benchmark.benchmark.associate","title":"<code>associate(benchmark_uid=typer.Option(..., '--benchmark_uid', '-b', help='UID of benchmark to associate with'), model_uid=typer.Option(None, '--model_uid', '-m', help='UID of model container to associate'), dataset_uid=typer.Option(None, '--data_uid', '-d', help='Server UID of registered dataset to associate'), approval=typer.Option(False, '-y', help='Skip approval step'), no_cache=typer.Option(False, '--no-cache', help='Execute the test even if results already exist'))</code>","text":"<p>Associates a benchmark with a given model or dataset. Only one option at a time.</p> Source code in <code>cli/medperf/commands/benchmark/benchmark.py</code> <pre><code>@app.command(\"associate\")\n@clean_except\ndef associate(\nbenchmark_uid: int = typer.Option(\n..., \"--benchmark_uid\", \"-b\", help=\"UID of benchmark to associate with\"\n),\nmodel_uid: int = typer.Option(\nNone, \"--model_uid\", \"-m\", help=\"UID of model container to associate\"\n),\ndataset_uid: int = typer.Option(\nNone, \"--data_uid\", \"-d\", help=\"Server UID of registered dataset to associate\"\n),\napproval: bool = typer.Option(False, \"-y\", help=\"Skip approval step\"),\nno_cache: bool = typer.Option(\nFalse,\n\"--no-cache\",\nhelp=\"Execute the test even if results already exist\",\n),\n):\n\"\"\"Associates a benchmark with a given model or dataset. Only one option at a time.\"\"\"\nAssociateBenchmark.run(\nbenchmark_uid, model_uid, dataset_uid, approved=approval, no_cache=no_cache\n)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/benchmark/benchmark/#commands.benchmark.benchmark.list","title":"<code>list(unregistered=typer.Option(False, '--unregistered', help='Get unregistered benchmarks'), mine=typer.Option(False, '--mine', help='Get current-user benchmarks'), name=typer.Option(None, '--name', help='Filter by name'), owner=typer.Option(None, '--owner', help='Filter by owner'), state=typer.Option(None, '--state', help='Filter by state (DEVELOPMENT/OPERATION)'), is_valid=typer.Option(None, '--valid/--invalid', help='Filter by valid status'), is_active=typer.Option(None, '--active/--inactive', help='Filter by active status'), data_prep=typer.Option(None, '-d', '--data-preparation-container', help='Filter by Data Preparation Container'))</code>","text":"<p>List benchmarks</p> Source code in <code>cli/medperf/commands/benchmark/benchmark.py</code> <pre><code>@app.command(\"ls\")\n@clean_except\ndef list(\nunregistered: bool = typer.Option(\nFalse, \"--unregistered\", help=\"Get unregistered benchmarks\"\n),\nmine: bool = typer.Option(False, \"--mine\", help=\"Get current-user benchmarks\"),\nname: str = typer.Option(None, \"--name\", help=\"Filter by name\"),\nowner: int = typer.Option(None, \"--owner\", help=\"Filter by owner\"),\nstate: str = typer.Option(\nNone, \"--state\", help=\"Filter by state (DEVELOPMENT/OPERATION)\"\n),\nis_valid: bool = typer.Option(\nNone, \"--valid/--invalid\", help=\"Filter by valid status\"\n),\nis_active: bool = typer.Option(\nNone, \"--active/--inactive\", help=\"Filter by active status\"\n),\ndata_prep: int = typer.Option(\nNone,\n\"-d\",\n\"--data-preparation-container\",\nhelp=\"Filter by Data Preparation Container\",\n),\n):\n\"\"\"List benchmarks\"\"\"\nfilters = {\n\"name\": name,\n\"owner\": owner,\n\"state\": state,\n\"is_valid\": is_valid,\n\"is_active\": is_active,\n\"data_preparation_mlcube\": data_prep,\n}\nEntityList.run(\nBenchmark,\nfields=[\n\"UID\",\n\"Name\",\n\"Description\",\n\"Data Preparation Container\",\n\"State\",\n\"Approval Status\",\n\"Registered\",\n],\nunregistered=unregistered,\nmine_only=mine,\n**filters,\n)\n</code></pre>"},{"location":"reference/commands/benchmark/benchmark/#commands.benchmark.benchmark.run","title":"<code>run(benchmark_uid=typer.Option(..., '--benchmark', '-b', help='UID of the desired benchmark'), data_uid=typer.Option(..., '--data_uid', '-d', help='Registered Dataset UID'), file=typer.Option(None, '--models-from-file', '-f', help='A file containing the model UIDs to be executed.\\n\\n        The file should contain a single line as a list of\\n\\n        comma-separated integers corresponding to the model UIDs'), ignore_model_errors=typer.Option(False, '--ignore-model-errors', help='Ignore failing models, allowing for possibly submitting partial results'), no_cache=typer.Option(False, '--no-cache', help='Execute even if results already exist'), rerun_finalized=typer.Option(False, '--rerun-finalized', help='Execute even if results have been already uploaded (this will create new records)'))</code>","text":"<p>Runs the benchmark execution step for a given benchmark, prepared dataset and model</p> Source code in <code>cli/medperf/commands/benchmark/benchmark.py</code> <pre><code>@app.command(\"run\")\n@clean_except\ndef run(\nbenchmark_uid: int = typer.Option(\n..., \"--benchmark\", \"-b\", help=\"UID of the desired benchmark\"\n),\ndata_uid: int = typer.Option(\n..., \"--data_uid\", \"-d\", help=\"Registered Dataset UID\"\n),\nfile: str = typer.Option(\nNone,\n\"--models-from-file\",\n\"-f\",\nhelp=\"\"\"A file containing the model UIDs to be executed.\\n\n        The file should contain a single line as a list of\\n\n        comma-separated integers corresponding to the model UIDs\"\"\",\n),\nignore_model_errors: bool = typer.Option(\nFalse,\n\"--ignore-model-errors\",\nhelp=\"Ignore failing models, allowing for possibly submitting partial results\",\n),\nno_cache: bool = typer.Option(\nFalse,\n\"--no-cache\",\nhelp=\"Execute even if results already exist\",\n),\nrerun_finalized: bool = typer.Option(\nFalse,\n\"--rerun-finalized\",\nhelp=\"Execute even if results have been already uploaded (this will create new records)\",\n),\n):\n\"\"\"Runs the benchmark execution step for a given benchmark, prepared dataset and model\"\"\"\nBenchmarkExecution.run(\nbenchmark_uid,\ndata_uid,\nmodels_uids=None,\nmodels_input_file=file,\nignore_model_errors=ignore_model_errors,\nno_cache=no_cache,\nshow_summary=True,\nignore_failed_experiments=True,\nrerun_finalized_executions=rerun_finalized,\n)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/benchmark/benchmark/#commands.benchmark.benchmark.submit","title":"<code>submit(name=typer.Option(..., '--name', '-n', help='Name of the benchmark'), description=typer.Option(..., '--description', '-d', help='Description of the benchmark'), docs_url=typer.Option('', '--docs-url', '-u', help='URL to documentation'), demo_url=typer.Option(..., '--demo-url', help='Identifier to download the demonstration dataset tarball file.\\n\\n        See `medperf container submit --help` for more information'), demo_hash=typer.Option('', '--demo-hash', help='Hash of demonstration dataset tarball file'), data_preparation_container=typer.Option(..., '--data-preparation-container', '-p', help='Data Preparation container UID'), reference_model_container=typer.Option(..., '--reference-model-container', '-m', help='Reference Model container UID'), evaluator_container=typer.Option(..., '--evaluator-container', '-e', help='Evaluator container UID'), skip_data_preparation_step=typer.Option(False, '--skip-demo-data-preparation', help='Use this flag if the demo dataset is already prepared'), operational=typer.Option(False, '--operational', help='Submit the Benchmark as OPERATIONAL'))</code>","text":"<p>Submits a new benchmark to the platform</p> Source code in <code>cli/medperf/commands/benchmark/benchmark.py</code> <pre><code>@app.command(\"submit\")\n@clean_except\ndef submit(\nname: str = typer.Option(..., \"--name\", \"-n\", help=\"Name of the benchmark\"),\ndescription: str = typer.Option(\n..., \"--description\", \"-d\", help=\"Description of the benchmark\"\n),\ndocs_url: str = typer.Option(\"\", \"--docs-url\", \"-u\", help=\"URL to documentation\"),\ndemo_url: str = typer.Option(\n...,\n\"--demo-url\",\nhelp=\"\"\"Identifier to download the demonstration dataset tarball file.\\n\n        See `medperf container submit --help` for more information\"\"\",\n),\ndemo_hash: str = typer.Option(\n\"\", \"--demo-hash\", help=\"Hash of demonstration dataset tarball file\"\n),\ndata_preparation_container: int = typer.Option(\n..., \"--data-preparation-container\", \"-p\", help=\"Data Preparation container UID\"\n),\nreference_model_container: int = typer.Option(\n..., \"--reference-model-container\", \"-m\", help=\"Reference Model container UID\"\n),\nevaluator_container: int = typer.Option(\n..., \"--evaluator-container\", \"-e\", help=\"Evaluator container UID\"\n),\nskip_data_preparation_step: bool = typer.Option(\nFalse,\n\"--skip-demo-data-preparation\",\nhelp=\"Use this flag if the demo dataset is already prepared\",\n),\noperational: bool = typer.Option(\nFalse,\n\"--operational\",\nhelp=\"Submit the Benchmark as OPERATIONAL\",\n),\n):\n\"\"\"Submits a new benchmark to the platform\"\"\"\nbenchmark_info = {\n\"name\": name,\n\"description\": description,\n\"docs_url\": docs_url,\n\"demo_dataset_tarball_url\": demo_url,\n\"demo_dataset_tarball_hash\": demo_hash,\n\"data_preparation_mlcube\": data_preparation_container,\n\"reference_model_mlcube\": reference_model_container,\n\"data_evaluator_mlcube\": evaluator_container,\n\"state\": \"OPERATION\" if operational else \"DEVELOPMENT\",\n}\nSubmitBenchmark.run(\nbenchmark_info,\nskip_data_preparation_step=skip_data_preparation_step,\n)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/benchmark/benchmark/#commands.benchmark.benchmark.view","title":"<code>view(entity_id=typer.Argument(None, help='Benchmark ID'), format=typer.Option('yaml', '-f', '--format', help='Format to display contents. Available formats: [yaml, json]'), unregistered=typer.Option(False, '--unregistered', help='Display unregistered benchmarks if benchmark ID is not provided'), mine=typer.Option(False, '--mine', help='Display current-user benchmarks if benchmark ID is not provided'), output=typer.Option(None, '--output', '-o', help='Output file to store contents. If not provided, the output will be displayed'))</code>","text":"<p>Displays the information of one or more benchmarks</p> Source code in <code>cli/medperf/commands/benchmark/benchmark.py</code> <pre><code>@app.command(\"view\")\n@clean_except\ndef view(\nentity_id: Optional[int] = typer.Argument(None, help=\"Benchmark ID\"),\nformat: str = typer.Option(\n\"yaml\",\n\"-f\",\n\"--format\",\nhelp=\"Format to display contents. Available formats: [yaml, json]\",\n),\nunregistered: bool = typer.Option(\nFalse,\n\"--unregistered\",\nhelp=\"Display unregistered benchmarks if benchmark ID is not provided\",\n),\nmine: bool = typer.Option(\nFalse,\n\"--mine\",\nhelp=\"Display current-user benchmarks if benchmark ID is not provided\",\n),\noutput: str = typer.Option(\nNone,\n\"--output\",\n\"-o\",\nhelp=\"Output file to store contents. If not provided, the output will be displayed\",\n),\n):\n\"\"\"Displays the information of one or more benchmarks\"\"\"\nEntityView.run(entity_id, Benchmark, format, unregistered, mine, output)\n</code></pre>"},{"location":"reference/commands/benchmark/submit/","title":"Submit","text":""},{"location":"reference/commands/benchmark/submit/#commands.benchmark.submit.SubmitBenchmark","title":"<code>SubmitBenchmark</code>","text":"Source code in <code>cli/medperf/commands/benchmark/submit.py</code> <pre><code>class SubmitBenchmark:\n@classmethod\ndef run(\ncls,\nbenchmark_info: dict,\nno_cache: bool = True,\nskip_data_preparation_step: bool = False,\n):\n\"\"\"Submits a new cube to the medperf platform\n        Args:\n            benchmark_info (dict): benchmark information\n                expected keys:\n                    name (str): benchmark name\n                    description (str): benchmark description\n                    docs_url (str): benchmark documentation url\n                    demo_url (str): benchmark demo dataset url\n                    demo_hash (str): benchmark demo dataset hash\n                    data_preparation_mlcube (int): benchmark data preparation mlcube uid\n                    reference_model_mlcube (int): benchmark reference model mlcube uid\n                    evaluator_mlcube (int): benchmark data evaluator mlcube uid\n        \"\"\"\nui = config.ui\nsubmission = cls(benchmark_info, no_cache, skip_data_preparation_step)\nwith ui.interactive():\nui.text = \"Getting additional information\"\nsubmission.get_extra_information()\nui.print(\"&gt; Completed benchmark registration information\")\nui.text = \"Submitting Benchmark to MedPerf\"\nupdated_benchmark_body = submission.submit()\nui.print(\"Uploaded\")\nsubmission.to_permanent_path(updated_benchmark_body)\nsubmission.write(updated_benchmark_body)\nreturn submission.bmk.id\ndef __init__(\nself,\nbenchmark_info: dict,\nno_cache: bool = True,\nskip_data_preparation_step: bool = False,\n):\nself.ui = config.ui\nself.bmk = Benchmark(**benchmark_info)\nself.no_cache = no_cache\nself.skip_data_preparation_step = skip_data_preparation_step\nself.bmk.metadata[\"demo_dataset_already_prepared\"] = skip_data_preparation_step\nconfig.tmp_paths.append(self.bmk.path)\ndef get_extra_information(self):\n\"\"\"Retrieves information that must be populated automatically,\n        like hash, generated uid and test results\n        \"\"\"\nbmk_demo_url = self.bmk.demo_dataset_tarball_url\nbmk_demo_hash = self.bmk.demo_dataset_tarball_hash\ntry:\n_, demo_hash = resources.get_benchmark_demo_dataset(\nbmk_demo_url, bmk_demo_hash\n)\nexcept InvalidEntityError as e:\nraise InvalidEntityError(f\"Demo dataset {bmk_demo_url}: {e}\")\nself.bmk.demo_dataset_tarball_hash = demo_hash\ndemo_uid, results = self.run_compatibility_test()\nself.bmk.demo_dataset_generated_uid = demo_uid\nself.bmk.metadata[\"results\"] = results\ndef run_compatibility_test(self):\n\"\"\"Runs a compatibility test to ensure elements are compatible,\n        and to extract additional information required for submission\n        \"\"\"\nself.ui.print(\"Running compatibility test\")\nself.bmk.write()\ndata_uid, results = CompatibilityTestExecution.run(\nbenchmark=self.bmk.local_id,\nno_cache=self.no_cache,\nskip_data_preparation_step=self.skip_data_preparation_step,\n)\nreturn data_uid, results\ndef submit(self):\nupdated_body = self.bmk.upload()\nreturn updated_body\ndef to_permanent_path(self, bmk_dict: dict):\n\"\"\"Renames the temporary benchmark submission to a permanent one\n        Args:\n            bmk_dict (dict): dictionary containing updated information of the submitted benchmark\n        \"\"\"\nold_bmk_loc = self.bmk.path\nupdated_bmk = Benchmark(**bmk_dict)\nnew_bmk_loc = updated_bmk.path\nremove_path(new_bmk_loc)\nos.rename(old_bmk_loc, new_bmk_loc)\ndef write(self, updated_body):\nself.bmk = Benchmark(**updated_body)\nself.bmk.write()\n</code></pre>"},{"location":"reference/commands/benchmark/submit/#commands.benchmark.submit.SubmitBenchmark.get_extra_information","title":"<code>get_extra_information()</code>","text":"<p>Retrieves information that must be populated automatically, like hash, generated uid and test results</p> Source code in <code>cli/medperf/commands/benchmark/submit.py</code> <pre><code>def get_extra_information(self):\n\"\"\"Retrieves information that must be populated automatically,\n    like hash, generated uid and test results\n    \"\"\"\nbmk_demo_url = self.bmk.demo_dataset_tarball_url\nbmk_demo_hash = self.bmk.demo_dataset_tarball_hash\ntry:\n_, demo_hash = resources.get_benchmark_demo_dataset(\nbmk_demo_url, bmk_demo_hash\n)\nexcept InvalidEntityError as e:\nraise InvalidEntityError(f\"Demo dataset {bmk_demo_url}: {e}\")\nself.bmk.demo_dataset_tarball_hash = demo_hash\ndemo_uid, results = self.run_compatibility_test()\nself.bmk.demo_dataset_generated_uid = demo_uid\nself.bmk.metadata[\"results\"] = results\n</code></pre>"},{"location":"reference/commands/benchmark/submit/#commands.benchmark.submit.SubmitBenchmark.run","title":"<code>run(benchmark_info, no_cache=True, skip_data_preparation_step=False)</code>  <code>classmethod</code>","text":"<p>Submits a new cube to the medperf platform</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_info</code> <code>dict</code> <p>benchmark information expected keys:     name (str): benchmark name     description (str): benchmark description     docs_url (str): benchmark documentation url     demo_url (str): benchmark demo dataset url     demo_hash (str): benchmark demo dataset hash     data_preparation_mlcube (int): benchmark data preparation mlcube uid     reference_model_mlcube (int): benchmark reference model mlcube uid     evaluator_mlcube (int): benchmark data evaluator mlcube uid</p> required Source code in <code>cli/medperf/commands/benchmark/submit.py</code> <pre><code>@classmethod\ndef run(\ncls,\nbenchmark_info: dict,\nno_cache: bool = True,\nskip_data_preparation_step: bool = False,\n):\n\"\"\"Submits a new cube to the medperf platform\n    Args:\n        benchmark_info (dict): benchmark information\n            expected keys:\n                name (str): benchmark name\n                description (str): benchmark description\n                docs_url (str): benchmark documentation url\n                demo_url (str): benchmark demo dataset url\n                demo_hash (str): benchmark demo dataset hash\n                data_preparation_mlcube (int): benchmark data preparation mlcube uid\n                reference_model_mlcube (int): benchmark reference model mlcube uid\n                evaluator_mlcube (int): benchmark data evaluator mlcube uid\n    \"\"\"\nui = config.ui\nsubmission = cls(benchmark_info, no_cache, skip_data_preparation_step)\nwith ui.interactive():\nui.text = \"Getting additional information\"\nsubmission.get_extra_information()\nui.print(\"&gt; Completed benchmark registration information\")\nui.text = \"Submitting Benchmark to MedPerf\"\nupdated_benchmark_body = submission.submit()\nui.print(\"Uploaded\")\nsubmission.to_permanent_path(updated_benchmark_body)\nsubmission.write(updated_benchmark_body)\nreturn submission.bmk.id\n</code></pre>"},{"location":"reference/commands/benchmark/submit/#commands.benchmark.submit.SubmitBenchmark.run_compatibility_test","title":"<code>run_compatibility_test()</code>","text":"<p>Runs a compatibility test to ensure elements are compatible, and to extract additional information required for submission</p> Source code in <code>cli/medperf/commands/benchmark/submit.py</code> <pre><code>def run_compatibility_test(self):\n\"\"\"Runs a compatibility test to ensure elements are compatible,\n    and to extract additional information required for submission\n    \"\"\"\nself.ui.print(\"Running compatibility test\")\nself.bmk.write()\ndata_uid, results = CompatibilityTestExecution.run(\nbenchmark=self.bmk.local_id,\nno_cache=self.no_cache,\nskip_data_preparation_step=self.skip_data_preparation_step,\n)\nreturn data_uid, results\n</code></pre>"},{"location":"reference/commands/benchmark/submit/#commands.benchmark.submit.SubmitBenchmark.to_permanent_path","title":"<code>to_permanent_path(bmk_dict)</code>","text":"<p>Renames the temporary benchmark submission to a permanent one</p> <p>Parameters:</p> Name Type Description Default <code>bmk_dict</code> <code>dict</code> <p>dictionary containing updated information of the submitted benchmark</p> required Source code in <code>cli/medperf/commands/benchmark/submit.py</code> <pre><code>def to_permanent_path(self, bmk_dict: dict):\n\"\"\"Renames the temporary benchmark submission to a permanent one\n    Args:\n        bmk_dict (dict): dictionary containing updated information of the submitted benchmark\n    \"\"\"\nold_bmk_loc = self.bmk.path\nupdated_bmk = Benchmark(**bmk_dict)\nnew_bmk_loc = updated_bmk.path\nremove_path(new_bmk_loc)\nos.rename(old_bmk_loc, new_bmk_loc)\n</code></pre>"},{"location":"reference/commands/benchmark/update_associations_poilcy/","title":"Update associations poilcy","text":""},{"location":"reference/commands/benchmark/update_associations_poilcy/#commands.benchmark.update_associations_poilcy.UpdateAssociationsPolicy","title":"<code>UpdateAssociationsPolicy</code>","text":"Source code in <code>cli/medperf/commands/benchmark/update_associations_poilcy.py</code> <pre><code>class UpdateAssociationsPolicy:\n@classmethod\ndef run(\ncls,\nbenchmark_uid: int,\ndataset_mode: str = None,\ndataset_emails_file: str = None,\ndataset_emails: str = None,\nmodel_mode: str = None,\nmodel_emails_file: str = None,\nmodel_emails: str = None,\n):\n\"\"\"\n        dataset_emails: a string containing space-separated list of emails\n        model_emails: a string containing space-separated list of emails\n        \"\"\"\nupdate_policy = cls(\nbenchmark_uid,\ndataset_mode,\ndataset_emails_file,\ndataset_emails,\nmodel_mode,\nmodel_emails_file,\nmodel_emails,\n)\nupdate_policy.validate()\nupdate_policy.read_emails()\nupdate_policy.validate_emails()\nupdate_policy.update()\ndef __init__(\nself,\nbenchmark_uid: int,\ndataset_mode: str = None,\ndataset_emails_file: str = None,\ndataset_emails: str = None,\nmodel_mode: str = None,\nmodel_emails_file: str = None,\nmodel_emails: str = None,\n):\nself.benchmark_uid = benchmark_uid\nself.dataset_mode = dataset_mode\nself.dataset_emails_file = sanitize_path(dataset_emails_file)\nself.dataset_emails = dataset_emails\nself.model_mode = model_mode\nself.model_emails_file = sanitize_path(model_emails_file)\nself.model_emails = model_emails\ndef __validate_mode(self, mode):\nif mode is None:\nreturn\nmodes = [e.value for e in AutoApprovalMode]\nmode = mode.upper()\nif mode not in modes:\nraise InvalidArgumentError(\nf\"auto_approve_mode should be one of {modes}. Got {mode}\"\n)\nreturn mode\ndef validate(self):\n# validate modes\nself.dataset_mode = self.__validate_mode(self.dataset_mode)\nself.model_mode = self.__validate_mode(self.model_mode)\n# File and list shouldn't both be provided\nif self.dataset_emails_file is not None and self.dataset_emails is not None:\nraise MedperfException(\n\"Internal Error: Both a file and a list of emails are provided.\"\n)\nif self.model_emails_file is not None and self.model_emails is not None:\nraise MedperfException(\n\"Internal Error: Both a file and a list of emails are provided.\"\n)\n# validate files if provided\nif self.dataset_emails_file and not os.path.isfile(self.dataset_emails_file):\nraise InvalidArgumentError(\nf\"File {self.dataset_emails_file} does not exist or is a directory\"\n)\nif self.model_emails_file and not os.path.isfile(self.model_emails_file):\nraise InvalidArgumentError(\nf\"File {self.dataset_emails_file} does not exist or is a directory\"\n)\ndef __read_emails_file(self, file):\nwith open(file) as f:\ncontents = f.read()\nallowed_emails = contents.strip().split(\"\\n\")\nreturn allowed_emails\ndef read_emails(self):\nif self.dataset_emails_file is not None:\nself.dataset_emails = self.__read_emails_file(self.dataset_emails_file)\nelif self.dataset_emails is not None:\nself.dataset_emails = self.dataset_emails.strip().split(\" \")\nif self.model_emails_file is not None:\nself.model_emails = self.__read_emails_file(self.model_emails_file)\nelif self.model_emails is not None:\nself.model_emails = self.model_emails.strip().split(\" \")\ndef __validate_emails(self, emails: list[str]):\nemails = [email.lower().strip() for email in emails if email.strip()]\nfor email in emails:\ntry:\nvalidate_email(email, check_deliverability=False)\nexcept EmailNotValidError as e:\nraise InvalidArgumentError(str(e))\nreturn emails\ndef validate_emails(self):\nif self.dataset_emails is not None:\nself.dataset_emails = self.__validate_emails(self.dataset_emails)\nif self.model_emails is not None:\nself.model_emails = self.__validate_emails(self.model_emails)\ndef update(self):\nif all(\n[\nself.dataset_emails is None\nand self.model_emails is None\nand self.dataset_mode is None\nand self.model_mode is None\n]\n):\nreturn\nbody = {}\nif self.dataset_emails is not None:\nbody[\"dataset_auto_approval_allow_list\"] = self.dataset_emails\nif self.model_emails is not None:\nbody[\"model_auto_approval_allow_list\"] = self.model_emails\nif self.dataset_mode is not None:\nbody[\"dataset_auto_approval_mode\"] = self.dataset_mode\nif self.model_mode is not None:\nbody[\"model_auto_approval_mode\"] = self.model_mode\nconfig.comms.update_benchmark(self.benchmark_uid, body)\n</code></pre>"},{"location":"reference/commands/benchmark/update_associations_poilcy/#commands.benchmark.update_associations_poilcy.UpdateAssociationsPolicy.run","title":"<code>run(benchmark_uid, dataset_mode=None, dataset_emails_file=None, dataset_emails=None, model_mode=None, model_emails_file=None, model_emails=None)</code>  <code>classmethod</code>","text":"<p>dataset_emails: a string containing space-separated list of emails model_emails: a string containing space-separated list of emails</p> Source code in <code>cli/medperf/commands/benchmark/update_associations_poilcy.py</code> <pre><code>@classmethod\ndef run(\ncls,\nbenchmark_uid: int,\ndataset_mode: str = None,\ndataset_emails_file: str = None,\ndataset_emails: str = None,\nmodel_mode: str = None,\nmodel_emails_file: str = None,\nmodel_emails: str = None,\n):\n\"\"\"\n    dataset_emails: a string containing space-separated list of emails\n    model_emails: a string containing space-separated list of emails\n    \"\"\"\nupdate_policy = cls(\nbenchmark_uid,\ndataset_mode,\ndataset_emails_file,\ndataset_emails,\nmodel_mode,\nmodel_emails_file,\nmodel_emails,\n)\nupdate_policy.validate()\nupdate_policy.read_emails()\nupdate_policy.validate_emails()\nupdate_policy.update()\n</code></pre>"},{"location":"reference/commands/ca/associate/","title":"Associate","text":""},{"location":"reference/commands/ca/associate/#commands.ca.associate.AssociateCA","title":"<code>AssociateCA</code>","text":"Source code in <code>cli/medperf/commands/ca/associate.py</code> <pre><code>class AssociateCA:\n@staticmethod\ndef run(training_exp_id: int, ca_uid: int, approved=False):\n\"\"\"Associates an ca with a training experiment\n        Args:\n            ca_uid (int): UID of the registered ca to associate\n            benchmark_uid (int): UID of the benchmark to associate with\n        \"\"\"\ncomms = config.comms\nui = config.ui\nca = CA.get(ca_uid)\nif ca.id is None:\nmsg = \"The provided ca is not registered.\"\nraise InvalidArgumentError(msg)\ntraining_exp = TrainingExp.get(training_exp_id)\nmsg = \"Please confirm that you would like to associate\"\nmsg += f\" the ca {ca.name} with the training exp {training_exp.name}.\"\nmsg += \" [Y/n]\"\napproved = approved or approval_prompt(msg)\nif approved:\nui.print(\"Generating ca training association\")\ncomms.associate_training_ca(ca.id, training_exp_id)\nelse:\nui.print(\"CA association operation cancelled.\")\n</code></pre>"},{"location":"reference/commands/ca/associate/#commands.ca.associate.AssociateCA.run","title":"<code>run(training_exp_id, ca_uid, approved=False)</code>  <code>staticmethod</code>","text":"<p>Associates an ca with a training experiment</p> <p>Parameters:</p> Name Type Description Default <code>ca_uid</code> <code>int</code> <p>UID of the registered ca to associate</p> required <code>benchmark_uid</code> <code>int</code> <p>UID of the benchmark to associate with</p> required Source code in <code>cli/medperf/commands/ca/associate.py</code> <pre><code>@staticmethod\ndef run(training_exp_id: int, ca_uid: int, approved=False):\n\"\"\"Associates an ca with a training experiment\n    Args:\n        ca_uid (int): UID of the registered ca to associate\n        benchmark_uid (int): UID of the benchmark to associate with\n    \"\"\"\ncomms = config.comms\nui = config.ui\nca = CA.get(ca_uid)\nif ca.id is None:\nmsg = \"The provided ca is not registered.\"\nraise InvalidArgumentError(msg)\ntraining_exp = TrainingExp.get(training_exp_id)\nmsg = \"Please confirm that you would like to associate\"\nmsg += f\" the ca {ca.name} with the training exp {training_exp.name}.\"\nmsg += \" [Y/n]\"\napproved = approved or approval_prompt(msg)\nif approved:\nui.print(\"Generating ca training association\")\ncomms.associate_training_ca(ca.id, training_exp_id)\nelse:\nui.print(\"CA association operation cancelled.\")\n</code></pre>"},{"location":"reference/commands/ca/ca/","title":"Ca","text":""},{"location":"reference/commands/ca/ca/#commands.ca.ca.associate","title":"<code>associate(ca_id=typer.Option(..., '--ca_id', '-c', help='UID of CA to associate with'), training_exp_id=typer.Option(..., '--training_exp_id', '-t', help='UID of training exp to associate with'), approval=typer.Option(False, '-y', help='Skip approval step'))</code>","text":"<p>Associates a CA with a training experiment.</p> Source code in <code>cli/medperf/commands/ca/ca.py</code> <pre><code>@app.command(\"associate\")\n@clean_except\ndef associate(\nca_id: int = typer.Option(..., \"--ca_id\", \"-c\", help=\"UID of CA to associate with\"),\ntraining_exp_id: int = typer.Option(\n..., \"--training_exp_id\", \"-t\", help=\"UID of training exp to associate with\"\n),\napproval: bool = typer.Option(False, \"-y\", help=\"Skip approval step\"),\n):\n\"\"\"Associates a CA with a training experiment.\"\"\"\nAssociateCA.run(ca_id, training_exp_id, approved=approval)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/ca/ca/#commands.ca.ca.list","title":"<code>list(unregistered=typer.Option(False, '--unregistered', help='Get unregistered CAs'), mine=typer.Option(False, '--mine', help='Get current-user CAs'))</code>","text":"<p>List CAs</p> Source code in <code>cli/medperf/commands/ca/ca.py</code> <pre><code>@app.command(\"ls\")\n@clean_except\ndef list(\nunregistered: bool = typer.Option(\nFalse, \"--unregistered\", help=\"Get unregistered CAs\"\n),\nmine: bool = typer.Option(False, \"--mine\", help=\"Get current-user CAs\"),\n):\n\"\"\"List CAs\"\"\"\nEntityList.run(\nCA,\nfields=[\"UID\", \"Name\", \"Address\", \"Port\"],\nunregistered=unregistered,\nmine_only=mine,\n)\n</code></pre>"},{"location":"reference/commands/ca/ca/#commands.ca.ca.submit","title":"<code>submit(name=typer.Option(..., '--name', '-n', help='Name of the ca'), config_path=typer.Option(..., '--config-path', '-c', help='Path to the configuration file (JSON) of the CA'), ca_mlcube=typer.Option(..., '--ca-container', help='CA container UID'), client_mlcube=typer.Option(..., '--client-container', help='container UID to be used by clients to get a cert'), server_mlcube=typer.Option(..., '--server-container', help='container UID to be used by servers to get a cert'))</code>","text":"<p>Submits a ca</p> Source code in <code>cli/medperf/commands/ca/ca.py</code> <pre><code>@app.command(\"submit\")\n@clean_except\ndef submit(\nname: str = typer.Option(..., \"--name\", \"-n\", help=\"Name of the ca\"),\nconfig_path: str = typer.Option(\n...,\n\"--config-path\",\n\"-c\",\nhelp=\"Path to the configuration file (JSON) of the CA\",\n),\nca_mlcube: int = typer.Option(..., \"--ca-container\", help=\"CA container UID\"),\nclient_mlcube: int = typer.Option(\n...,\n\"--client-container\",\nhelp=\"container UID to be used by clients to get a cert\",\n),\nserver_mlcube: int = typer.Option(\n...,\n\"--server-container\",\nhelp=\"container UID to be used by servers to get a cert\",\n),\n):\n\"\"\"Submits a ca\"\"\"\nSubmitCA.run(name, config_path, ca_mlcube, client_mlcube, server_mlcube)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/ca/ca/#commands.ca.ca.view","title":"<code>view(entity_id=typer.Argument(None, help='Benchmark ID'), format=typer.Option('yaml', '-f', '--format', help='Format to display contents. Available formats: [yaml, json]'), unregistered=typer.Option(False, '--unregistered', help='Display unregistered benchmarks if benchmark ID is not provided'), mine=typer.Option(False, '--mine', help='Display current-user benchmarks if benchmark ID is not provided'), output=typer.Option(None, '--output', '-o', help='Output file to store contents. If not provided, the output will be displayed'))</code>","text":"<p>Displays the information of one or more CAs</p> Source code in <code>cli/medperf/commands/ca/ca.py</code> <pre><code>@app.command(\"view\")\n@clean_except\ndef view(\nentity_id: Optional[int] = typer.Argument(None, help=\"Benchmark ID\"),\nformat: str = typer.Option(\n\"yaml\",\n\"-f\",\n\"--format\",\nhelp=\"Format to display contents. Available formats: [yaml, json]\",\n),\nunregistered: bool = typer.Option(\nFalse,\n\"--unregistered\",\nhelp=\"Display unregistered benchmarks if benchmark ID is not provided\",\n),\nmine: bool = typer.Option(\nFalse,\n\"--mine\",\nhelp=\"Display current-user benchmarks if benchmark ID is not provided\",\n),\noutput: str = typer.Option(\nNone,\n\"--output\",\n\"-o\",\nhelp=\"Output file to store contents. If not provided, the output will be displayed\",\n),\n):\n\"\"\"Displays the information of one or more CAs\"\"\"\nEntityView.run(entity_id, CA, format, unregistered, mine, output)\n</code></pre>"},{"location":"reference/commands/ca/submit/","title":"Submit","text":""},{"location":"reference/commands/ca/submit/#commands.ca.submit.SubmitCA","title":"<code>SubmitCA</code>","text":"Source code in <code>cli/medperf/commands/ca/submit.py</code> <pre><code>class SubmitCA:\n@classmethod\ndef run(\ncls,\nname: str,\nconfig_path: str,\nca_mlcube: int,\nclient_mlcube: int,\nserver_mlcube: int,\n):\n\"\"\"Submits a new ca to the medperf platform\n        Args:\n            name (str): ca name\n            config_path (dict): ca config\n            ca_mlcube (int): ca_mlcube mlcube uid\n            client_mlcube (int): client_mlcube mlcube uid\n            server_mlcube (int): server_mlcube mlcube uid\n        \"\"\"\nui = config.ui\nsubmission = cls(name, config_path, ca_mlcube, client_mlcube, server_mlcube)\nwith ui.interactive():\nui.text = \"Submitting CA to MedPerf\"\nsubmission.validate_ca_cubes()\nupdated_benchmark_body = submission.submit()\nui.print(\"Uploaded\")\nsubmission.write(updated_benchmark_body)\ndef __init__(\nself,\nname: str,\nconfig_path: str,\nca_mlcube: int,\nclient_mlcube: int,\nserver_mlcube: int,\n):\nself.ui = config.ui\nself.ca = CA(\nname=name,\nconfig=config_path,\nca_mlcube=ca_mlcube,\nclient_mlcube=client_mlcube,\nserver_mlcube=server_mlcube,\n)\nconfig.tmp_paths.append(self.ca.path)\ndef validate_ca_cubes(self):\nCube.get(self.ca.ca_mlcube)\nCube.get(self.ca.client_mlcube)\nCube.get(self.ca.server_mlcube)\ndef submit(self):\nupdated_body = self.ca.upload()\nreturn updated_body\ndef write(self, updated_body):\nremove_path(self.ca.path)\nca = CA(**updated_body)\nca.write()\n</code></pre>"},{"location":"reference/commands/ca/submit/#commands.ca.submit.SubmitCA.run","title":"<code>run(name, config_path, ca_mlcube, client_mlcube, server_mlcube)</code>  <code>classmethod</code>","text":"<p>Submits a new ca to the medperf platform</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>ca name</p> required <code>config_path</code> <code>dict</code> <p>ca config</p> required <code>ca_mlcube</code> <code>int</code> <p>ca_mlcube mlcube uid</p> required <code>client_mlcube</code> <code>int</code> <p>client_mlcube mlcube uid</p> required <code>server_mlcube</code> <code>int</code> <p>server_mlcube mlcube uid</p> required Source code in <code>cli/medperf/commands/ca/submit.py</code> <pre><code>@classmethod\ndef run(\ncls,\nname: str,\nconfig_path: str,\nca_mlcube: int,\nclient_mlcube: int,\nserver_mlcube: int,\n):\n\"\"\"Submits a new ca to the medperf platform\n    Args:\n        name (str): ca name\n        config_path (dict): ca config\n        ca_mlcube (int): ca_mlcube mlcube uid\n        client_mlcube (int): client_mlcube mlcube uid\n        server_mlcube (int): server_mlcube mlcube uid\n    \"\"\"\nui = config.ui\nsubmission = cls(name, config_path, ca_mlcube, client_mlcube, server_mlcube)\nwith ui.interactive():\nui.text = \"Submitting CA to MedPerf\"\nsubmission.validate_ca_cubes()\nupdated_benchmark_body = submission.submit()\nui.print(\"Uploaded\")\nsubmission.write(updated_benchmark_body)\n</code></pre>"},{"location":"reference/commands/certificate/certificate/","title":"Certificate","text":""},{"location":"reference/commands/certificate/certificate/#commands.certificate.certificate.get_client_certificate","title":"<code>get_client_certificate(training_exp_id=typer.Option(..., '--training_exp_id', '-t', help='UID of training exp which you intend to be a part of'), overwrite=typer.Option(False, '--overwrite', help='Overwrite cert and key if present'))</code>","text":"<p>get a client certificate</p> Source code in <code>cli/medperf/commands/certificate/certificate.py</code> <pre><code>@app.command(\"get_client_certificate\")\n@clean_except\ndef get_client_certificate(\ntraining_exp_id: int = typer.Option(\n...,\n\"--training_exp_id\",\n\"-t\",\nhelp=\"UID of training exp which you intend to be a part of\",\n),\noverwrite: bool = typer.Option(\nFalse, \"--overwrite\", help=\"Overwrite cert and key if present\"\n),\n):\n\"\"\"get a client certificate\"\"\"\nGetUserCertificate.run(training_exp_id, overwrite)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/certificate/certificate/#commands.certificate.certificate.get_server_certificate","title":"<code>get_server_certificate(training_exp_id=typer.Option(..., '--training_exp_id', '-t', help='UID of training exp which you intend to be a part of'), overwrite=typer.Option(False, '--overwrite', help='Overwrite cert and key if present'))</code>","text":"<p>get a server certificate</p> Source code in <code>cli/medperf/commands/certificate/certificate.py</code> <pre><code>@app.command(\"get_server_certificate\")\n@clean_except\ndef get_server_certificate(\ntraining_exp_id: int = typer.Option(\n...,\n\"--training_exp_id\",\n\"-t\",\nhelp=\"UID of training exp which you intend to be a part of\",\n),\noverwrite: bool = typer.Option(\nFalse, \"--overwrite\", help=\"Overwrite cert and key if present\"\n),\n):\n\"\"\"get a server certificate\"\"\"\nGetServerCertificate.run(training_exp_id, overwrite)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/certificate/client_certificate/","title":"Client certificate","text":""},{"location":"reference/commands/certificate/client_certificate/#commands.certificate.client_certificate.GetUserCertificate","title":"<code>GetUserCertificate</code>","text":"Source code in <code>cli/medperf/commands/certificate/client_certificate.py</code> <pre><code>class GetUserCertificate:\n@staticmethod\ndef run(training_exp_id: int, overwrite: bool = False):\n\"\"\"get user cert\"\"\"\nca = CA.from_experiment(training_exp_id)\nemail = get_medperf_user_data()[\"email\"]\noutput_path = get_pki_assets_path(email, ca.name)\nif os.path.exists(output_path):\nif not overwrite:\nraise MedperfException(\n\"Cert and key already present. Rerun the command with --overwrite\"\n)\nremove_path(output_path)\nget_client_cert(ca, email, output_path)\n</code></pre>"},{"location":"reference/commands/certificate/client_certificate/#commands.certificate.client_certificate.GetUserCertificate.run","title":"<code>run(training_exp_id, overwrite=False)</code>  <code>staticmethod</code>","text":"<p>get user cert</p> Source code in <code>cli/medperf/commands/certificate/client_certificate.py</code> <pre><code>@staticmethod\ndef run(training_exp_id: int, overwrite: bool = False):\n\"\"\"get user cert\"\"\"\nca = CA.from_experiment(training_exp_id)\nemail = get_medperf_user_data()[\"email\"]\noutput_path = get_pki_assets_path(email, ca.name)\nif os.path.exists(output_path):\nif not overwrite:\nraise MedperfException(\n\"Cert and key already present. Rerun the command with --overwrite\"\n)\nremove_path(output_path)\nget_client_cert(ca, email, output_path)\n</code></pre>"},{"location":"reference/commands/certificate/server_certificate/","title":"Server certificate","text":""},{"location":"reference/commands/certificate/server_certificate/#commands.certificate.server_certificate.GetServerCertificate","title":"<code>GetServerCertificate</code>","text":"Source code in <code>cli/medperf/commands/certificate/server_certificate.py</code> <pre><code>class GetServerCertificate:\n@staticmethod\ndef run(training_exp_id: int, overwrite: bool = False):\n\"\"\"get server cert\"\"\"\nca = CA.from_experiment(training_exp_id)\naggregator = Aggregator.from_experiment(training_exp_id)\naddress = aggregator.address\noutput_path = get_pki_assets_path(address, ca.name)\nif os.path.exists(output_path):\nif not overwrite:\nraise MedperfException(\n\"Cert and key already present. Rerun the command with --overwrite\"\n)\nremove_path(output_path)\nget_server_cert(ca, address, output_path)\n</code></pre>"},{"location":"reference/commands/certificate/server_certificate/#commands.certificate.server_certificate.GetServerCertificate.run","title":"<code>run(training_exp_id, overwrite=False)</code>  <code>staticmethod</code>","text":"<p>get server cert</p> Source code in <code>cli/medperf/commands/certificate/server_certificate.py</code> <pre><code>@staticmethod\ndef run(training_exp_id: int, overwrite: bool = False):\n\"\"\"get server cert\"\"\"\nca = CA.from_experiment(training_exp_id)\naggregator = Aggregator.from_experiment(training_exp_id)\naddress = aggregator.address\noutput_path = get_pki_assets_path(address, ca.name)\nif os.path.exists(output_path):\nif not overwrite:\nraise MedperfException(\n\"Cert and key already present. Rerun the command with --overwrite\"\n)\nremove_path(output_path)\nget_server_cert(ca, address, output_path)\n</code></pre>"},{"location":"reference/commands/compatibility_test/compatibility_test/","title":"Compatibility test","text":""},{"location":"reference/commands/compatibility_test/compatibility_test/#commands.compatibility_test.compatibility_test.list","title":"<code>list()</code>","text":"<p>List previously executed tests reports.</p> Source code in <code>cli/medperf/commands/compatibility_test/compatibility_test.py</code> <pre><code>@app.command(\"ls\")\n@clean_except\ndef list():\n\"\"\"List previously executed tests reports.\"\"\"\nEntityList.run(\nTestReport,\nfields=[\"UID\", \"Data Source\", \"Model\", \"Evaluator\"],\nunregistered=True,\n)\n</code></pre>"},{"location":"reference/commands/compatibility_test/compatibility_test/#commands.compatibility_test.compatibility_test.run","title":"<code>run(benchmark_uid=typer.Option(None, '--benchmark', '-b', help='UID of the benchmark to test. Optional'), data_uid=typer.Option(None, '--data_uid', '-d', help='Prepared Dataset UID. Used for dataset testing. Optional. Defaults to benchmark demo dataset.'), demo_dataset_url=typer.Option(None, '--demo_dataset_url', help='Identifier to download the demonstration dataset tarball file.\\n\\n            See `medperf container submit --help` for more information'), demo_dataset_hash=typer.Option(None, '--demo_dataset_hash', help='Hash of the demo dataset, if provided.'), data_path=typer.Option(None, '--data_path', help='Path to raw input data.'), labels_path=typer.Option(None, '--labels_path', help='Path to the labels of the raw input data, if provided.'), data_prep=typer.Option(None, '--data_preparator', '-p', help='UID or local path to the data preparation container config file. Optional. Defaults to benchmark data preparator.'), model=typer.Option(None, '--model', '-m', help='UID or local path to the model container config file. Optional. Defaults to benchmark reference model.'), evaluator=typer.Option(None, '--evaluator', '-e', help='UID or local path to the evaluator container config file. Optional. Defaults to benchmark evaluator.'), no_cache=typer.Option(False, '--no-cache', help='Execute the test even if results already exist'), offline=typer.Option(False, '--offline', help='Execute the test without connecting to the MedPerf server.'), skip_data_preparation_step=typer.Option(False, '--skip-demo-data-preparation', help='Use this flag if the passed demo dataset or data path is already prepared'), use_local_model_image=typer.Option(False, '--use-local-model-image', help=\"Use this flag if you don't want MedPerf to pull the model container image before running\"))</code>","text":"<p>Executes a compatibility test.</p> Source code in <code>cli/medperf/commands/compatibility_test/compatibility_test.py</code> <pre><code>@app.command(\"run\")\n@clean_except\ndef run(\nbenchmark_uid: int = typer.Option(\nNone, \"--benchmark\", \"-b\", help=\"UID of the benchmark to test. Optional\"\n),\ndata_uid: str = typer.Option(\nNone,\n\"--data_uid\",\n\"-d\",\nhelp=\"Prepared Dataset UID. Used for dataset testing. Optional. Defaults to benchmark demo dataset.\",\n),\ndemo_dataset_url: str = typer.Option(\nNone,\n\"--demo_dataset_url\",\nhelp=\"\"\"Identifier to download the demonstration dataset tarball file.\\n\n            See `medperf container submit --help` for more information\"\"\",\n),\ndemo_dataset_hash: str = typer.Option(\nNone, \"--demo_dataset_hash\", help=\"Hash of the demo dataset, if provided.\"\n),\ndata_path: str = typer.Option(None, \"--data_path\", help=\"Path to raw input data.\"),\nlabels_path: str = typer.Option(\nNone,\n\"--labels_path\",\nhelp=\"Path to the labels of the raw input data, if provided.\",\n),\ndata_prep: str = typer.Option(\nNone,\n\"--data_preparator\",\n\"-p\",\nhelp=(\n\"UID or local path to the data preparation container config file.\"\n\" Optional. Defaults to benchmark data preparator.\"\n),\n),\nmodel: str = typer.Option(\nNone,\n\"--model\",\n\"-m\",\nhelp=(\n\"UID or local path to the model container config file.\"\n\" Optional. Defaults to benchmark reference model.\"\n),\n),\nevaluator: str = typer.Option(\nNone,\n\"--evaluator\",\n\"-e\",\nhelp=(\n\"UID or local path to the evaluator container config file.\"\n\" Optional. Defaults to benchmark evaluator.\"\n),\n),\nno_cache: bool = typer.Option(\nFalse, \"--no-cache\", help=\"Execute the test even if results already exist\"\n),\noffline: bool = typer.Option(\nFalse,\n\"--offline\",\nhelp=\"Execute the test without connecting to the MedPerf server.\",\n),\nskip_data_preparation_step: bool = typer.Option(\nFalse,\n\"--skip-demo-data-preparation\",\nhelp=\"Use this flag if the passed demo dataset or data path is already prepared\",\n),\nuse_local_model_image: bool = typer.Option(\nFalse,\n\"--use-local-model-image\",\nhelp=\"Use this flag if you don't want MedPerf to pull the model container image before running\",\n),\n):\n\"\"\"\n    Executes a compatibility test.\n    \"\"\"\nCompatibilityTestExecution.run(\nbenchmark_uid,\ndata_prep,\nmodel,\nevaluator,\ndata_path,\nlabels_path,\ndemo_dataset_url,\ndemo_dataset_hash,\ndata_uid,\nno_cache=no_cache,\noffline=offline,\nskip_data_preparation_step=skip_data_preparation_step,\nuse_local_model_image=use_local_model_image,\n)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/compatibility_test/compatibility_test/#commands.compatibility_test.compatibility_test.view","title":"<code>view(entity_id=typer.Argument(None, help='Test report ID'), format=typer.Option('yaml', '-f', '--format', help='Format to display contents. Available formats: [yaml, json]'), output=typer.Option(None, '--output', '-o', help='Output file to store contents. If not provided, the output will be displayed'))</code>","text":"<p>Displays the information of one or more test reports</p> Source code in <code>cli/medperf/commands/compatibility_test/compatibility_test.py</code> <pre><code>@app.command(\"view\")\n@clean_except\ndef view(\nentity_id: Optional[str] = typer.Argument(None, help=\"Test report ID\"),\nformat: str = typer.Option(\n\"yaml\",\n\"-f\",\n\"--format\",\nhelp=\"Format to display contents. Available formats: [yaml, json]\",\n),\noutput: str = typer.Option(\nNone,\n\"--output\",\n\"-o\",\nhelp=\"Output file to store contents. If not provided, the output will be displayed\",\n),\n):\n\"\"\"Displays the information of one or more test reports\"\"\"\nEntityView.run(entity_id, TestReport, format, unregistered=True, output=output)\n</code></pre>"},{"location":"reference/commands/compatibility_test/run/","title":"Run","text":""},{"location":"reference/commands/compatibility_test/run/#commands.compatibility_test.run.CompatibilityTestExecution","title":"<code>CompatibilityTestExecution</code>","text":"Source code in <code>cli/medperf/commands/compatibility_test/run.py</code> <pre><code>class CompatibilityTestExecution:\n@classmethod\ndef run(\ncls,\nbenchmark: int = None,\ndata_prep: str = None,\nmodel: str = None,\nevaluator: str = None,\ndata_path: str = None,\nlabels_path: str = None,\ndemo_dataset_url: str = None,\ndemo_dataset_hash: str = None,\ndata_uid: str = None,\nno_cache: bool = False,\noffline: bool = False,\nskip_data_preparation_step: bool = False,\nuse_local_model_image: bool = False,\n) -&gt; (str, dict):\n\"\"\"Execute a test workflow. Components of a complete workflow should be passed.\n        When only the benchmark is provided, it implies the following workflow will be used:\n        - the benchmark's demo dataset is used as the raw data\n        - the benchmark's data preparation cube is used\n        - the benchmark's reference model cube is used\n        - the benchmark's metrics cube is used\n        Overriding benchmark's components:\n        - The data prepration, model, and metrics cubes can be overriden by specifying a cube either\n        as an integer (registered) or a path (local). The path can refer either to the mlcube config\n        file or to the mlcube directory containing the mlcube config file.\n        - Instead of using the demo dataset of the benchmark, The input raw data can be overriden by providing:\n            - a demo dataset url and its hash\n            - data path and labels path\n        - A prepared dataset can be directly used. In this case the data preparator cube is never used.\n        The prepared data can be provided by either specifying an integer (registered) or a hash of a\n        locally prepared dataset.\n        Whether the benchmark is provided or not, the command will fail either if the user fails to\n        provide a valid complete workflow, or if the user provided extra redundant parameters.\n        Args:\n            benchmark (int, optional): Benchmark to run the test workflow for\n            data_prep (str, optional): data preparation mlcube uid or local path.\n            model (str, optional): model mlcube uid or local path.\n            evaluator (str, optional): evaluator mlcube uid or local path.\n            data_path (str, optional): path to a local raw data\n            labels_path (str, optional): path to the labels of the local raw data\n            demo_dataset_url (str, optional): Identifier to download the demonstration dataset tarball file.\\n\n            See `medperf mlcube submit --help` for more information\n            demo_dataset_hash (str, optional): The hash of the demo dataset tarball file\n            data_uid (str, optional): A prepared dataset UID\n            no_cache (bool): Whether to ignore cached results of the test execution. Defaults to False.\n            offline (bool): Whether to disable communication to the MedPerf server and rely only on\n            local copies of the server assets. Defaults to False.\n        Returns:\n            (str): Prepared Dataset UID used for the test. Could be the one provided or a generated one.\n            (dict): Results generated by the test.\n        \"\"\"\nlogging.info(\"Starting test execution\")\ntest_exec = cls(\nbenchmark,\ndata_prep,\nmodel,\nevaluator,\ndata_path,\nlabels_path,\ndemo_dataset_url,\ndemo_dataset_hash,\ndata_uid,\nno_cache,\noffline,\nskip_data_preparation_step,\nuse_local_model_image,\n)\ntest_exec.validate()\ntest_exec.set_data_source()\ntest_exec.process_benchmark()\nwith config.ui.interactive():\ntest_exec.prepare_cubes()\ntest_exec.prepare_dataset()\ntest_exec.initialize_report()\nresults = test_exec.cached_results()\nif results is None:\nresults = test_exec.execute()\ntest_exec.write(results)\nelse:\nlogging.info(\"Existing results are found. Test would not be re-executed.\")\nlogging.debug(f\"Existing results: {results}\")\nreturn test_exec.data_uid, results\ndef __init__(\nself,\nbenchmark: int = None,\ndata_prep: str = None,\nmodel: str = None,\nevaluator: str = None,\ndata_path: str = None,\nlabels_path: str = None,\ndemo_dataset_url: str = None,\ndemo_dataset_hash: str = None,\ndata_uid: str = None,\nno_cache: bool = False,\noffline: bool = False,\nskip_data_preparation_step: bool = False,\nuse_local_model_image: bool = False,\n):\nself.benchmark_uid = benchmark\nself.data_prep = data_prep\nself.model = model\nself.evaluator = evaluator\nself.data_path = data_path\nself.labels_path = labels_path\nself.demo_dataset_url = demo_dataset_url\nself.demo_dataset_hash = demo_dataset_hash\nself.data_uid = data_uid\nself.no_cache = no_cache\nself.offline = offline\nself.skip_data_preparation_step = skip_data_preparation_step\nself.use_local_model_image = use_local_model_image\n# This property will be set to either \"path\", \"demo\", \"prepared\", or \"benchmark\"\nself.data_source = None\nself.dataset = None\nself.model_cube = None\nself.evaluator_cube = None\nself.validator = CompatibilityTestParamsValidator(\nself.benchmark_uid,\nself.data_prep,\nself.model,\nself.evaluator,\nself.data_path,\nself.labels_path,\nself.demo_dataset_url,\nself.demo_dataset_hash,\nself.data_uid,\n)\ndef validate(self):\nself.validator.validate()\ndef set_data_source(self):\nself.data_source = self.validator.get_data_source()\ndef process_benchmark(self):\n\"\"\"Process the benchmark input if given. Sets the needed parameters from\n        the benchmark.\"\"\"\nif not self.benchmark_uid:\nreturn\nbenchmark = Benchmark.get(self.benchmark_uid, local_only=self.offline)\nif self.data_source != \"prepared\":\nself.data_prep = self.data_prep or benchmark.data_preparation_mlcube\nself.model = self.model or benchmark.reference_model_mlcube\nself.evaluator = self.evaluator or benchmark.data_evaluator_mlcube\nif self.data_source == \"benchmark\":\nself.demo_dataset_url = benchmark.demo_dataset_tarball_url\nself.demo_dataset_hash = benchmark.demo_dataset_tarball_hash\nself.skip_data_preparation_step = benchmark.metadata.get(\n\"demo_dataset_already_prepared\", False\n)\ndef prepare_cubes(self):\n\"\"\"Prepares the mlcubes. If the provided mlcube is a path, it will create\n        a temporary uid and link the cube path to the medperf storage path.\"\"\"\nif self.data_source != \"prepared\":\nlogging.info(\nf\"Establishing the data preparation container: {self.data_prep}\"\n)\nself.data_prep = prepare_cube(self.data_prep)\nlogging.info(f\"Establishing the model container: {self.model}\")\nself.model = prepare_cube(self.model)\nlogging.info(f\"Establishing the evaluator container: {self.evaluator}\")\nself.evaluator = prepare_cube(self.evaluator)\nself.model_cube = get_cube(\nself.model,\n\"Model\",\nlocal_only=self.offline,\nuse_local_model_image=self.use_local_model_image,\n)\nself.evaluator_cube = get_cube(\nself.evaluator, \"Evaluator\", local_only=self.offline\n)\ndef prepare_dataset(self):\n\"\"\"Assigns the data_uid used for testing and retrieves the dataset.\n        If the data is not prepared, it calls the data preparation step\n        on the given local data path or using a remote demo dataset.\"\"\"\nlogging.info(\"Establishing data_uid for test execution\")\nif self.data_source != \"prepared\":\nif self.data_source == \"path\":\ndata_path, labels_path = self.data_path, self.labels_path\n# TODO: this has to be redesigned. Compatibility tests command\n#       is starting to have a lot of input arguments. For now\n#       let's not support accepting a metadata path\nmetadata_path = None\nelse:\ndata_path, labels_path, metadata_path = download_demo_data(\nself.demo_dataset_url, self.demo_dataset_hash\n)\nself.data_uid = create_test_dataset(\ndata_path,\nlabels_path,\nmetadata_path,\nself.data_prep,\nself.skip_data_preparation_step,\n)\nself.dataset = Dataset.get(self.data_uid, local_only=self.offline)\ndef initialize_report(self):\n\"\"\"Initializes an instance of `TestReport` to hold the current test information.\"\"\"\nreport_data = {\n\"demo_dataset_url\": self.demo_dataset_url,\n\"demo_dataset_hash\": self.demo_dataset_hash,\n\"data_path\": self.data_path,\n\"labels_path\": self.labels_path,\n\"prepared_data_hash\": self.data_uid,\n\"data_preparation_mlcube\": self.data_prep,\n\"model\": self.model,\n\"data_evaluator_mlcube\": self.evaluator,\n}\nself.report = TestReport(**report_data)\ndef cached_results(self):\n\"\"\"checks the existance of, and retrieves if possible, the compatibility test\n        result. This method is called prior to the test execution.\n        Returns:\n            (dict|None): None if the results does not exist or if self.no_cache is True,\n            otherwise it returns the found results.\n        \"\"\"\nif self.no_cache:\nreturn\nuid = self.report.local_id\ntry:\nreport = TestReport.get(uid)\nexcept InvalidArgumentError:\nreturn\nlogging.info(f\"Existing report {uid} was detected.\")\nlogging.info(\"The compatibilty test will not be re-executed.\")\nreturn report.results\ndef execute(self):\n\"\"\"Runs the test execution flow and returns the results\n        Returns:\n            dict: returns the results of the test execution.\n        \"\"\"\nexecution_summary = ExecutionFlow.run(\ndataset=self.dataset,\nmodel=self.model_cube,\nevaluator=self.evaluator_cube,\nignore_model_errors=False,\n)\nreturn execution_summary[\"results\"]\ndef write(self, results):\n\"\"\"Writes a report of the test execution to the disk\n        Args:\n            results (dict): the results of the test execution\n        \"\"\"\nself.report.set_results(results)\nself.report.write()\n</code></pre>"},{"location":"reference/commands/compatibility_test/run/#commands.compatibility_test.run.CompatibilityTestExecution.cached_results","title":"<code>cached_results()</code>","text":"<p>checks the existance of, and retrieves if possible, the compatibility test result. This method is called prior to the test execution.</p> <p>Returns:</p> Type Description <code>dict | None</code> <p>None if the results does not exist or if self.no_cache is True,</p> <p>otherwise it returns the found results.</p> Source code in <code>cli/medperf/commands/compatibility_test/run.py</code> <pre><code>def cached_results(self):\n\"\"\"checks the existance of, and retrieves if possible, the compatibility test\n    result. This method is called prior to the test execution.\n    Returns:\n        (dict|None): None if the results does not exist or if self.no_cache is True,\n        otherwise it returns the found results.\n    \"\"\"\nif self.no_cache:\nreturn\nuid = self.report.local_id\ntry:\nreport = TestReport.get(uid)\nexcept InvalidArgumentError:\nreturn\nlogging.info(f\"Existing report {uid} was detected.\")\nlogging.info(\"The compatibilty test will not be re-executed.\")\nreturn report.results\n</code></pre>"},{"location":"reference/commands/compatibility_test/run/#commands.compatibility_test.run.CompatibilityTestExecution.execute","title":"<code>execute()</code>","text":"<p>Runs the test execution flow and returns the results</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>returns the results of the test execution.</p> Source code in <code>cli/medperf/commands/compatibility_test/run.py</code> <pre><code>def execute(self):\n\"\"\"Runs the test execution flow and returns the results\n    Returns:\n        dict: returns the results of the test execution.\n    \"\"\"\nexecution_summary = ExecutionFlow.run(\ndataset=self.dataset,\nmodel=self.model_cube,\nevaluator=self.evaluator_cube,\nignore_model_errors=False,\n)\nreturn execution_summary[\"results\"]\n</code></pre>"},{"location":"reference/commands/compatibility_test/run/#commands.compatibility_test.run.CompatibilityTestExecution.initialize_report","title":"<code>initialize_report()</code>","text":"<p>Initializes an instance of <code>TestReport</code> to hold the current test information.</p> Source code in <code>cli/medperf/commands/compatibility_test/run.py</code> <pre><code>def initialize_report(self):\n\"\"\"Initializes an instance of `TestReport` to hold the current test information.\"\"\"\nreport_data = {\n\"demo_dataset_url\": self.demo_dataset_url,\n\"demo_dataset_hash\": self.demo_dataset_hash,\n\"data_path\": self.data_path,\n\"labels_path\": self.labels_path,\n\"prepared_data_hash\": self.data_uid,\n\"data_preparation_mlcube\": self.data_prep,\n\"model\": self.model,\n\"data_evaluator_mlcube\": self.evaluator,\n}\nself.report = TestReport(**report_data)\n</code></pre>"},{"location":"reference/commands/compatibility_test/run/#commands.compatibility_test.run.CompatibilityTestExecution.prepare_cubes","title":"<code>prepare_cubes()</code>","text":"<p>Prepares the mlcubes. If the provided mlcube is a path, it will create a temporary uid and link the cube path to the medperf storage path.</p> Source code in <code>cli/medperf/commands/compatibility_test/run.py</code> <pre><code>def prepare_cubes(self):\n\"\"\"Prepares the mlcubes. If the provided mlcube is a path, it will create\n    a temporary uid and link the cube path to the medperf storage path.\"\"\"\nif self.data_source != \"prepared\":\nlogging.info(\nf\"Establishing the data preparation container: {self.data_prep}\"\n)\nself.data_prep = prepare_cube(self.data_prep)\nlogging.info(f\"Establishing the model container: {self.model}\")\nself.model = prepare_cube(self.model)\nlogging.info(f\"Establishing the evaluator container: {self.evaluator}\")\nself.evaluator = prepare_cube(self.evaluator)\nself.model_cube = get_cube(\nself.model,\n\"Model\",\nlocal_only=self.offline,\nuse_local_model_image=self.use_local_model_image,\n)\nself.evaluator_cube = get_cube(\nself.evaluator, \"Evaluator\", local_only=self.offline\n)\n</code></pre>"},{"location":"reference/commands/compatibility_test/run/#commands.compatibility_test.run.CompatibilityTestExecution.prepare_dataset","title":"<code>prepare_dataset()</code>","text":"<p>Assigns the data_uid used for testing and retrieves the dataset. If the data is not prepared, it calls the data preparation step on the given local data path or using a remote demo dataset.</p> Source code in <code>cli/medperf/commands/compatibility_test/run.py</code> <pre><code>def prepare_dataset(self):\n\"\"\"Assigns the data_uid used for testing and retrieves the dataset.\n    If the data is not prepared, it calls the data preparation step\n    on the given local data path or using a remote demo dataset.\"\"\"\nlogging.info(\"Establishing data_uid for test execution\")\nif self.data_source != \"prepared\":\nif self.data_source == \"path\":\ndata_path, labels_path = self.data_path, self.labels_path\n# TODO: this has to be redesigned. Compatibility tests command\n#       is starting to have a lot of input arguments. For now\n#       let's not support accepting a metadata path\nmetadata_path = None\nelse:\ndata_path, labels_path, metadata_path = download_demo_data(\nself.demo_dataset_url, self.demo_dataset_hash\n)\nself.data_uid = create_test_dataset(\ndata_path,\nlabels_path,\nmetadata_path,\nself.data_prep,\nself.skip_data_preparation_step,\n)\nself.dataset = Dataset.get(self.data_uid, local_only=self.offline)\n</code></pre>"},{"location":"reference/commands/compatibility_test/run/#commands.compatibility_test.run.CompatibilityTestExecution.process_benchmark","title":"<code>process_benchmark()</code>","text":"<p>Process the benchmark input if given. Sets the needed parameters from the benchmark.</p> Source code in <code>cli/medperf/commands/compatibility_test/run.py</code> <pre><code>def process_benchmark(self):\n\"\"\"Process the benchmark input if given. Sets the needed parameters from\n    the benchmark.\"\"\"\nif not self.benchmark_uid:\nreturn\nbenchmark = Benchmark.get(self.benchmark_uid, local_only=self.offline)\nif self.data_source != \"prepared\":\nself.data_prep = self.data_prep or benchmark.data_preparation_mlcube\nself.model = self.model or benchmark.reference_model_mlcube\nself.evaluator = self.evaluator or benchmark.data_evaluator_mlcube\nif self.data_source == \"benchmark\":\nself.demo_dataset_url = benchmark.demo_dataset_tarball_url\nself.demo_dataset_hash = benchmark.demo_dataset_tarball_hash\nself.skip_data_preparation_step = benchmark.metadata.get(\n\"demo_dataset_already_prepared\", False\n)\n</code></pre>"},{"location":"reference/commands/compatibility_test/run/#commands.compatibility_test.run.CompatibilityTestExecution.run","title":"<code>run(benchmark=None, data_prep=None, model=None, evaluator=None, data_path=None, labels_path=None, demo_dataset_url=None, demo_dataset_hash=None, data_uid=None, no_cache=False, offline=False, skip_data_preparation_step=False, use_local_model_image=False)</code>  <code>classmethod</code>","text":"<p>Execute a test workflow. Components of a complete workflow should be passed. When only the benchmark is provided, it implies the following workflow will be used: - the benchmark's demo dataset is used as the raw data - the benchmark's data preparation cube is used - the benchmark's reference model cube is used - the benchmark's metrics cube is used</p> <p>Overriding benchmark's components: - The data prepration, model, and metrics cubes can be overriden by specifying a cube either as an integer (registered) or a path (local). The path can refer either to the mlcube config file or to the mlcube directory containing the mlcube config file. - Instead of using the demo dataset of the benchmark, The input raw data can be overriden by providing:     - a demo dataset url and its hash     - data path and labels path - A prepared dataset can be directly used. In this case the data preparator cube is never used. The prepared data can be provided by either specifying an integer (registered) or a hash of a locally prepared dataset.</p> <p>Whether the benchmark is provided or not, the command will fail either if the user fails to provide a valid complete workflow, or if the user provided extra redundant parameters.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark</code> <code>int</code> <p>Benchmark to run the test workflow for</p> <code>None</code> <code>data_prep</code> <code>str</code> <p>data preparation mlcube uid or local path.</p> <code>None</code> <code>model</code> <code>str</code> <p>model mlcube uid or local path.</p> <code>None</code> <code>evaluator</code> <code>str</code> <p>evaluator mlcube uid or local path.</p> <code>None</code> <code>data_path</code> <code>str</code> <p>path to a local raw data</p> <code>None</code> <code>labels_path</code> <code>str</code> <p>path to the labels of the local raw data</p> <code>None</code> <code>demo_dataset_url</code> <code>str</code> <p>Identifier to download the demonstration dataset tarball file.</p> <code>None</code> <code>demo_dataset_hash</code> <code>str</code> <p>The hash of the demo dataset tarball file</p> <code>None</code> <code>data_uid</code> <code>str</code> <p>A prepared dataset UID</p> <code>None</code> <code>no_cache</code> <code>bool</code> <p>Whether to ignore cached results of the test execution. Defaults to False.</p> <code>False</code> <code>offline</code> <code>bool</code> <p>Whether to disable communication to the MedPerf server and rely only on</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>Prepared Dataset UID used for the test. Could be the one provided or a generated one.</p> <code>dict</code> <p>Results generated by the test.</p> Source code in <code>cli/medperf/commands/compatibility_test/run.py</code> <pre><code>@classmethod\ndef run(\ncls,\nbenchmark: int = None,\ndata_prep: str = None,\nmodel: str = None,\nevaluator: str = None,\ndata_path: str = None,\nlabels_path: str = None,\ndemo_dataset_url: str = None,\ndemo_dataset_hash: str = None,\ndata_uid: str = None,\nno_cache: bool = False,\noffline: bool = False,\nskip_data_preparation_step: bool = False,\nuse_local_model_image: bool = False,\n) -&gt; (str, dict):\n\"\"\"Execute a test workflow. Components of a complete workflow should be passed.\n    When only the benchmark is provided, it implies the following workflow will be used:\n    - the benchmark's demo dataset is used as the raw data\n    - the benchmark's data preparation cube is used\n    - the benchmark's reference model cube is used\n    - the benchmark's metrics cube is used\n    Overriding benchmark's components:\n    - The data prepration, model, and metrics cubes can be overriden by specifying a cube either\n    as an integer (registered) or a path (local). The path can refer either to the mlcube config\n    file or to the mlcube directory containing the mlcube config file.\n    - Instead of using the demo dataset of the benchmark, The input raw data can be overriden by providing:\n        - a demo dataset url and its hash\n        - data path and labels path\n    - A prepared dataset can be directly used. In this case the data preparator cube is never used.\n    The prepared data can be provided by either specifying an integer (registered) or a hash of a\n    locally prepared dataset.\n    Whether the benchmark is provided or not, the command will fail either if the user fails to\n    provide a valid complete workflow, or if the user provided extra redundant parameters.\n    Args:\n        benchmark (int, optional): Benchmark to run the test workflow for\n        data_prep (str, optional): data preparation mlcube uid or local path.\n        model (str, optional): model mlcube uid or local path.\n        evaluator (str, optional): evaluator mlcube uid or local path.\n        data_path (str, optional): path to a local raw data\n        labels_path (str, optional): path to the labels of the local raw data\n        demo_dataset_url (str, optional): Identifier to download the demonstration dataset tarball file.\\n\n        See `medperf mlcube submit --help` for more information\n        demo_dataset_hash (str, optional): The hash of the demo dataset tarball file\n        data_uid (str, optional): A prepared dataset UID\n        no_cache (bool): Whether to ignore cached results of the test execution. Defaults to False.\n        offline (bool): Whether to disable communication to the MedPerf server and rely only on\n        local copies of the server assets. Defaults to False.\n    Returns:\n        (str): Prepared Dataset UID used for the test. Could be the one provided or a generated one.\n        (dict): Results generated by the test.\n    \"\"\"\nlogging.info(\"Starting test execution\")\ntest_exec = cls(\nbenchmark,\ndata_prep,\nmodel,\nevaluator,\ndata_path,\nlabels_path,\ndemo_dataset_url,\ndemo_dataset_hash,\ndata_uid,\nno_cache,\noffline,\nskip_data_preparation_step,\nuse_local_model_image,\n)\ntest_exec.validate()\ntest_exec.set_data_source()\ntest_exec.process_benchmark()\nwith config.ui.interactive():\ntest_exec.prepare_cubes()\ntest_exec.prepare_dataset()\ntest_exec.initialize_report()\nresults = test_exec.cached_results()\nif results is None:\nresults = test_exec.execute()\ntest_exec.write(results)\nelse:\nlogging.info(\"Existing results are found. Test would not be re-executed.\")\nlogging.debug(f\"Existing results: {results}\")\nreturn test_exec.data_uid, results\n</code></pre>"},{"location":"reference/commands/compatibility_test/run/#commands.compatibility_test.run.CompatibilityTestExecution.write","title":"<code>write(results)</code>","text":"<p>Writes a report of the test execution to the disk</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict</code> <p>the results of the test execution</p> required Source code in <code>cli/medperf/commands/compatibility_test/run.py</code> <pre><code>def write(self, results):\n\"\"\"Writes a report of the test execution to the disk\n    Args:\n        results (dict): the results of the test execution\n    \"\"\"\nself.report.set_results(results)\nself.report.write()\n</code></pre>"},{"location":"reference/commands/compatibility_test/utils/","title":"Utils","text":""},{"location":"reference/commands/compatibility_test/utils/#commands.compatibility_test.utils.download_demo_data","title":"<code>download_demo_data(dset_url, dset_hash)</code>","text":"<p>Retrieves the demo dataset associated to the specified benchmark</p> <p>Returns:</p> Name Type Description <code>data_path</code> <code>str</code> <p>Location of the downloaded data</p> <code>labels_path</code> <code>str</code> <p>Location of the downloaded labels</p> Source code in <code>cli/medperf/commands/compatibility_test/utils.py</code> <pre><code>def download_demo_data(dset_url, dset_hash):\n\"\"\"Retrieves the demo dataset associated to the specified benchmark\n    Returns:\n        data_path (str): Location of the downloaded data\n        labels_path (str): Location of the downloaded labels\n    \"\"\"\ntry:\ndemo_dset_path, _ = resources.get_benchmark_demo_dataset(dset_url, dset_hash)\nexcept InvalidEntityError as e:\nraise InvalidEntityError(f\"Demo dataset {dset_url}: {e}\")\n# It is assumed that all demo datasets contain a file\n# which specifies the input of the data preparation step\npaths_file = os.path.join(demo_dset_path, config.demo_dset_paths_file)\nwith open(paths_file, \"r\") as f:\npaths = yaml.safe_load(f)\ndata_path = os.path.join(demo_dset_path, paths[\"data_path\"])\nlabels_path = os.path.join(demo_dset_path, paths[\"labels_path\"])\nmetadata_path = None\nif \"metadata_path\" in paths:\nmetadata_path = os.path.join(demo_dset_path, paths[\"metadata_path\"])\nreturn data_path, labels_path, metadata_path\n</code></pre>"},{"location":"reference/commands/compatibility_test/utils/#commands.compatibility_test.utils.prepare_cube","title":"<code>prepare_cube(cube_uid)</code>","text":"<p>Assigns the attr used for testing according to the initialization parameters. If the value is a path, it will create a temporary uid and link the cube path to the medperf storage path.</p> <p>Parameters:</p> Name Type Description Default <code>attr</code> <code>str</code> <p>Attribute to check and/or reassign.</p> required <code>fallback</code> <code>any</code> <p>Value to assign if attribute is empty. Defaults to None.</p> required Source code in <code>cli/medperf/commands/compatibility_test/utils.py</code> <pre><code>def prepare_cube(cube_uid: str):\n\"\"\"Assigns the attr used for testing according to the initialization parameters.\n    If the value is a path, it will create a temporary uid and link the cube path to\n    the medperf storage path.\n    Arguments:\n        attr (str): Attribute to check and/or reassign.\n        fallback (any): Value to assign if attribute is empty. Defaults to None.\n    \"\"\"\n# Test if value looks like an mlcube_uid, if so skip path validation\nif str(cube_uid).isdigit():\nlogging.info(f\"Container identifier {cube_uid} resembles a server ID\")\nreturn cube_uid\n# Check if value is a local mlcube\npath = sanitize_path(cube_uid)\nif os.path.isfile(path):\nlogging.info(\"local path provided. Creating symbolic link\")\ntemp_uid = prepare_local_cube(path)\nreturn temp_uid\nlogging.error(f\"container {cube_uid} was not found\")\nraise InvalidArgumentError(\nf\"The provided container ({cube_uid}) could not be found as a local or remote container\"\n)\n</code></pre>"},{"location":"reference/commands/compatibility_test/validate_params/","title":"Validate params","text":""},{"location":"reference/commands/compatibility_test/validate_params/#commands.compatibility_test.validate_params.CompatibilityTestParamsValidator","title":"<code>CompatibilityTestParamsValidator</code>","text":"<p>Validates the input parameters to the CompatibilityTestExecution class</p> Source code in <code>cli/medperf/commands/compatibility_test/validate_params.py</code> <pre><code>class CompatibilityTestParamsValidator:\n\"\"\"Validates the input parameters to the CompatibilityTestExecution class\"\"\"\ndef __init__(\nself,\nbenchmark: int = None,\ndata_prep: str = None,\nmodel: str = None,\nevaluator: str = None,\ndata_path: str = None,\nlabels_path: str = None,\ndemo_dataset_url: str = None,\ndemo_dataset_hash: str = None,\ndata_uid: str = None,\n):\nself.benchmark_uid = benchmark\nself.data_prep = data_prep\nself.model = model\nself.evaluator = evaluator\nself.data_path = data_path\nself.labels_path = labels_path\nself.demo_dataset_url = demo_dataset_url\nself.demo_dataset_hash = demo_dataset_hash\nself.data_uid = data_uid\ndef __validate_cubes(self):\nif not self.model and not self.benchmark_uid:\nraise InvalidArgumentError(\n\"A model or a benchmark should at least be specified\"\n)\nif not self.evaluator and not self.benchmark_uid:\nraise InvalidArgumentError(\n\"A metrics container or a benchmark should at least be specified\"\n)\ndef __raise_redundant_data_source(self):\nmsg = \"Make sure you pass only one data source: \"\nmsg += \"either a prepared dataset, a data path and labels path, or a demo dataset url\"\nraise InvalidArgumentError(msg)\ndef __validate_prepared_data_source(self):\nif any(\n[\nself.data_path,\nself.labels_path,\nself.demo_dataset_url,\nself.demo_dataset_hash,\n]\n):\nself.__raise_redundant_data_source()\nif self.data_prep:\nraise InvalidArgumentError(\n\"A data preparation container is not needed when specifying a prepared dataset\"\n)\ndef __validate_data_path_source(self):\nif not self.labels_path:\nraise InvalidArgumentError(\n\"Labels path should be specified when providing data path\"\n)\nif any([self.demo_dataset_url, self.demo_dataset_hash, self.data_uid]):\nself.__raise_redundant_data_source()\nif not self.data_prep and not self.benchmark_uid:\nraise InvalidArgumentError(\n\"A data preparation container should be passed when specifying raw data input\"\n)\ndef __validate_demo_data_source(self):\nif not self.demo_dataset_hash:\nraise InvalidArgumentError(\n\"The hash of the provided demo dataset should be specified\"\n)\nif any([self.data_path, self.labels_path, self.data_uid]):\nself.__raise_redundant_data_source()\nif not self.data_prep and not self.benchmark_uid:\nraise InvalidArgumentError(\n\"A data preparation container should be passed when specifying raw data input\"\n)\ndef __validate_data_source(self):\nif self.data_uid:\nself.__validate_prepared_data_source()\nreturn\nif self.data_path:\nself.__validate_data_path_source()\nreturn\nif self.demo_dataset_url:\nself.__validate_demo_data_source()\nreturn\nif self.benchmark_uid:\nreturn\nmsg = \"A data source should at least be specified, either by providing\"\nmsg += \" a prepared data uid, a demo dataset url, data path, or a benchmark\"\nraise InvalidArgumentError(msg)\ndef __validate_redundant_benchmark(self):\nif not self.benchmark_uid:\nreturn\nredundant_bmk_demo = any([self.data_uid, self.data_path, self.demo_dataset_url])\nredundant_bmk_model = self.model is not None\nredundant_bmk_evaluator = self.evaluator is not None\nredundant_bmk_preparator = (\nself.data_prep is not None or self.data_uid is not None\n)\nif all(\n[\nredundant_bmk_demo,\nredundant_bmk_model,\nredundant_bmk_evaluator,\nredundant_bmk_preparator,\n]\n):\nraise InvalidArgumentError(\"The provided benchmark will not be used\")\ndef validate(self):\n\"\"\"Ensures test has been passed a valid combination of parameters.\n        Raises `medperf.exceptions.InvalidArgumentError` when the parameters are\n        invalid.\n        \"\"\"\nself.__validate_cubes()\nself.__validate_data_source()\nself.__validate_redundant_benchmark()\ndef get_data_source(self):\n\"\"\"Parses the input parameters and returns a string, one of:\n        \"prepared\", if the source of data is a prepared dataset uid,\n        \"path\", if the source of data is a local path to raw data,\n        \"demo\", if the source of data is a demo dataset url,\n        or \"benchmark\", if the source of data is the demo dataset of a benchmark.\n        This function assumes the passed parameters to the constructor have been already\n        validated.\n        \"\"\"\nif self.data_uid:\nreturn \"prepared\"\nif self.data_path:\nreturn \"path\"\nif self.demo_dataset_url:\nreturn \"demo\"\nif self.benchmark_uid:\nreturn \"benchmark\"\nraise MedperfException(\n\"Ensure calling the `validate` method before using this method\"\n)\n</code></pre>"},{"location":"reference/commands/compatibility_test/validate_params/#commands.compatibility_test.validate_params.CompatibilityTestParamsValidator.get_data_source","title":"<code>get_data_source()</code>","text":"<p>Parses the input parameters and returns a string, one of: \"prepared\", if the source of data is a prepared dataset uid, \"path\", if the source of data is a local path to raw data, \"demo\", if the source of data is a demo dataset url, or \"benchmark\", if the source of data is the demo dataset of a benchmark.</p> <p>This function assumes the passed parameters to the constructor have been already validated.</p> Source code in <code>cli/medperf/commands/compatibility_test/validate_params.py</code> <pre><code>def get_data_source(self):\n\"\"\"Parses the input parameters and returns a string, one of:\n    \"prepared\", if the source of data is a prepared dataset uid,\n    \"path\", if the source of data is a local path to raw data,\n    \"demo\", if the source of data is a demo dataset url,\n    or \"benchmark\", if the source of data is the demo dataset of a benchmark.\n    This function assumes the passed parameters to the constructor have been already\n    validated.\n    \"\"\"\nif self.data_uid:\nreturn \"prepared\"\nif self.data_path:\nreturn \"path\"\nif self.demo_dataset_url:\nreturn \"demo\"\nif self.benchmark_uid:\nreturn \"benchmark\"\nraise MedperfException(\n\"Ensure calling the `validate` method before using this method\"\n)\n</code></pre>"},{"location":"reference/commands/compatibility_test/validate_params/#commands.compatibility_test.validate_params.CompatibilityTestParamsValidator.validate","title":"<code>validate()</code>","text":"<p>Ensures test has been passed a valid combination of parameters. Raises <code>medperf.exceptions.InvalidArgumentError</code> when the parameters are invalid.</p> Source code in <code>cli/medperf/commands/compatibility_test/validate_params.py</code> <pre><code>def validate(self):\n\"\"\"Ensures test has been passed a valid combination of parameters.\n    Raises `medperf.exceptions.InvalidArgumentError` when the parameters are\n    invalid.\n    \"\"\"\nself.__validate_cubes()\nself.__validate_data_source()\nself.__validate_redundant_benchmark()\n</code></pre>"},{"location":"reference/commands/dataset/associate/","title":"Associate","text":""},{"location":"reference/commands/dataset/associate/#commands.dataset.associate.AssociateDataset","title":"<code>AssociateDataset</code>","text":"Source code in <code>cli/medperf/commands/dataset/associate.py</code> <pre><code>class AssociateDataset:\n@staticmethod\ndef run(\ndata_uid: int,\nbenchmark_uid: int = None,\ntraining_exp_uid: int = None,\napproved=False,\nno_cache=False,\n):\n\"\"\"Associates a dataset with a benchmark or a training exp\"\"\"\ntoo_many_resources = benchmark_uid and training_exp_uid\nno_resource = benchmark_uid is None and training_exp_uid is None\nif no_resource or too_many_resources:\nraise InvalidArgumentError(\n\"Must provide either a benchmark or a training experiment\"\n)\nif benchmark_uid:\nAssociateBenchmarkDataset.run(\ndata_uid, benchmark_uid, approved=approved, no_cache=no_cache\n)\nif training_exp_uid:\nif no_cache:\nraise InvalidArgumentError(\n\"no_cache argument is only valid when associating with a benchmark\"\n)\nAssociateTrainingDataset.run(data_uid, training_exp_uid, approved=approved)\n</code></pre>"},{"location":"reference/commands/dataset/associate/#commands.dataset.associate.AssociateDataset.run","title":"<code>run(data_uid, benchmark_uid=None, training_exp_uid=None, approved=False, no_cache=False)</code>  <code>staticmethod</code>","text":"<p>Associates a dataset with a benchmark or a training exp</p> Source code in <code>cli/medperf/commands/dataset/associate.py</code> <pre><code>@staticmethod\ndef run(\ndata_uid: int,\nbenchmark_uid: int = None,\ntraining_exp_uid: int = None,\napproved=False,\nno_cache=False,\n):\n\"\"\"Associates a dataset with a benchmark or a training exp\"\"\"\ntoo_many_resources = benchmark_uid and training_exp_uid\nno_resource = benchmark_uid is None and training_exp_uid is None\nif no_resource or too_many_resources:\nraise InvalidArgumentError(\n\"Must provide either a benchmark or a training experiment\"\n)\nif benchmark_uid:\nAssociateBenchmarkDataset.run(\ndata_uid, benchmark_uid, approved=approved, no_cache=no_cache\n)\nif training_exp_uid:\nif no_cache:\nraise InvalidArgumentError(\n\"no_cache argument is only valid when associating with a benchmark\"\n)\nAssociateTrainingDataset.run(data_uid, training_exp_uid, approved=approved)\n</code></pre>"},{"location":"reference/commands/dataset/associate_benchmark/","title":"Associate benchmark","text":""},{"location":"reference/commands/dataset/associate_benchmark/#commands.dataset.associate_benchmark.AssociateBenchmarkDataset","title":"<code>AssociateBenchmarkDataset</code>","text":"Source code in <code>cli/medperf/commands/dataset/associate_benchmark.py</code> <pre><code>class AssociateBenchmarkDataset:\n@staticmethod\ndef run(data_uid: int, benchmark_uid: int, approved=False, no_cache=False):\n\"\"\"Associates a registered dataset with a benchmark\n        Args:\n            data_uid (int): UID of the registered dataset to associate\n            benchmark_uid (int): UID of the benchmark to associate with\n        \"\"\"\ncomms = config.comms\nui = config.ui\ndset = Dataset.get(data_uid)\nif dset.id is None:\nmsg = \"The provided dataset is not registered.\"\nraise InvalidArgumentError(msg)\nbenchmark = Benchmark.get(benchmark_uid)\nif dset.data_preparation_mlcube != benchmark.data_preparation_mlcube:\nraise InvalidArgumentError(\n\"The specified dataset wasn't prepared for this benchmark\"\n)\nexecution = BenchmarkExecution.run(\nbenchmark_uid,\ndata_uid,\n[benchmark.reference_model_mlcube],\nno_cache=no_cache,\n)[0]\nresults = execution.read_results()\nui.print(\"These are the results generated by the compatibility test. \")\nui.print(\"This will be sent along the association request.\")\nui.print(\"They will not be part of the benchmark.\")\ndict_pretty_print(results)\nmsg = \"Please confirm that you would like to associate\"\nmsg += f\" the dataset {dset.name} with the benchmark {benchmark.name}.\"\nmsg += \" [Y/n]\"\napproved = approved or approval_prompt(msg)\nif approved:\nui.print(\"Generating dataset benchmark association\")\nmetadata = {\"test_result\": results}\ncomms.associate_benchmark_dataset(dset.id, benchmark_uid, metadata)\nelse:\nraise CleanExit(\"Dataset association operation cancelled.\")\n</code></pre>"},{"location":"reference/commands/dataset/associate_benchmark/#commands.dataset.associate_benchmark.AssociateBenchmarkDataset.run","title":"<code>run(data_uid, benchmark_uid, approved=False, no_cache=False)</code>  <code>staticmethod</code>","text":"<p>Associates a registered dataset with a benchmark</p> <p>Parameters:</p> Name Type Description Default <code>data_uid</code> <code>int</code> <p>UID of the registered dataset to associate</p> required <code>benchmark_uid</code> <code>int</code> <p>UID of the benchmark to associate with</p> required Source code in <code>cli/medperf/commands/dataset/associate_benchmark.py</code> <pre><code>@staticmethod\ndef run(data_uid: int, benchmark_uid: int, approved=False, no_cache=False):\n\"\"\"Associates a registered dataset with a benchmark\n    Args:\n        data_uid (int): UID of the registered dataset to associate\n        benchmark_uid (int): UID of the benchmark to associate with\n    \"\"\"\ncomms = config.comms\nui = config.ui\ndset = Dataset.get(data_uid)\nif dset.id is None:\nmsg = \"The provided dataset is not registered.\"\nraise InvalidArgumentError(msg)\nbenchmark = Benchmark.get(benchmark_uid)\nif dset.data_preparation_mlcube != benchmark.data_preparation_mlcube:\nraise InvalidArgumentError(\n\"The specified dataset wasn't prepared for this benchmark\"\n)\nexecution = BenchmarkExecution.run(\nbenchmark_uid,\ndata_uid,\n[benchmark.reference_model_mlcube],\nno_cache=no_cache,\n)[0]\nresults = execution.read_results()\nui.print(\"These are the results generated by the compatibility test. \")\nui.print(\"This will be sent along the association request.\")\nui.print(\"They will not be part of the benchmark.\")\ndict_pretty_print(results)\nmsg = \"Please confirm that you would like to associate\"\nmsg += f\" the dataset {dset.name} with the benchmark {benchmark.name}.\"\nmsg += \" [Y/n]\"\napproved = approved or approval_prompt(msg)\nif approved:\nui.print(\"Generating dataset benchmark association\")\nmetadata = {\"test_result\": results}\ncomms.associate_benchmark_dataset(dset.id, benchmark_uid, metadata)\nelse:\nraise CleanExit(\"Dataset association operation cancelled.\")\n</code></pre>"},{"location":"reference/commands/dataset/associate_training/","title":"Associate training","text":""},{"location":"reference/commands/dataset/associate_training/#commands.dataset.associate_training.AssociateTrainingDataset","title":"<code>AssociateTrainingDataset</code>","text":"Source code in <code>cli/medperf/commands/dataset/associate_training.py</code> <pre><code>class AssociateTrainingDataset:\n@staticmethod\ndef run(data_uid: int, training_exp_uid: int, approved=False):\n\"\"\"Associates a dataset with a training experiment\n        Args:\n            data_uid (int): UID of the registered dataset to associate\n            training_exp_uid (int): UID of the training experiment to associate with\n        \"\"\"\ncomms = config.comms\nui = config.ui\ndset: Dataset = Dataset.get(data_uid)\nif dset.id is None:\nmsg = \"The provided dataset is not registered.\"\nraise InvalidArgumentError(msg)\ntraining_exp = TrainingExp.get(training_exp_uid)\nif dset.data_preparation_mlcube != training_exp.data_preparation_mlcube:\nraise InvalidArgumentError(\n\"The specified dataset wasn't prepared for this experiment\"\n)\nmsg = \"Please confirm that you would like to associate\"\nmsg += f\" the dataset {dset.name} with the training experiment {training_exp.name}.\"\nmsg += \" [Y/n]\"\napproved = approved or approval_prompt(msg)\nif approved:\nui.print(\"Generating dataset training experiment association\")\ncomms.associate_training_dataset(dset.id, training_exp_uid)\nelse:\nui.print(\"Dataset association operation cancelled.\")\n</code></pre>"},{"location":"reference/commands/dataset/associate_training/#commands.dataset.associate_training.AssociateTrainingDataset.run","title":"<code>run(data_uid, training_exp_uid, approved=False)</code>  <code>staticmethod</code>","text":"<p>Associates a dataset with a training experiment</p> <p>Parameters:</p> Name Type Description Default <code>data_uid</code> <code>int</code> <p>UID of the registered dataset to associate</p> required <code>training_exp_uid</code> <code>int</code> <p>UID of the training experiment to associate with</p> required Source code in <code>cli/medperf/commands/dataset/associate_training.py</code> <pre><code>@staticmethod\ndef run(data_uid: int, training_exp_uid: int, approved=False):\n\"\"\"Associates a dataset with a training experiment\n    Args:\n        data_uid (int): UID of the registered dataset to associate\n        training_exp_uid (int): UID of the training experiment to associate with\n    \"\"\"\ncomms = config.comms\nui = config.ui\ndset: Dataset = Dataset.get(data_uid)\nif dset.id is None:\nmsg = \"The provided dataset is not registered.\"\nraise InvalidArgumentError(msg)\ntraining_exp = TrainingExp.get(training_exp_uid)\nif dset.data_preparation_mlcube != training_exp.data_preparation_mlcube:\nraise InvalidArgumentError(\n\"The specified dataset wasn't prepared for this experiment\"\n)\nmsg = \"Please confirm that you would like to associate\"\nmsg += f\" the dataset {dset.name} with the training experiment {training_exp.name}.\"\nmsg += \" [Y/n]\"\napproved = approved or approval_prompt(msg)\nif approved:\nui.print(\"Generating dataset training experiment association\")\ncomms.associate_training_dataset(dset.id, training_exp_uid)\nelse:\nui.print(\"Dataset association operation cancelled.\")\n</code></pre>"},{"location":"reference/commands/dataset/dataset/","title":"Dataset","text":""},{"location":"reference/commands/dataset/dataset/#commands.dataset.dataset.associate","title":"<code>associate(data_uid=typer.Option(..., '--data_uid', '-d', help='Registered Dataset UID'), benchmark_uid=typer.Option(None, '--benchmark_uid', '-b', help='Benchmark UID'), training_exp_uid=typer.Option(None, '--training_exp_uid', '-t', help='Training experiment UID'), approval=typer.Option(False, '-y', help='Skip approval step'), no_cache=typer.Option(False, '--no-cache', help='Execute the benchmark association test even if results already exist'))</code>","text":"<p>Associate a registered dataset with a specific benchmark or experiment.</p> Source code in <code>cli/medperf/commands/dataset/dataset.py</code> <pre><code>@app.command(\"associate\")\n@clean_except\ndef associate(\ndata_uid: int = typer.Option(\n..., \"--data_uid\", \"-d\", help=\"Registered Dataset UID\"\n),\nbenchmark_uid: int = typer.Option(\nNone, \"--benchmark_uid\", \"-b\", help=\"Benchmark UID\"\n),\ntraining_exp_uid: int = typer.Option(\nNone, \"--training_exp_uid\", \"-t\", help=\"Training experiment UID\"\n),\napproval: bool = typer.Option(False, \"-y\", help=\"Skip approval step\"),\nno_cache: bool = typer.Option(\nFalse,\n\"--no-cache\",\nhelp=\"Execute the benchmark association test even if results already exist\",\n),\n):\n\"\"\"Associate a registered dataset with a specific benchmark or experiment.\"\"\"\nui = config.ui\nAssociateDataset.run(\ndata_uid, benchmark_uid, training_exp_uid, approved=approval, no_cache=no_cache\n)\nui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/dataset/dataset/#commands.dataset.dataset.export_dataset","title":"<code>export_dataset(data_uid=typer.Option(..., '--data_uid', '-d', help='Dataset UID to be exported'), output=typer.Option(..., '--output', '-o', help='Path of the folder that will contain the tar.gz dataset backup.'))</code>","text":"<p>Exports dataset files to a tar.gz file in the specified output folder.</p> Source code in <code>cli/medperf/commands/dataset/dataset.py</code> <pre><code>@app.command(\"export\")\n@clean_except\ndef export_dataset(\ndata_uid: str = typer.Option(\n..., \"--data_uid\", \"-d\", help=\"Dataset UID to be exported\"\n),\noutput: str = typer.Option(\n...,\n\"--output\",\n\"-o\",\nhelp=\"Path of the folder that will contain the tar.gz dataset backup.\",\n),\n):\n\"\"\"Exports dataset files to a tar.gz file in the specified output folder.\"\"\"\nExportDataset.run(data_uid, output)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/dataset/dataset/#commands.dataset.dataset.import_dataset","title":"<code>import_dataset(data_uid=typer.Option(..., '--data_uid', '-d', help='Dataset UID to be imported'), input_path=typer.Option(..., '--input', '-i', help='Path of the tar.gz file (dataset backup) to be imported.'), raw_path=typer.Option(None, '--raw_dataset_path', help=\"New path of the DEVELOPMENT dataset raw data to be saved. Directory should be empty or doesn't exist.\"))</code>","text":"<p>Imports dataset files from specified tar.gz file.</p> Source code in <code>cli/medperf/commands/dataset/dataset.py</code> <pre><code>@app.command(\"import\")\n@clean_except\ndef import_dataset(\ndata_uid: str = typer.Option(\n..., \"--data_uid\", \"-d\", help=\"Dataset UID to be imported\"\n),\ninput_path: str = typer.Option(\n...,\n\"--input\",\n\"-i\",\nhelp=\"Path of the tar.gz file (dataset backup) to be imported.\",\n),\nraw_path: str = typer.Option(\nNone,\n\"--raw_dataset_path\",\nhelp=\"New path of the DEVELOPMENT dataset raw data to be saved. Directory should be empty or doesn't exist.\",\n),\n):\n\"\"\"Imports dataset files from specified tar.gz file.\"\"\"\nImportDataset.run(data_uid, input_path, raw_path)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/dataset/dataset/#commands.dataset.dataset.list","title":"<code>list(unregistered=typer.Option(False, '--unregistered', help='Get unregistered datasets'), mine=typer.Option(False, '--mine', help='Get current-user datasets'), mlcube=typer.Option(None, '--data-preparation-container', '-m', help='Get datasets for a given data preparation container'), name=typer.Option(None, '--name', help='Filter by name'), owner=typer.Option(None, '--owner', help='Filter by owner'), state=typer.Option(None, '--state', help='Filter by state (DEVELOPMENT/OPERATION)'), is_valid=typer.Option(None, '--valid/--invalid', help='Filter by valid status'))</code>","text":"<p>List datasets</p> Source code in <code>cli/medperf/commands/dataset/dataset.py</code> <pre><code>@app.command(\"ls\")\n@clean_except\ndef list(\nunregistered: bool = typer.Option(\nFalse, \"--unregistered\", help=\"Get unregistered datasets\"\n),\nmine: bool = typer.Option(False, \"--mine\", help=\"Get current-user datasets\"),\nmlcube: int = typer.Option(\nNone,\n\"--data-preparation-container\",\n\"-m\",\nhelp=\"Get datasets for a given data preparation container\",\n),\nname: str = typer.Option(None, \"--name\", help=\"Filter by name\"),\nowner: int = typer.Option(None, \"--owner\", help=\"Filter by owner\"),\nstate: str = typer.Option(None, \"--state\", help=\"Filter by state (DEVELOPMENT/OPERATION)\"),\nis_valid: bool = typer.Option(None, \"--valid/--invalid\", help=\"Filter by valid status\"),\n):\n\"\"\"List datasets\"\"\"\nEntityList.run(\nDataset,\nfields=[\n\"UID\",\n\"Name\",\n\"Data Preparation Container UID\",\n\"State\",\n\"Status\",\n\"Owner\",\n],\nunregistered=unregistered,\nmine_only=mine,\nmlcube=mlcube,\nname=name,\nowner=owner,\nstate=state,\nis_valid=is_valid,\n)\n</code></pre>"},{"location":"reference/commands/dataset/dataset/#commands.dataset.dataset.prepare","title":"<code>prepare(data_uid=typer.Option(..., '--data_uid', '-d', help='Dataset UID'), approval=typer.Option(False, '-y', help='Skip report submission approval step (In this case, it is assumed to be approved)'))</code>","text":"<p>Runs the Data preparation step for a raw dataset</p> Source code in <code>cli/medperf/commands/dataset/dataset.py</code> <pre><code>@app.command(\"prepare\")\n@clean_except\ndef prepare(\ndata_uid: str = typer.Option(..., \"--data_uid\", \"-d\", help=\"Dataset UID\"),\napproval: bool = typer.Option(\nFalse,\n\"-y\",\nhelp=\"Skip report submission approval step (In this case, it is assumed to be approved)\",\n),\n):\n\"\"\"Runs the Data preparation step for a raw dataset\"\"\"\nui = config.ui\nDataPreparation.run(data_uid, approve_sending_reports=approval)\nui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/dataset/dataset/#commands.dataset.dataset.set_operational","title":"<code>set_operational(data_uid=typer.Option(..., '--data_uid', '-d', help='Dataset UID'), approval=typer.Option(False, '-y', help='Skip confirmation and statistics submission approval step'))</code>","text":"<p>Marks a dataset as Operational</p> Source code in <code>cli/medperf/commands/dataset/dataset.py</code> <pre><code>@app.command(\"set_operational\")\n@clean_except\ndef set_operational(\ndata_uid: str = typer.Option(..., \"--data_uid\", \"-d\", help=\"Dataset UID\"),\napproval: bool = typer.Option(\nFalse, \"-y\", help=\"Skip confirmation and statistics submission approval step\"\n),\n):\n\"\"\"Marks a dataset as Operational\"\"\"\nui = config.ui\nDatasetSetOperational.run(data_uid, approved=approval)\nui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/dataset/dataset/#commands.dataset.dataset.submit","title":"<code>submit(benchmark_uid=typer.Option(None, '--benchmark', '-b', help='UID of the desired benchmark'), data_prep_uid=typer.Option(None, '--data_prep', '-p', help='UID of the desired preparation container'), data_path=typer.Option(..., '--data_path', '-d', help='Path to the data'), labels_path=typer.Option(..., '--labels_path', '-l', help='Path to the labels'), metadata_path=typer.Option(None, '--metadata_path', '-m', help='Metadata folder location (Might be required if the dataset is already prepared)'), name=typer.Option(..., '--name', help='A human-readable name of the dataset'), description=typer.Option(None, '--description', help='A description of the dataset'), location=typer.Option(None, '--location', help='Location or Institution the data belongs to'), approval=typer.Option(False, '-y', help='Skip approval step'), submit_as_prepared=typer.Option(False, '--submit-as-prepared', help='Use this flag if the dataset is already prepared'))</code>","text":"<p>Submits a Dataset instance to the backend</p> Source code in <code>cli/medperf/commands/dataset/dataset.py</code> <pre><code>@app.command(\"submit\")\n@clean_except\ndef submit(\nbenchmark_uid: int = typer.Option(\nNone, \"--benchmark\", \"-b\", help=\"UID of the desired benchmark\"\n),\ndata_prep_uid: int = typer.Option(\nNone, \"--data_prep\", \"-p\", help=\"UID of the desired preparation container\"\n),\ndata_path: str = typer.Option(..., \"--data_path\", \"-d\", help=\"Path to the data\"),\nlabels_path: str = typer.Option(\n..., \"--labels_path\", \"-l\", help=\"Path to the labels\"\n),\nmetadata_path: str = typer.Option(\nNone,\n\"--metadata_path\",\n\"-m\",\nhelp=\"Metadata folder location (Might be required if the dataset is already prepared)\",\n),\nname: str = typer.Option(\n..., \"--name\", help=\"A human-readable name of the dataset\"\n),\ndescription: str = typer.Option(\nNone, \"--description\", help=\"A description of the dataset\"\n),\nlocation: str = typer.Option(\nNone, \"--location\", help=\"Location or Institution the data belongs to\"\n),\napproval: bool = typer.Option(False, \"-y\", help=\"Skip approval step\"),\nsubmit_as_prepared: bool = typer.Option(\nFalse,\n\"--submit-as-prepared\",\nhelp=\"Use this flag if the dataset is already prepared\",\n),\n):\n\"\"\"Submits a Dataset instance to the backend\"\"\"\nui = config.ui\nDataCreation.run(\nbenchmark_uid,\ndata_prep_uid,\ndata_path,\nlabels_path,\nmetadata_path,\nname=name,\ndescription=description,\nlocation=location,\napproved=approval,\nsubmit_as_prepared=submit_as_prepared,\n)\nui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/dataset/dataset/#commands.dataset.dataset.train","title":"<code>train(training_exp_id=typer.Option(..., '--training_exp_id', '-t', help='UID of the desired benchmark'), data_uid=typer.Option(..., '--data_uid', '-d', help='Registered Dataset UID'), overwrite=typer.Option(False, '--overwrite', help='Overwrite outputs if present'), restart_on_failure=typer.Option(False, '--restart_on_failure', help='Keep restarting failing training processes until Keyboard interrupt'), approval=typer.Option(False, '-y', help='Skip approval step'), skip_restart_on_failure_prompt=typer.Option(False, '--skip_restart_on_failure_prompt', help='Skip restart on failure prompt'))</code>","text":"<p>Runs training</p> Source code in <code>cli/medperf/commands/dataset/dataset.py</code> <pre><code>@app.command(\"train\")\n@clean_except\ndef train(\ntraining_exp_id: int = typer.Option(\n..., \"--training_exp_id\", \"-t\", help=\"UID of the desired benchmark\"\n),\ndata_uid: int = typer.Option(\n..., \"--data_uid\", \"-d\", help=\"Registered Dataset UID\"\n),\noverwrite: bool = typer.Option(\nFalse, \"--overwrite\", help=\"Overwrite outputs if present\"\n),\nrestart_on_failure: bool = typer.Option(\nFalse,\n\"--restart_on_failure\",\nhelp=\"Keep restarting failing training processes until Keyboard interrupt\",\n),\napproval: bool = typer.Option(False, \"-y\", help=\"Skip approval step\"),\nskip_restart_on_failure_prompt: bool = typer.Option(\nFalse, \"--skip_restart_on_failure_prompt\", help=\"Skip restart on failure prompt\"\n),\n):\n\"\"\"Runs training\"\"\"\nTrainingExecution.run(\ntraining_exp_id,\ndata_uid,\noverwrite,\napproval,\nrestart_on_failure,\nskip_restart_on_failure_prompt,\n)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/dataset/dataset/#commands.dataset.dataset.view","title":"<code>view(entity_id=typer.Argument(None, help='Dataset ID'), format=typer.Option('yaml', '-f', '--format', help='Format to display contents. Available formats: [yaml, json]'), unregistered=typer.Option(False, '--unregistered', help='Display unregistered datasets if dataset ID is not provided'), mine=typer.Option(False, '--mine', help='Display current-user datasets if dataset ID is not provided'), output=typer.Option(None, '--output', '-o', help='Output file to store contents. If not provided, the output will be displayed'))</code>","text":"<p>Displays the information of one or more datasets</p> Source code in <code>cli/medperf/commands/dataset/dataset.py</code> <pre><code>@app.command(\"view\")\n@clean_except\ndef view(\nentity_id: Optional[str] = typer.Argument(None, help=\"Dataset ID\"),\nformat: str = typer.Option(\n\"yaml\",\n\"-f\",\n\"--format\",\nhelp=\"Format to display contents. Available formats: [yaml, json]\",\n),\nunregistered: bool = typer.Option(\nFalse,\n\"--unregistered\",\nhelp=\"Display unregistered datasets if dataset ID is not provided\",\n),\nmine: bool = typer.Option(\nFalse,\n\"--mine\",\nhelp=\"Display current-user datasets if dataset ID is not provided\",\n),\noutput: str = typer.Option(\nNone,\n\"--output\",\n\"-o\",\nhelp=\"Output file to store contents. If not provided, the output will be displayed\",\n),\n):\n\"\"\"Displays the information of one or more datasets\"\"\"\nEntityView.run(entity_id, Dataset, format, unregistered, mine, output)\n</code></pre>"},{"location":"reference/commands/dataset/export_dataset/","title":"Export dataset","text":""},{"location":"reference/commands/dataset/import_dataset/","title":"Import dataset","text":""},{"location":"reference/commands/dataset/prepare/","title":"Prepare","text":""},{"location":"reference/commands/dataset/set_operational/","title":"Set operational","text":""},{"location":"reference/commands/dataset/set_operational/#commands.dataset.set_operational.DatasetSetOperational","title":"<code>DatasetSetOperational</code>","text":"Source code in <code>cli/medperf/commands/dataset/set_operational.py</code> <pre><code>class DatasetSetOperational:\n# TODO: this will be refactored when merging entity edit PR\n@classmethod\ndef run(cls, dataset_id: int, approved: bool = False):\npreparation = cls(dataset_id, approved)\npreparation.validate()\npreparation.generate_uids()\npreparation.set_statistics()\npreparation.set_operational()\npreparation.update()\npreparation.write()\nreturn preparation.dataset.id\ndef __init__(self, dataset_id: int, approved: bool):\nself.ui = config.ui\nself.dataset = Dataset.get(dataset_id)\nself.approved = approved\ndef validate(self):\nif self.dataset.state == \"OPERATION\":\nraise InvalidArgumentError(\"The dataset is already operational\")\nif not self.dataset.is_ready():\nraise InvalidArgumentError(\"The dataset is not checked\")\ndef generate_uids(self):\n\"\"\"Auto-generates dataset UIDs for both input and output paths\"\"\"\nraw_data_path, raw_labels_path = self.dataset.get_raw_paths()\nprepared_data_path = self.dataset.data_path\nprepared_labels_path = self.dataset.labels_path\nin_uid = get_folders_hash([raw_data_path, raw_labels_path])\ngenerated_uid = get_folders_hash([prepared_data_path, prepared_labels_path])\nself.dataset.input_data_hash = in_uid\nself.dataset.generated_uid = generated_uid\ndef set_statistics(self):\nwith open(self.dataset.statistics_path, \"r\") as f:\nstats = yaml.safe_load(f)\nself.dataset.generated_metadata = stats\ndef set_operational(self):\nself.dataset.state = \"OPERATION\"\ndef update(self):\nmsg = \"This is the information that is going to be transmitted to the medperf server\"\nconfig.ui.print_warning(msg)\nbody = self.todict()\ndict_pretty_print(body)\nmsg = \"Do you approve sending the presented data to MedPerf? [Y/n] \"\nself.approved = self.approved or approval_prompt(msg)\nif self.approved:\nconfig.comms.update_dataset(self.dataset.id, body)\nreturn\nraise CleanExit(\"Setting Dataset as operational was cancelled\")\ndef todict(self) -&gt; dict:\n\"\"\"Dictionary representation of the update body\n        Returns:\n            dict: dictionary containing information pertaining the dataset.\n        \"\"\"\nreturn {\n\"input_data_hash\": self.dataset.input_data_hash,\n\"generated_uid\": self.dataset.generated_uid,\n\"generated_metadata\": self.dataset.generated_metadata,\n\"state\": self.dataset.state,\n}\ndef write(self) -&gt; str:\n\"\"\"Writes the registration into disk\n        Args:\n            filename (str, optional): name of the file. Defaults to config.reg_file.\n        \"\"\"\nself.dataset.write()\n</code></pre>"},{"location":"reference/commands/dataset/set_operational/#commands.dataset.set_operational.DatasetSetOperational.generate_uids","title":"<code>generate_uids()</code>","text":"<p>Auto-generates dataset UIDs for both input and output paths</p> Source code in <code>cli/medperf/commands/dataset/set_operational.py</code> <pre><code>def generate_uids(self):\n\"\"\"Auto-generates dataset UIDs for both input and output paths\"\"\"\nraw_data_path, raw_labels_path = self.dataset.get_raw_paths()\nprepared_data_path = self.dataset.data_path\nprepared_labels_path = self.dataset.labels_path\nin_uid = get_folders_hash([raw_data_path, raw_labels_path])\ngenerated_uid = get_folders_hash([prepared_data_path, prepared_labels_path])\nself.dataset.input_data_hash = in_uid\nself.dataset.generated_uid = generated_uid\n</code></pre>"},{"location":"reference/commands/dataset/set_operational/#commands.dataset.set_operational.DatasetSetOperational.todict","title":"<code>todict()</code>","text":"<p>Dictionary representation of the update body</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dictionary containing information pertaining the dataset.</p> Source code in <code>cli/medperf/commands/dataset/set_operational.py</code> <pre><code>def todict(self) -&gt; dict:\n\"\"\"Dictionary representation of the update body\n    Returns:\n        dict: dictionary containing information pertaining the dataset.\n    \"\"\"\nreturn {\n\"input_data_hash\": self.dataset.input_data_hash,\n\"generated_uid\": self.dataset.generated_uid,\n\"generated_metadata\": self.dataset.generated_metadata,\n\"state\": self.dataset.state,\n}\n</code></pre>"},{"location":"reference/commands/dataset/set_operational/#commands.dataset.set_operational.DatasetSetOperational.write","title":"<code>write()</code>","text":"<p>Writes the registration into disk</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>name of the file. Defaults to config.reg_file.</p> required Source code in <code>cli/medperf/commands/dataset/set_operational.py</code> <pre><code>def write(self) -&gt; str:\n\"\"\"Writes the registration into disk\n    Args:\n        filename (str, optional): name of the file. Defaults to config.reg_file.\n    \"\"\"\nself.dataset.write()\n</code></pre>"},{"location":"reference/commands/dataset/submit/","title":"Submit","text":""},{"location":"reference/commands/dataset/submit/#commands.dataset.submit.DataCreation","title":"<code>DataCreation</code>","text":"Source code in <code>cli/medperf/commands/dataset/submit.py</code> <pre><code>class DataCreation:\n@classmethod\ndef run(\ncls,\nbenchmark_uid: int,\nprep_cube_uid: int,\ndata_path: str,\nlabels_path: str,\nmetadata_path: str = None,\nname: str = None,\ndescription: str = None,\nlocation: str = None,\napproved: bool = False,\nsubmit_as_prepared: bool = False,\nfor_test: bool = False,\n):\npreparation = cls(\nbenchmark_uid,\nprep_cube_uid,\ndata_path,\nlabels_path,\nmetadata_path,\nname,\ndescription,\nlocation,\napproved,\nsubmit_as_prepared,\nfor_test,\n)\npreparation.validate()\npreparation.validate_prep_cube()\npreparation.create_dataset_object()\nif submit_as_prepared:\npreparation.make_dataset_prepared()\nupdated_dataset_dict = preparation.upload()\npreparation.to_permanent_path(updated_dataset_dict)\npreparation.write(updated_dataset_dict)\nreturn updated_dataset_dict[\"id\"]\ndef __init__(\nself,\nbenchmark_uid: int,\nprep_cube_uid: int,\ndata_path: str,\nlabels_path: str,\nmetadata_path: str,\nname: str,\ndescription: str,\nlocation: str,\napproved: bool,\nsubmit_as_prepared: bool,\nfor_test: bool,\n):\nself.ui = config.ui\nself.data_path = sanitize_path(data_path)\nself.labels_path = sanitize_path(labels_path)\nself.metadata_path = sanitize_path(metadata_path)\nself.name = name\nself.description = description\nself.location = location\nself.benchmark_uid = benchmark_uid\nself.prep_cube_uid = prep_cube_uid\nself.approved = approved\nself.submit_as_prepared = submit_as_prepared\nself.for_test = for_test\ndef validate(self):\nif not os.path.exists(self.data_path):\nraise InvalidArgumentError(\"The provided data path doesn't exist\")\nif not os.path.exists(self.labels_path):\nraise InvalidArgumentError(\"The provided labels path doesn't exist\")\nif not self.submit_as_prepared and self.metadata_path:\nraise InvalidArgumentError(\n\"metadata path should only be provided when the dataset is submitted as prepared\"\n)\nif self.metadata_path:\nself.metadata_path = str(Path(self.metadata_path).resolve())\nif not os.path.exists(self.metadata_path):\nraise InvalidArgumentError(\"The provided metadata path doesn't exist\")\n# TODO: should we check the prep mlcube and accordingly check if metadata path\n#       is required? For now, we will anyway create an empty metadata folder\n#       (in self.make_dataset_prepared)\ntoo_many_resources = self.benchmark_uid and self.prep_cube_uid\nno_resource = self.benchmark_uid is None and self.prep_cube_uid is None\nif no_resource or too_many_resources:\nraise InvalidArgumentError(\n\"Must provide either a benchmark or a preparation container\"\n)\ndef validate_prep_cube(self):\nif self.prep_cube_uid is None:\nbenchmark = Benchmark.get(self.benchmark_uid)\nself.prep_cube_uid = benchmark.data_preparation_mlcube\nCube.get(self.prep_cube_uid)\ndef create_dataset_object(self):\n\"\"\"generates dataset UIDs for both input path\"\"\"\nin_uid = get_folders_hash([self.data_path, self.labels_path])\ndataset = Dataset(\nname=self.name,\ndescription=self.description,\nlocation=self.location,\ndata_preparation_mlcube=self.prep_cube_uid,\ninput_data_hash=in_uid,\ngenerated_uid=in_uid,\nsplit_seed=0,\ngenerated_metadata={},\nstate=\"DEVELOPMENT\",\nsubmitted_as_prepared=self.submit_as_prepared,\nfor_test=self.for_test,\n)\ndataset.write()\nconfig.tmp_paths.append(dataset.path)\ndataset.set_raw_paths(\nraw_data_path=self.data_path,\nraw_labels_path=self.labels_path,\n)\nself.dataset = dataset\ndef make_dataset_prepared(self):\nshutil.copytree(self.data_path, self.dataset.data_path)\nshutil.copytree(self.labels_path, self.dataset.labels_path)\nif self.metadata_path:\nshutil.copytree(self.metadata_path, self.dataset.metadata_path)\nelse:\n# Create an empty folder. The statistics logic should\n# also expect an empty folder to accommodate for users who\n# have prepared datasets with no the metadata information\nos.makedirs(self.dataset.metadata_path, exist_ok=True)\ndef upload(self):\nsubmission_dict = self.dataset.todict()\ndict_pretty_print(submission_dict)\nmsg = \"Do you approve the registration of the presented data to MedPerf? [Y/n] \"\nwarning = (\n\"Upon submission, your email address will be visible to the Data Preparation\"\n+ \" Owner for traceability and debugging purposes.\"\n)\nself.ui.print_warning(warning)\nself.approved = self.approved or approval_prompt(msg)\nif self.approved:\nupdated_body = self.dataset.upload()\nreturn updated_body\nraise CleanExit(\"Dataset submission operation cancelled\")\ndef to_permanent_path(self, updated_dataset_dict: dict):\n\"\"\"Renames the temporary benchmark submission to a permanent one\n        Args:\n            bmk_dict (dict): dictionary containing updated information of the submitted benchmark\n        \"\"\"\nold_dataset_loc = self.dataset.path\nupdated_dataset = Dataset(**updated_dataset_dict)\nnew_dataset_loc = updated_dataset.path\nremove_path(new_dataset_loc)\nos.rename(old_dataset_loc, new_dataset_loc)\ndef write(self, updated_dataset_dict):\ndataset = Dataset(**updated_dataset_dict)\ndataset.write()\n</code></pre>"},{"location":"reference/commands/dataset/submit/#commands.dataset.submit.DataCreation.create_dataset_object","title":"<code>create_dataset_object()</code>","text":"<p>generates dataset UIDs for both input path</p> Source code in <code>cli/medperf/commands/dataset/submit.py</code> <pre><code>def create_dataset_object(self):\n\"\"\"generates dataset UIDs for both input path\"\"\"\nin_uid = get_folders_hash([self.data_path, self.labels_path])\ndataset = Dataset(\nname=self.name,\ndescription=self.description,\nlocation=self.location,\ndata_preparation_mlcube=self.prep_cube_uid,\ninput_data_hash=in_uid,\ngenerated_uid=in_uid,\nsplit_seed=0,\ngenerated_metadata={},\nstate=\"DEVELOPMENT\",\nsubmitted_as_prepared=self.submit_as_prepared,\nfor_test=self.for_test,\n)\ndataset.write()\nconfig.tmp_paths.append(dataset.path)\ndataset.set_raw_paths(\nraw_data_path=self.data_path,\nraw_labels_path=self.labels_path,\n)\nself.dataset = dataset\n</code></pre>"},{"location":"reference/commands/dataset/submit/#commands.dataset.submit.DataCreation.to_permanent_path","title":"<code>to_permanent_path(updated_dataset_dict)</code>","text":"<p>Renames the temporary benchmark submission to a permanent one</p> <p>Parameters:</p> Name Type Description Default <code>bmk_dict</code> <code>dict</code> <p>dictionary containing updated information of the submitted benchmark</p> required Source code in <code>cli/medperf/commands/dataset/submit.py</code> <pre><code>def to_permanent_path(self, updated_dataset_dict: dict):\n\"\"\"Renames the temporary benchmark submission to a permanent one\n    Args:\n        bmk_dict (dict): dictionary containing updated information of the submitted benchmark\n    \"\"\"\nold_dataset_loc = self.dataset.path\nupdated_dataset = Dataset(**updated_dataset_dict)\nnew_dataset_loc = updated_dataset.path\nremove_path(new_dataset_loc)\nos.rename(old_dataset_loc, new_dataset_loc)\n</code></pre>"},{"location":"reference/commands/dataset/train/","title":"Train","text":""},{"location":"reference/commands/dataset/train/#commands.dataset.train.TrainingExecution","title":"<code>TrainingExecution</code>","text":"Source code in <code>cli/medperf/commands/dataset/train.py</code> <pre><code>class TrainingExecution:\n@classmethod\ndef run(\ncls,\ntraining_exp_id: int,\ndata_uid: int,\noverwrite: bool = False,\napproved: bool = False,\nrestart_on_failure: bool = False,\nskip_restart_on_failure_prompt: bool = False,\n):\n\"\"\"Starts the aggregation server of a training experiment\n        Args:\n            training_exp_id (int): Training experiment UID.\n        \"\"\"\nif restart_on_failure:\napproved = True\noverwrite = True\nexecution = cls(training_exp_id, data_uid, overwrite, approved)\nif restart_on_failure and not skip_restart_on_failure_prompt:\nexecution.confirm_restart_on_failure()\nwhile True:\nexecution.prepare()\nexecution.validate()\nexecution.check_existing_outputs()\nexecution.prepare_plan()\nexecution.prepare_pki_assets()\nexecution.confirm_run()\nwith config.ui.interactive():\nexecution.prepare_training_cube()\ntry:\nexecution.run_experiment()\nbreak\nexcept ExecutionError as e:\nprint(str(e))\nif not restart_on_failure:\nbreak\ndef __init__(\nself, training_exp_id: int, data_uid: int, overwrite: bool, approved: bool\n) -&gt; None:\nself.training_exp_id = training_exp_id\nself.data_uid = data_uid\nself.overwrite = overwrite\nself.ui = config.ui\nself.approved = approved\ndef confirm_restart_on_failure(self):\nmsg = (\n\"You chose to restart on failure. This means that the training process\"\n\" will automatically restart, without your approval, even if training configuration\"\n\" changes from the server side. Do you confirm? [Y/n] \"\n)\nif not approval_prompt(msg):\nraise CleanExit(\n\"Training cancelled. Rerun without the --restart_on_failure flag.\"\n)\ndef prepare(self):\nself.training_exp = TrainingExp.get(self.training_exp_id)\nself.ui.print(f\"Training Execution: {self.training_exp.name}\")\nself.event = TrainingEvent.from_experiment(self.training_exp_id)\nself.dataset = Dataset.get(self.data_uid)\nself.user_email: str = get_medperf_user_data()[\"email\"]\nself.out_logs = os.path.join(self.event.col_out_logs, str(self.dataset.id))\ndef validate(self):\nif self.dataset.id is None:\nmsg = \"The provided dataset is not registered.\"\nraise InvalidArgumentError(msg)\nif self.dataset.state != \"OPERATION\":\nmsg = \"The provided dataset is not operational.\"\nraise InvalidArgumentError(msg)\nif self.event.finished:\nmsg = \"The provided training experiment has to start a training event.\"\nraise InvalidArgumentError(msg)\ndef check_existing_outputs(self):\nmsg = (\n\"Outputs still exist from previous runs. Overwrite\"\n\" them by rerunning the command with --overwrite\"\n)\npaths = [self.out_logs]\nfor path in paths:\nif os.path.exists(path):\nif not self.overwrite:\nraise MedperfException(msg)\nremove_path(path)\ndef prepare_plan(self):\nself.training_exp.prepare_plan()\ndef prepare_pki_assets(self):\nca = CA.from_experiment(self.training_exp_id)\ntrust(ca)\nself.dataset_pki_assets = get_pki_assets_path(self.user_email, ca.name)\nself.ca = ca\ndef confirm_run(self):\nmsg = (\n\"Above is the training configuration that will be used.\"\n\" Do you confirm starting training? [Y/n] \"\n)\ndict_pretty_print(self.training_exp.plan)\nself.approved = self.approved or approval_prompt(msg)\nif not self.approved:\nraise CleanExit(\"Training cancelled.\")\ndef prepare_training_cube(self):\nself.cube = self.__get_cube(self.training_exp.fl_mlcube, \"FL\")\ndef __get_cube(self, uid: int, name: str) -&gt; Cube:\nself.ui.text = (\n\"Retrieving and setting up training container. This may take some time.\"\n)\ncube = Cube.get(uid)\ncube.download_run_files()\nself.ui.print(f\"&gt; Container '{name}' download complete\")\nreturn cube\ndef run_experiment(self):\nparticipant_label = get_participant_label(self.user_email, self.dataset.id)\nenv = {\"MEDPERF_PARTICIPANT_LABEL\": participant_label}\nmounts = {\n\"data_path\": self.dataset.data_path,\n\"labels_path\": self.dataset.labels_path,\n\"node_cert_folder\": self.dataset_pki_assets,\n\"ca_cert_folder\": self.ca.pki_assets,\n\"plan_path\": self.training_exp.plan_path,\n\"output_logs\": self.out_logs,\n}\nself.ui.text = \"Running Training\"\nself.cube.run(task=\"train\", mounts=mounts, env=env, disable_network=False)\n</code></pre>"},{"location":"reference/commands/dataset/train/#commands.dataset.train.TrainingExecution.run","title":"<code>run(training_exp_id, data_uid, overwrite=False, approved=False, restart_on_failure=False, skip_restart_on_failure_prompt=False)</code>  <code>classmethod</code>","text":"<p>Starts the aggregation server of a training experiment</p> <p>Parameters:</p> Name Type Description Default <code>training_exp_id</code> <code>int</code> <p>Training experiment UID.</p> required Source code in <code>cli/medperf/commands/dataset/train.py</code> <pre><code>@classmethod\ndef run(\ncls,\ntraining_exp_id: int,\ndata_uid: int,\noverwrite: bool = False,\napproved: bool = False,\nrestart_on_failure: bool = False,\nskip_restart_on_failure_prompt: bool = False,\n):\n\"\"\"Starts the aggregation server of a training experiment\n    Args:\n        training_exp_id (int): Training experiment UID.\n    \"\"\"\nif restart_on_failure:\napproved = True\noverwrite = True\nexecution = cls(training_exp_id, data_uid, overwrite, approved)\nif restart_on_failure and not skip_restart_on_failure_prompt:\nexecution.confirm_restart_on_failure()\nwhile True:\nexecution.prepare()\nexecution.validate()\nexecution.check_existing_outputs()\nexecution.prepare_plan()\nexecution.prepare_pki_assets()\nexecution.confirm_run()\nwith config.ui.interactive():\nexecution.prepare_training_cube()\ntry:\nexecution.run_experiment()\nbreak\nexcept ExecutionError as e:\nprint(str(e))\nif not restart_on_failure:\nbreak\n</code></pre>"},{"location":"reference/commands/execution/create/","title":"Create","text":""},{"location":"reference/commands/execution/create/#commands.execution.create.BenchmarkExecution","title":"<code>BenchmarkExecution</code>","text":"Source code in <code>cli/medperf/commands/execution/create.py</code> <pre><code>class BenchmarkExecution:\n@classmethod\ndef run(\ncls,\nbenchmark_uid: int,\ndata_uid: int,\nmodels_uids: Optional[List[int]] = None,\nmodels_input_file: Optional[str] = None,\nignore_model_errors=False,\nignore_failed_experiments=False,\nno_cache=False,\nshow_summary=False,\nrerun_finalized_executions=False,\n):\n\"\"\"Benchmark execution flow.\n        How the following variables affect whether to execute or no:\n        no_cache, rerun_finalized_executions, and\n        execution object state ((0) doesn't exist, (1) exist, (2) executed, (3) finalized):\n        if no_cache is false and rerun_finalized_executions is false, then execute if (0) or (1)\n        if no_cache is true and rerun_finalized_executions is false, then execute if (0) or (1) or(2)\n        if no_cache is true and rerun_finalized_executions is true, then execute anyway\n        Args:\n            benchmark_uid (int): UID of the desired benchmark\n            data_uid (str): Registered Dataset UID\n            models_uids (List|None): list of model UIDs to execute.\n                                    if None, models_input_file will be used\n            models_input_file: filename to read from\n            if models_uids and models_input_file are None, use all benchmark models\n        \"\"\"\nif rerun_finalized_executions:\nno_cache = True\nexecution_flow = cls(\nbenchmark_uid,\ndata_uid,\nmodels_uids,\nmodels_input_file,\nignore_model_errors,\nignore_failed_experiments,\nno_cache,\nrerun_finalized_executions,\n)\nwith execution_flow.ui.interactive():\nexecution_flow.prepare()\nexecution_flow.validate()\nexecution_flow.prepare_models()\nexecution_flow.load_existing_executions()\nwith execution_flow.ui.interactive():\nexecutions = execution_flow.run_experiments()\nif show_summary:\nexecution_flow.print_summary()\nreturn executions\ndef __init__(\nself,\nbenchmark_uid: int,\ndata_uid: int,\nmodels_uids,\nmodels_input_file: str = None,\nignore_model_errors=False,\nignore_failed_experiments=False,\nno_cache=False,\nrerun_finalized_executions=False,\n):\nself.benchmark_uid = benchmark_uid\nself.data_uid = data_uid\nself.models_uids = models_uids\nself.models_input_file = models_input_file\nself.ui = config.ui\nself.evaluator = None\nself.ignore_model_errors = ignore_model_errors\nself.ignore_failed_experiments = ignore_failed_experiments\nself.existing_executions = {}\nself.experiments = []\nself.no_cache = no_cache\nself.rerun_finalized_executions = rerun_finalized_executions\ndef prepare(self):\nself.benchmark = Benchmark.get(self.benchmark_uid)\nself.ui.print(f\"Benchmark Execution: {self.benchmark.name}\")\nself.dataset = Dataset.get(self.data_uid)\nevaluator_uid = self.benchmark.data_evaluator_mlcube\nself.evaluator = self.__get_cube(evaluator_uid, \"Evaluator\")\ndef validate(self):\ndset_prep_cube = self.dataset.data_preparation_mlcube\nbmark_prep_cube = self.benchmark.data_preparation_mlcube\nif self.dataset.id is None:\nmsg = \"The provided dataset is not registered.\"\nraise InvalidArgumentError(msg)\nif self.dataset.state != \"OPERATION\":\nmsg = \"The provided dataset is not operational.\"\nraise InvalidArgumentError(msg)\nif dset_prep_cube != bmark_prep_cube:\nmsg = \"The provided dataset is not compatible with the specified benchmark.\"\nraise InvalidArgumentError(msg)\n# TODO: there is no check if dataset is associated with the benchmark\n#       Note that if it is present, this will break dataset association creation logic\ndef prepare_models(self):\nif self.models_input_file:\nself.models_uids = self.__get_models_from_file()\nif self.models_uids == [self.benchmark.reference_model_mlcube]:\n# avoid the need of sending a request to the server for\n# finding the benchmark's associated models\nreturn\nbenchmark_models = Benchmark.get_models_uids(self.benchmark_uid)\nbenchmark_models.append(self.benchmark.reference_model_mlcube)\nif self.models_uids is None:\nself.models_uids = benchmark_models\nelse:\nself.__validate_models(benchmark_models)\ndef __get_models_from_file(self):\nif not os.path.exists(self.models_input_file):\nraise InvalidArgumentError(\"The given file does not exist\")\nwith open(self.models_input_file) as f:\ntext = f.read()\nmodels = text.strip().split(\",\")\ntry:\nreturn list(map(int, models))\nexcept ValueError as e:\nmsg = f\"Could not parse the given file: {e}. \"\nmsg += \"The file should contain a list of comma-separated integers\"\nraise InvalidArgumentError(msg)\ndef __validate_models(self, benchmark_models):\nmodels_set = set(self.models_uids)\nbenchmark_models_set = set(benchmark_models)\nnon_assoc_cubes = models_set.difference(benchmark_models_set)\nif non_assoc_cubes:\nif len(non_assoc_cubes) &gt; 1:\nmsg = f\"Model of UID {non_assoc_cubes} is not associated with the specified benchmark.\"\nelse:\nmsg = f\"Models of UIDs {non_assoc_cubes} are not associated with the specified benchmark.\"\nraise InvalidArgumentError(msg)\ndef load_existing_executions(self):\nuser_id = get_medperf_user_data()[\"id\"]\nexecutions = Execution.all(filters={\"owner\": user_id})\nbenchmark_dset_executions = [\nexecution\nfor execution in executions\nif execution.benchmark == self.benchmark_uid\nand execution.dataset == self.data_uid\n]\nbenchmark_dset_executions = filter_latest_executions(benchmark_dset_executions)\nself.existing_executions = {\nexecution.model: execution for execution in benchmark_dset_executions\n}\ndef __get_cube(self, uid: int, name: str) -&gt; Cube:\ncube = Cube.get(uid)\nself.ui.text = f\"Retrieving {name} container '{cube.name}'\"\ncube.download_run_files()\nself.ui.print(f\"&gt; {name} Container '{cube.name}' download complete\")\nreturn cube\ndef run_experiments(self) -&gt; list[Execution]:\nfor model_uid in self.models_uids:\nexecution = self.existing_executions.get(model_uid, None)\nif (\nexecution is None\nor execution.finalized\nand self.rerun_finalized_executions\n):\nexecution = self.__create_execution(model_uid)\nif self.rerun_finalized_executions:\nshould_run = True\nelse:\nif self.no_cache:\nshould_run = not execution.finalized\nelse:\nshould_run = not execution.is_executed()\nif should_run:\nexecution.unmark_as_executed()\nelse:\nself.experiments.append(\n{\n\"model_uid\": model_uid,\n\"execution\": execution,\n\"cached\": True,\n\"error\": \"\",\n\"partial\": execution.is_partial(),\n}\n)\ncontinue\ntry:\nmodel_cube = self.__get_cube(model_uid, \"Model\")\nexecution_summary = ExecutionFlow.run(\ndataset=self.dataset,\nmodel=model_cube,\nevaluator=self.evaluator,\nexecution=execution,\nignore_model_errors=self.ignore_model_errors,\n)\nexcept MedperfException as e:\nself.__handle_experiment_error(model_uid, e)\nself.experiments.append(\n{\n\"model_uid\": model_uid,\n\"execution\": None,\n\"cached\": False,\n\"error\": str(e),\n\"partial\": \"N/A\",\n}\n)\ncontinue\nexecution.mark_as_executed()\nexecution.save_results(\nexecution_summary[\"results\"], execution_summary[\"partial\"]\n)\nself.experiments.append(\n{\n\"model_uid\": model_uid,\n\"execution\": execution,\n\"cached\": False,\n\"error\": \"\",\n\"partial\": execution_summary[\"partial\"],\n}\n)\nreturn [experiment[\"execution\"] for experiment in self.experiments]\ndef __handle_experiment_error(self, model_uid, exception):\nif isinstance(exception, InvalidEntityError):\nconfig.ui.print_error(\nf\"There was an error when retrieving the model container {model_uid}: {exception}\"\n)\nelif isinstance(exception, ExecutionError):\nconfig.ui.print_error(\nf\"There was an error when executing the benchmark with the model {model_uid}: {exception}\"\n)\nelse:\nraise exception\nif not self.ignore_failed_experiments:\nraise exception\ndef __create_execution(self, model_uid: int) -&gt; Execution:\n# Get or create an execution object on the server\nquery_dict = {\n\"dataset\": self.data_uid,\n\"model\": model_uid,\n\"benchmark\": self.benchmark_uid,\n\"name\": self.__execution_name(model_uid),\n}\nexecution = Execution(**query_dict)\nupdated_exec_dict = execution.upload()\nexecution = Execution(**updated_exec_dict)\nexecution.write()\nreturn execution\ndef __execution_name(self, model_uid):\nreturn f\"b{self.benchmark_uid}m{model_uid}d{self.data_uid}\"\ndef print_summary(self):\nheaders = [\"model\", \"Execution UID\", \"partial result\", \"from cache\", \"error\"]\ndata_lists_for_display = []\nnum_total = len(self.experiments)\nnum_success_run = 0\nnum_failed = 0\nnum_skipped = 0\nnum_partial_skipped = 0\nnum_partial_run = 0\nfor experiment in self.experiments:\n# populate display data\nif experiment[\"execution\"]:\ndata_lists_for_display.append(\n[\nexperiment[\"model_uid\"],\nexperiment[\"execution\"].id,\nexperiment[\"partial\"],\nexperiment[\"cached\"],\nexperiment[\"error\"],\n]\n)\nelse:\ndata_lists_for_display.append(\n[experiment[\"model_uid\"], \"\", \"\", \"\", experiment[\"error\"]]\n)\n# statistics\nif experiment[\"error\"]:\nnum_failed += 1\nelif experiment[\"cached\"]:\nnum_skipped += 1\nif experiment[\"partial\"]:\nnum_partial_skipped += 1\nelif experiment[\"execution\"]:\nnum_success_run += 1\nif experiment[\"partial\"]:\nnum_partial_run += 1\ntab = tabulate(data_lists_for_display, headers=headers)\nmsg = f\"Total number of models: {num_total}\\n\"\nmsg += f\"\\t{num_skipped} were skipped (already executed), \"\nmsg += f\"of which {num_partial_run} have partial results\\n\"\nmsg += f\"\\t{num_failed} failed\\n\"\nmsg += f\"\\t{num_success_run} ran successfully, \"\nmsg += f\"of which {num_partial_run} have partial results\\n\"\nconfig.ui.print(tab)\nconfig.ui.print(msg)\n</code></pre>"},{"location":"reference/commands/execution/create/#commands.execution.create.BenchmarkExecution.run","title":"<code>run(benchmark_uid, data_uid, models_uids=None, models_input_file=None, ignore_model_errors=False, ignore_failed_experiments=False, no_cache=False, show_summary=False, rerun_finalized_executions=False)</code>  <code>classmethod</code>","text":"<p>Benchmark execution flow. How the following variables affect whether to execute or no: no_cache, rerun_finalized_executions, and execution object state ((0) doesn't exist, (1) exist, (2) executed, (3) finalized):</p> <p>if no_cache is false and rerun_finalized_executions is false, then execute if (0) or (1) if no_cache is true and rerun_finalized_executions is false, then execute if (0) or (1) or(2) if no_cache is true and rerun_finalized_executions is true, then execute anyway</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>UID of the desired benchmark</p> required <code>data_uid</code> <code>str</code> <p>Registered Dataset UID</p> required <code>models_uids</code> <code>List | None</code> <p>list of model UIDs to execute.                     if None, models_input_file will be used</p> <code>None</code> <code>models_input_file</code> <code>Optional[str]</code> <p>filename to read from</p> <code>None</code> Source code in <code>cli/medperf/commands/execution/create.py</code> <pre><code>@classmethod\ndef run(\ncls,\nbenchmark_uid: int,\ndata_uid: int,\nmodels_uids: Optional[List[int]] = None,\nmodels_input_file: Optional[str] = None,\nignore_model_errors=False,\nignore_failed_experiments=False,\nno_cache=False,\nshow_summary=False,\nrerun_finalized_executions=False,\n):\n\"\"\"Benchmark execution flow.\n    How the following variables affect whether to execute or no:\n    no_cache, rerun_finalized_executions, and\n    execution object state ((0) doesn't exist, (1) exist, (2) executed, (3) finalized):\n    if no_cache is false and rerun_finalized_executions is false, then execute if (0) or (1)\n    if no_cache is true and rerun_finalized_executions is false, then execute if (0) or (1) or(2)\n    if no_cache is true and rerun_finalized_executions is true, then execute anyway\n    Args:\n        benchmark_uid (int): UID of the desired benchmark\n        data_uid (str): Registered Dataset UID\n        models_uids (List|None): list of model UIDs to execute.\n                                if None, models_input_file will be used\n        models_input_file: filename to read from\n        if models_uids and models_input_file are None, use all benchmark models\n    \"\"\"\nif rerun_finalized_executions:\nno_cache = True\nexecution_flow = cls(\nbenchmark_uid,\ndata_uid,\nmodels_uids,\nmodels_input_file,\nignore_model_errors,\nignore_failed_experiments,\nno_cache,\nrerun_finalized_executions,\n)\nwith execution_flow.ui.interactive():\nexecution_flow.prepare()\nexecution_flow.validate()\nexecution_flow.prepare_models()\nexecution_flow.load_existing_executions()\nwith execution_flow.ui.interactive():\nexecutions = execution_flow.run_experiments()\nif show_summary:\nexecution_flow.print_summary()\nreturn executions\n</code></pre>"},{"location":"reference/commands/execution/execution/","title":"Execution","text":""},{"location":"reference/commands/execution/execution/#commands.execution.execution.create","title":"<code>create(benchmark_uid=typer.Option(..., '--benchmark', '-b', help='UID of the desired benchmark'), data_uid=typer.Option(..., '--data_uid', '-d', help='Registered Dataset UID'), model_uid=typer.Option(..., '--model_uid', '-m', help='UID of model to execute'), ignore_model_errors=typer.Option(False, '--ignore-model-errors', help='Ignore failing models, allowing for possibly submitting partial results'), no_cache=typer.Option(False, '--no-cache', help='Execute even if results already exist'), new_result=typer.Option(False, '--new-result', help='Works if the result of the execution was already uploaded.This will rerun and create a new record.'))</code>","text":"<p>Runs the benchmark execution step for a given benchmark, prepared dataset and model</p> Source code in <code>cli/medperf/commands/execution/execution.py</code> <pre><code>@app.command(\"create\")\n@clean_except\ndef create(\nbenchmark_uid: int = typer.Option(\n..., \"--benchmark\", \"-b\", help=\"UID of the desired benchmark\"\n),\ndata_uid: int = typer.Option(\n..., \"--data_uid\", \"-d\", help=\"Registered Dataset UID\"\n),\nmodel_uid: int = typer.Option(\n..., \"--model_uid\", \"-m\", help=\"UID of model to execute\"\n),\nignore_model_errors: bool = typer.Option(\nFalse,\n\"--ignore-model-errors\",\nhelp=\"Ignore failing models, allowing for possibly submitting partial results\",\n),\nno_cache: bool = typer.Option(\nFalse,\n\"--no-cache\",\nhelp=\"Execute even if results already exist\",\n),\nnew_result: bool = typer.Option(\nFalse,\n\"--new-result\",\nhelp=(\n\"Works if the result of the execution was already uploaded.\"\n\"This will rerun and create a new record.\"\n),\n),\n):\n\"\"\"Runs the benchmark execution step for a given benchmark, prepared dataset and model\"\"\"\nBenchmarkExecution.run(\nbenchmark_uid,\ndata_uid,\n[model_uid],\nno_cache=no_cache,\nignore_model_errors=ignore_model_errors,\nrerun_finalized_executions=new_result,\n)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/execution/execution/#commands.execution.execution.list","title":"<code>list(unregistered=typer.Option(False, '--unregistered', help='Get unregistered results'), mine=typer.Option(False, '--mine', help='Get current-user results'), benchmark=typer.Option(None, '--benchmark', '-b', help='Get results for a given benchmark'), model=typer.Option(None, '--model', '-m', help='Get results for a given model'), dataset=typer.Option(None, '--dataset', '-d', help='Get reuslts for a given dataset'))</code>","text":"<p>List results</p> Source code in <code>cli/medperf/commands/execution/execution.py</code> <pre><code>@app.command(\"ls\")\n@clean_except\ndef list(\nunregistered: bool = typer.Option(\nFalse, \"--unregistered\", help=\"Get unregistered results\"\n),\nmine: bool = typer.Option(False, \"--mine\", help=\"Get current-user results\"),\nbenchmark: int = typer.Option(\nNone, \"--benchmark\", \"-b\", help=\"Get results for a given benchmark\"\n),\nmodel: int = typer.Option(\nNone, \"--model\", \"-m\", help=\"Get results for a given model\"\n),\ndataset: int = typer.Option(\nNone, \"--dataset\", \"-d\", help=\"Get reuslts for a given dataset\"\n),\n):\n\"\"\"List results\"\"\"\nEntityList.run(\nExecution,\nfields=[\n\"UID\",\n\"Name\",\n\"Benchmark\",\n\"Model\",\n\"Dataset\",\n\"Executed\",\n\"Finalized\",\n],\nunregistered=unregistered,\nmine_only=mine,\nbenchmark=benchmark,\nmodel=model,\ndataset=dataset,\n)\n</code></pre>"},{"location":"reference/commands/execution/execution/#commands.execution.execution.show_local_results","title":"<code>show_local_results(result_id=typer.Argument(..., help='Result ID'), format=typer.Option('yaml', '-f', '--format', help='Format to display contents. Available formats: [yaml, json]'), output=typer.Option(None, '--output', '-o', help='Output file to store contents. If not provided, the output will be displayed'))</code>","text":"<p>Displays the information of one or more results</p> Source code in <code>cli/medperf/commands/execution/execution.py</code> <pre><code>@app.command(\"show_local_results\")\n@clean_except\ndef show_local_results(\nresult_id: int = typer.Argument(..., help=\"Result ID\"),\nformat: str = typer.Option(\n\"yaml\",\n\"-f\",\n\"--format\",\nhelp=\"Format to display contents. Available formats: [yaml, json]\",\n),\noutput: str = typer.Option(\nNone,\n\"--output\",\n\"-o\",\nhelp=\"Output file to store contents. If not provided, the output will be displayed\",\n),\n):\n\"\"\"Displays the information of one or more results\"\"\"\nShowLocalResults.run(result_id, format, output)\n</code></pre>"},{"location":"reference/commands/execution/execution/#commands.execution.execution.submit","title":"<code>submit(result_uid=typer.Option(None, '--result', '-r', help='UID of the result'), benchmark_uid=typer.Option(None, '--benchmark', '-b', help='UID of the desired benchmark'), data_uid=typer.Option(None, '--data_uid', '-d', help='Registered Dataset UID'), model_uid=typer.Option(None, '--model_uid', '-m', help='UID of model to execute'), approval=typer.Option(False, '-y', help='Skip approval step'))</code>","text":"<p>Submits already obtained results to the server</p> Source code in <code>cli/medperf/commands/execution/execution.py</code> <pre><code>@app.command(\"submit\")\n@clean_except\ndef submit(\nresult_uid: int = typer.Option(None, \"--result\", \"-r\", help=\"UID of the result\"),\nbenchmark_uid: int = typer.Option(\nNone, \"--benchmark\", \"-b\", help=\"UID of the desired benchmark\"\n),\ndata_uid: int = typer.Option(\nNone, \"--data_uid\", \"-d\", help=\"Registered Dataset UID\"\n),\nmodel_uid: int = typer.Option(\nNone, \"--model_uid\", \"-m\", help=\"UID of model to execute\"\n),\napproval: bool = typer.Option(False, \"-y\", help=\"Skip approval step\"),\n):\n\"\"\"Submits already obtained results to the server\"\"\"\nResultSubmission.run(\nresult_uid, benchmark_uid, data_uid, model_uid, approved=approval\n)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/execution/execution/#commands.execution.execution.view","title":"<code>view(entity_id=typer.Argument(None, help='Result ID'), format=typer.Option('yaml', '-f', '--format', help='Format to display contents. Available formats: [yaml, json]'), unregistered=typer.Option(False, '--unregistered', help='Display unregistered results if result ID is not provided'), mine=typer.Option(False, '--mine', help='Display current-user results if result ID is not provided'), benchmark=typer.Option(None, '--benchmark', '-b', help='Get results for a given benchmark'), output=typer.Option(None, '--output', '-o', help='Output file to store contents. If not provided, the output will be displayed'))</code>","text":"<p>Displays the information of one or more results</p> Source code in <code>cli/medperf/commands/execution/execution.py</code> <pre><code>@app.command(\"view\")\n@clean_except\ndef view(\nentity_id: Optional[str] = typer.Argument(None, help=\"Result ID\"),\nformat: str = typer.Option(\n\"yaml\",\n\"-f\",\n\"--format\",\nhelp=\"Format to display contents. Available formats: [yaml, json]\",\n),\nunregistered: bool = typer.Option(\nFalse,\n\"--unregistered\",\nhelp=\"Display unregistered results if result ID is not provided\",\n),\nmine: bool = typer.Option(\nFalse,\n\"--mine\",\nhelp=\"Display current-user results if result ID is not provided\",\n),\nbenchmark: int = typer.Option(\nNone, \"--benchmark\", \"-b\", help=\"Get results for a given benchmark\"\n),\noutput: str = typer.Option(\nNone,\n\"--output\",\n\"-o\",\nhelp=\"Output file to store contents. If not provided, the output will be displayed\",\n),\n):\n\"\"\"Displays the information of one or more results\"\"\"\nEntityView.run(\nentity_id, Execution, format, unregistered, mine, output, benchmark=benchmark\n)\n</code></pre>"},{"location":"reference/commands/execution/execution_flow/","title":"Execution flow","text":""},{"location":"reference/commands/execution/execution_flow/#commands.execution.execution_flow.ExecutionFlow","title":"<code>ExecutionFlow</code>","text":"Source code in <code>cli/medperf/commands/execution/execution_flow.py</code> <pre><code>class ExecutionFlow:\n@classmethod\ndef run(\ncls,\ndataset: Dataset,\nmodel: Cube,\nevaluator: Cube,\nexecution: Execution = None,\nignore_model_errors=False,\n):\n\"\"\"Benchmark execution flow.\n        Args:\n            benchmark_uid (int): UID of the desired benchmark\n            data_uid (str): Registered Dataset UID\n            model_uid (int): UID of model to execute\n        \"\"\"\nexecution_flow = cls(dataset, model, evaluator, execution, ignore_model_errors)\nexecution_flow.prepare()\nwith execution_flow.ui.interactive():\nexecution_flow.set_pending_status()\nexecution_flow.run_inference()\nexecution_flow.run_evaluation()\nexecution_summary = execution_flow.todict()\nreturn execution_summary\ndef __init__(\nself,\ndataset: Dataset,\nmodel: Cube,\nevaluator: Cube,\nexecution: Execution = None,\nignore_model_errors=False,\n):\nself.comms = config.comms\nself.ui = config.ui\nself.dataset = dataset\nself.model = model\nself.evaluator = evaluator\nself.execution = execution\nself.ignore_model_errors = ignore_model_errors\ndef prepare(self):\nself.partial = False\nself.preds_path = self.__setup_predictions_path()\nself.model_logs_path, self.metrics_logs_path = self.__setup_logs_path()\nself.results_path = generate_tmp_path()\nself.local_outputs_path = self.__setup_local_outputs_path()\nlogging.debug(f\"tmp results output: {self.results_path}\")\ndef __setup_logs_path(self):\nmodel_uid = self.model.local_id\neval_uid = self.evaluator.local_id\ndata_uid = self.dataset.local_id\nlogs_path = os.path.join(\nconfig.experiments_logs_folder, str(model_uid), str(data_uid)\n)\nos.makedirs(logs_path, exist_ok=True)\nmodel_logs_path = os.path.join(logs_path, \"model.log\")\nmetrics_logs_path = os.path.join(logs_path, f\"metrics_{eval_uid}.log\")\nreturn model_logs_path, metrics_logs_path\ndef __setup_predictions_path(self):\nif self.execution is not None and self.execution.id is not None:\ntimestamp = str(time()).replace(\".\", \"_\")\npreds_path = os.path.join(\nconfig.predictions_folder, str(self.execution.id), timestamp\n)\nelse:\n# for compatibility test execution flows\nmodel_uid = self.model.local_id\ndata_uid = self.dataset.local_id\npreds_path = os.path.join(\nconfig.predictions_folder, str(model_uid), str(data_uid)\n)\nremove_path(preds_path)  # clear it\nreturn preds_path\ndef __setup_local_outputs_path(self):\nif self.execution is not None and self.execution.id is not None:\nlocal_outputs_path = self.execution.local_outputs_path\nelse:\n# Non-persistent for compatibility test execution flows\n# TODO: make this better\nlocal_outputs_path = generate_tmp_path()\nreturn local_outputs_path\ndef set_pending_status(self):\nself.__send_model_report(\"pending\")\nself.__send_evaluator_report(\"pending\")\ndef run_inference(self):\nself.ui.text = f\"Running inference of model '{self.model.name}' on dataset\"\ninfer_timeout = config.infer_timeout\ninference_mounts = {\n\"data_path\": self.dataset.data_path,\n\"output_path\": self.preds_path,\n}\nself.__send_model_report(\"started\")\ntry:\nself.model.run(\ntask=\"infer\",\noutput_logs=self.model_logs_path,\ntimeout=infer_timeout,\nmounts=inference_mounts,\n)\nself.ui.print(\"&gt; Model execution complete\")\nexcept ExecutionError as e:\nself.__send_model_report(\"failed\")\nif not self.ignore_model_errors:\nlogging.error(f\"Model Execution failed: {e}\")\nraise ExecutionError(f\"Model Execution failed: {e}\")\nelse:\nself.partial = True\nlogging.warning(f\"Model Execution failed: {e}\")\nreturn\nexcept KeyboardInterrupt:\nlogging.warning(\"Model Execution interrupted by user\")\nself.__send_model_report(\"interrupted\")\nraise CleanExit(\"Model Execution interrupted by user\")\nself.__send_model_report(\"finished\")\ndef run_evaluation(self):\nself.ui.text = f\"Calculating metrics for model '{self.model.name}' predictions\"\nevaluate_timeout = config.evaluate_timeout\nevaluator_mounts = {\n\"predictions\": self.preds_path,\n\"labels\": self.dataset.labels_path,\n\"output_path\": self.results_path,\n\"local_outputs_path\": self.local_outputs_path,\n}\nself.__send_evaluator_report(\"started\")\ntry:\nself.evaluator.run(\ntask=\"evaluate\",\noutput_logs=self.metrics_logs_path,\ntimeout=evaluate_timeout,\nmounts=evaluator_mounts,\n)\nexcept ExecutionError as e:\nlogging.error(f\"Metrics calculation failed: {e}\")\nself.__send_evaluator_report(\"failed\")\nraise ExecutionError(f\"Metrics calculation failed: {e}\")\nexcept KeyboardInterrupt:\nlogging.warning(\"Metrics calculation interrupted by user\")\nself.__send_evaluator_report(\"interrupted\")\nraise CleanExit(\"Metrics calculation interrupted by user\")\nself.__send_evaluator_report(\"finished\")\ndef todict(self):\nreturn {\n\"results\": self.get_results(),\n\"partial\": self.partial,\n}\ndef get_results(self):\nif not os.path.exists(self.results_path):\nraise ExecutionError(\"Results file does not exist\")\nwith open(self.results_path, \"r\") as f:\nresults = yaml.safe_load(f)\nif results is None:\nraise ExecutionError(\"Results file is empty\")\nreturn results\ndef __send_model_report(self, status: str):\nself.__send_report(\"model_report\", status)\ndef __send_evaluator_report(self, status: str):\nself.__send_report(\"evaluation_report\", status)\ndef __send_report(self, field: str, status: str):\nif self.execution is None or self.execution.id is None:\nreturn\nexecution_id = self.execution.id\nbody = {field: {\"execution_status\": status}}\ntry:\nconfig.comms.update_execution(execution_id, body)\nexcept CommunicationError as e:\nlogging.error(str(e))\nreturn\n</code></pre>"},{"location":"reference/commands/execution/execution_flow/#commands.execution.execution_flow.ExecutionFlow.run","title":"<code>run(dataset, model, evaluator, execution=None, ignore_model_errors=False)</code>  <code>classmethod</code>","text":"<p>Benchmark execution flow.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>UID of the desired benchmark</p> required <code>data_uid</code> <code>str</code> <p>Registered Dataset UID</p> required <code>model_uid</code> <code>int</code> <p>UID of model to execute</p> required Source code in <code>cli/medperf/commands/execution/execution_flow.py</code> <pre><code>@classmethod\ndef run(\ncls,\ndataset: Dataset,\nmodel: Cube,\nevaluator: Cube,\nexecution: Execution = None,\nignore_model_errors=False,\n):\n\"\"\"Benchmark execution flow.\n    Args:\n        benchmark_uid (int): UID of the desired benchmark\n        data_uid (str): Registered Dataset UID\n        model_uid (int): UID of model to execute\n    \"\"\"\nexecution_flow = cls(dataset, model, evaluator, execution, ignore_model_errors)\nexecution_flow.prepare()\nwith execution_flow.ui.interactive():\nexecution_flow.set_pending_status()\nexecution_flow.run_inference()\nexecution_flow.run_evaluation()\nexecution_summary = execution_flow.todict()\nreturn execution_summary\n</code></pre>"},{"location":"reference/commands/execution/show_local_results/","title":"Show local results","text":""},{"location":"reference/commands/execution/submit/","title":"Submit","text":""},{"location":"reference/commands/execution/utils/","title":"Utils","text":""},{"location":"reference/commands/execution/utils/#commands.execution.utils.filter_latest_executions","title":"<code>filter_latest_executions(executions)</code>","text":"<p>Given a list of executions, this function retrieves a list containing the latest executions of each model-dataset-benchmark triplet.</p> <p>Parameters:</p> Name Type Description Default <code>executions</code> <code>list[dict]</code> <p>the list of executions</p> required <p>Returns:</p> Type Description <code>List[Execution]</code> <p>list[dict]: the list containing the latest executions of each         model-dataset-benchmark triplet.</p> Source code in <code>cli/medperf/commands/execution/utils.py</code> <pre><code>def filter_latest_executions(executions: List[Execution]) -&gt; List[Execution]:\n\"\"\"Given a list of executions, this function\n    retrieves a list containing the latest executions of each\n    model-dataset-benchmark triplet.\n    Args:\n        executions (list[dict]): the list of executions\n    Returns:\n        list[dict]: the list containing the latest executions of each\n                    model-dataset-benchmark triplet.\n    \"\"\"\nexecutions.sort(key=lambda exec: parse_datetime(exec.created_at))\nlatest_executions = {}\nfor exec in executions:\nmodel = exec.model\ndataset = exec.dataset\nbenchmark = exec.benchmark\nlatest_executions[(model, dataset, benchmark)] = exec\nreturn list(latest_executions.values())\n</code></pre>"},{"location":"reference/commands/mlcube/associate/","title":"Associate","text":""},{"location":"reference/commands/mlcube/associate/#commands.mlcube.associate.AssociateCube","title":"<code>AssociateCube</code>","text":"Source code in <code>cli/medperf/commands/mlcube/associate.py</code> <pre><code>class AssociateCube:\n@classmethod\ndef run(\ncls,\ncube_uid: int,\nbenchmark_uid: int,\napproved=False,\nno_cache=False,\n):\n\"\"\"Associates a cube with a given benchmark\n        Args:\n            cube_uid (int): UID of model MLCube\n            benchmark_uid (int): UID of benchmark\n            approved (bool): Skip validation step. Defualts to False\n        \"\"\"\ncomms = config.comms\nui = config.ui\ncube = Cube.get(cube_uid)\nbenchmark = Benchmark.get(benchmark_uid)\n_, results = CompatibilityTestExecution.run(\nbenchmark=benchmark_uid, model=cube_uid, no_cache=no_cache\n)\nui.print(\"These are the results generated by the compatibility test. \")\nui.print(\"This will be sent along the association request.\")\nui.print(\"They will not be part of the benchmark.\")\ndict_pretty_print(results)\nmsg = \"Please confirm that you would like to associate \"\nmsg += f\"the model '{cube.name}' with the benchmark '{benchmark.name}' [Y/n]\"\napproved = approved or approval_prompt(msg)\nif approved:\nui.print(\"Generating model benchmark association\")\nmetadata = {\"test_result\": results}\ncomms.associate_benchmark_model(cube_uid, benchmark_uid, metadata)\nelse:\nraise CleanExit(\"Model association operation cancelled\")\n</code></pre>"},{"location":"reference/commands/mlcube/associate/#commands.mlcube.associate.AssociateCube.run","title":"<code>run(cube_uid, benchmark_uid, approved=False, no_cache=False)</code>  <code>classmethod</code>","text":"<p>Associates a cube with a given benchmark</p> <p>Parameters:</p> Name Type Description Default <code>cube_uid</code> <code>int</code> <p>UID of model MLCube</p> required <code>benchmark_uid</code> <code>int</code> <p>UID of benchmark</p> required <code>approved</code> <code>bool</code> <p>Skip validation step. Defualts to False</p> <code>False</code> Source code in <code>cli/medperf/commands/mlcube/associate.py</code> <pre><code>@classmethod\ndef run(\ncls,\ncube_uid: int,\nbenchmark_uid: int,\napproved=False,\nno_cache=False,\n):\n\"\"\"Associates a cube with a given benchmark\n    Args:\n        cube_uid (int): UID of model MLCube\n        benchmark_uid (int): UID of benchmark\n        approved (bool): Skip validation step. Defualts to False\n    \"\"\"\ncomms = config.comms\nui = config.ui\ncube = Cube.get(cube_uid)\nbenchmark = Benchmark.get(benchmark_uid)\n_, results = CompatibilityTestExecution.run(\nbenchmark=benchmark_uid, model=cube_uid, no_cache=no_cache\n)\nui.print(\"These are the results generated by the compatibility test. \")\nui.print(\"This will be sent along the association request.\")\nui.print(\"They will not be part of the benchmark.\")\ndict_pretty_print(results)\nmsg = \"Please confirm that you would like to associate \"\nmsg += f\"the model '{cube.name}' with the benchmark '{benchmark.name}' [Y/n]\"\napproved = approved or approval_prompt(msg)\nif approved:\nui.print(\"Generating model benchmark association\")\nmetadata = {\"test_result\": results}\ncomms.associate_benchmark_model(cube_uid, benchmark_uid, metadata)\nelse:\nraise CleanExit(\"Model association operation cancelled\")\n</code></pre>"},{"location":"reference/commands/mlcube/create/","title":"Create","text":""},{"location":"reference/commands/mlcube/create/#commands.mlcube.create.CreateCube","title":"<code>CreateCube</code>","text":"Source code in <code>cli/medperf/commands/mlcube/create.py</code> <pre><code>class CreateCube:\n@classmethod\ndef run(\ncls,\ntemplate_name: str,\nimage_name: str,\nfolder_name: str,\noutput_path: str = \".\",\n):\n\"\"\"Creates a new MLCube based on one of the provided templates\n        Args:\n            template_name (str): The name of the template to use\n            output_path (str, Optional): The desired path for the MLCube. Defaults to current path.\n            config_file (str, Optional): Path to a JSON configuration file. If not passed, user is prompted.\n        \"\"\"\ntemplate_dirs = config.templates\nif template_name not in template_dirs:\ntemplates = list(template_dirs.keys())\nraise InvalidArgumentError(\nf\"Invalid type. Available types: [{' | '.join(templates)}]\"\n)\n# Get package parent path\npath = abspath(Path(__file__).parent.parent.parent)\ntemplate_dir = template_dirs[template_name]\ncookiecutter(\npath,\ndirectory=template_dir,\noutput_dir=output_path,\nextra_context={\"image_name\": image_name, \"project_slug\": folder_name},\nno_input=True,\n)\n</code></pre>"},{"location":"reference/commands/mlcube/create/#commands.mlcube.create.CreateCube.run","title":"<code>run(template_name, image_name, folder_name, output_path='.')</code>  <code>classmethod</code>","text":"<p>Creates a new MLCube based on one of the provided templates</p> <p>Parameters:</p> Name Type Description Default <code>template_name</code> <code>str</code> <p>The name of the template to use</p> required <code>output_path</code> <code>(str, Optional)</code> <p>The desired path for the MLCube. Defaults to current path.</p> <code>'.'</code> <code>config_file</code> <code>(str, Optional)</code> <p>Path to a JSON configuration file. If not passed, user is prompted.</p> required Source code in <code>cli/medperf/commands/mlcube/create.py</code> <pre><code>@classmethod\ndef run(\ncls,\ntemplate_name: str,\nimage_name: str,\nfolder_name: str,\noutput_path: str = \".\",\n):\n\"\"\"Creates a new MLCube based on one of the provided templates\n    Args:\n        template_name (str): The name of the template to use\n        output_path (str, Optional): The desired path for the MLCube. Defaults to current path.\n        config_file (str, Optional): Path to a JSON configuration file. If not passed, user is prompted.\n    \"\"\"\ntemplate_dirs = config.templates\nif template_name not in template_dirs:\ntemplates = list(template_dirs.keys())\nraise InvalidArgumentError(\nf\"Invalid type. Available types: [{' | '.join(templates)}]\"\n)\n# Get package parent path\npath = abspath(Path(__file__).parent.parent.parent)\ntemplate_dir = template_dirs[template_name]\ncookiecutter(\npath,\ndirectory=template_dir,\noutput_dir=output_path,\nextra_context={\"image_name\": image_name, \"project_slug\": folder_name},\nno_input=True,\n)\n</code></pre>"},{"location":"reference/commands/mlcube/mlcube/","title":"Mlcube","text":""},{"location":"reference/commands/mlcube/mlcube/#commands.mlcube.mlcube.associate","title":"<code>associate(benchmark_uid=typer.Option(..., '--benchmark', '-b', help='Benchmark UID'), model_uid=typer.Option(..., '--model_uid', '-m', help='Model UID'), approval=typer.Option(False, '-y', help='Skip approval step'), no_cache=typer.Option(False, '--no-cache', help='Execute the test even if results already exist'))</code>","text":"<p>Associates a model to a benchmark</p> Source code in <code>cli/medperf/commands/mlcube/mlcube.py</code> <pre><code>@app.command(\"associate\")\n@clean_except\ndef associate(\nbenchmark_uid: int = typer.Option(..., \"--benchmark\", \"-b\", help=\"Benchmark UID\"),\nmodel_uid: int = typer.Option(..., \"--model_uid\", \"-m\", help=\"Model UID\"),\napproval: bool = typer.Option(False, \"-y\", help=\"Skip approval step\"),\nno_cache: bool = typer.Option(\nFalse,\n\"--no-cache\",\nhelp=\"Execute the test even if results already exist\",\n),\n):\n\"\"\"Associates a model to a benchmark\"\"\"\nAssociateCube.run(model_uid, benchmark_uid, approved=approval, no_cache=no_cache)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/mlcube/mlcube/#commands.mlcube.mlcube.create","title":"<code>create(template=typer.Argument(..., help=f'Container type. Available types: [{' | '.join(config.templates.keys())}]'), image_name=typer.Option(..., '--image', '-i', help='Image name'), folder_name=typer.Option(..., '--folder_name', '-f', help='Folder name of the container files template to be created'), output_path=typer.Option('.', '--output', '-o', help='Save the generated template to the specified path'))</code>","text":"<p>Creates a container files template</p> Source code in <code>cli/medperf/commands/mlcube/mlcube.py</code> <pre><code>@app.command(\"create\")\n@clean_except\ndef create(\ntemplate: str = typer.Argument(\n...,\nhelp=f\"Container type. Available types: [{' | '.join(config.templates.keys())}]\",\n),\nimage_name: str = typer.Option(\n...,\n\"--image\",\n\"-i\",\nhelp=\"Image name\",\n),\nfolder_name: str = typer.Option(\n...,\n\"--folder_name\",\n\"-f\",\nhelp=\"Folder name of the container files template to be created\",\n),\noutput_path: str = typer.Option(\n\".\", \"--output\", \"-o\", help=\"Save the generated template to the specified path\"\n),\n):\n\"\"\"Creates a container files template\"\"\"\nCreateCube.run(template, image_name, folder_name, output_path)\n</code></pre>"},{"location":"reference/commands/mlcube/mlcube/#commands.mlcube.mlcube.list","title":"<code>list(unregistered=typer.Option(False, '--unregistered', help='Get unregistered containers'), mine=typer.Option(False, '--mine', help='Get current-user containers'), name=typer.Option(None, '--name', '-n', help='Filter out by container Name'), owner=typer.Option(None, '--owner', help='Filter by owner ID'), state=typer.Option(None, '--state', help='Filter by state (DEVELOPMENT/OPERATION)'), is_active=typer.Option(None, '--active/--inactive', help='Filter by active status'))</code>","text":"<p>List containers</p> Source code in <code>cli/medperf/commands/mlcube/mlcube.py</code> <pre><code>@app.command(\"ls\")\n@clean_except\ndef list(\nunregistered: bool = typer.Option(\nFalse, \"--unregistered\", help=\"Get unregistered containers\"\n),\nmine: bool = typer.Option(False, \"--mine\", help=\"Get current-user containers\"),\nname: str = typer.Option(None, \"--name\", \"-n\", help=\"Filter out by container Name\"),\nowner: int = typer.Option(None, \"--owner\", help=\"Filter by owner ID\"),\nstate: str = typer.Option(\nNone, \"--state\", help=\"Filter by state (DEVELOPMENT/OPERATION)\"\n),\nis_active: bool = typer.Option(\nNone, \"--active/--inactive\", help=\"Filter by active status\"\n),\n):\n\"\"\"List containers\"\"\"\nEntityList.run(\nCube,\nfields=[\"UID\", \"Name\", \"State\", \"Registered\"],\nunregistered=unregistered,\nmine_only=mine,\nname=name,\nowner=owner,\nstate=state,\nis_active=is_active,\n)\n</code></pre>"},{"location":"reference/commands/mlcube/mlcube/#commands.mlcube.mlcube.run_test","title":"<code>run_test(mlcube_path=typer.Option(..., '--container', '-m', help='path to container config'), task=typer.Option(..., '--task', '-t', help='container task to run'), parameters_file_path=typer.Option(None, '--parameters_file_path', help='path to container parameters file'), additional_files_path=typer.Option(None, '--additional_files_path', help='path to ciontainer additional files'), output_logs=typer.Option(None, '--output_logs', '-o', help='where to store stdout'), timeout=typer.Option(None, '--timeout', help='comma separated list of key=value pairs'), mounts=typer.Option('', '--mounts', '-m', help='comma separated list of key=value pairs'), env=typer.Option('', '--env', '-e', help='comma separated list of key=value pairs'), ports=typer.Option('', '--ports', '-P', help='comma separated list of ports to expose'), allow_network=typer.Option(False, '--allow_network', help='comma separated list of key=value pairs'), download=typer.Option(False, '--download', help='whether to pull docker image'))</code>","text":"<p>Runs a container for testing only</p> Source code in <code>cli/medperf/commands/mlcube/mlcube.py</code> <pre><code>@app.command(\"run_test\")\n@clean_except\ndef run_test(\nmlcube_path: str = typer.Option(\n..., \"--container\", \"-m\", help=\"path to container config\"\n),\ntask: str = typer.Option(..., \"--task\", \"-t\", help=\"container task to run\"),\nparameters_file_path: str = typer.Option(\nNone, \"--parameters_file_path\", help=\"path to container parameters file\"\n),\nadditional_files_path: str = typer.Option(\nNone, \"--additional_files_path\", help=\"path to ciontainer additional files\"\n),\noutput_logs: str = typer.Option(\nNone, \"--output_logs\", \"-o\", help=\"where to store stdout\"\n),\ntimeout: int = typer.Option(\nNone, \"--timeout\", help=\"comma separated list of key=value pairs\"\n),\nmounts: str = typer.Option(\n\"\", \"--mounts\", \"-m\", help=\"comma separated list of key=value pairs\"\n),\nenv: str = typer.Option(\n\"\", \"--env\", \"-e\", help=\"comma separated list of key=value pairs\"\n),\nports: str = typer.Option(\n\"\", \"--ports\", \"-P\", help=\"comma separated list of ports to expose\"\n),\nallow_network: bool = typer.Option(\nFalse, \"--allow_network\", help=\"comma separated list of key=value pairs\"\n),\ndownload: int = typer.Option(\nFalse, \"--download\", help=\"whether to pull docker image\"\n),\n):\n\"\"\"Runs a container for testing only\"\"\"\nmounts = dict([p.split(\"=\") for p in mounts.strip().strip(\",\").split(\",\") if p])\nenv = dict([p.split(\"=\") for p in env.strip().strip(\",\").split(\",\") if p])\nports = [p for p in ports.split(\",\") if p]\nrun_mlcube(\nmlcube_path,\ntask,\nparameters_file_path,\nadditional_files_path,\noutput_logs,\ntimeout,\nmounts,\nenv,\nports,\nnot allow_network,\ndownload,\n)\n</code></pre>"},{"location":"reference/commands/mlcube/mlcube/#commands.mlcube.mlcube.submit","title":"<code>submit(name=typer.Option(..., '--name', '-n', help='Name of the container'), mlcube_file=typer.Option(..., '--container-config-file', '-m', help='Identifier to download the container config file. See the description above'), mlcube_hash=typer.Option('', '--container-config-hash', help='hash of container config file'), parameters_file=typer.Option('', '--parameters-file', '-p', help='Identifier to download the parameters file. See the description above'), parameters_hash=typer.Option('', '--parameters-hash', help='hash of parameters file'), additional_file=typer.Option('', '--additional-file', '-a', help='Identifier to download the additional files tarball. See the description above'), additional_hash=typer.Option('', '--additional-hash', help='hash of additional file'), image_hash=typer.Option('', '--image-hash', help='hash of image file'), operational=typer.Option(False, '--operational', help='Submit the container as OPERATIONAL'))</code>","text":"<p>Submits a new container to the platform.</p> The following assets <ul> <li> <p>container config file</p> </li> <li> <p>parameters_file</p> </li> <li> <p>additional_file</p> </li> <li> <p>image_file</p> </li> </ul> <p>are expected to be given in the following format:  where <code>source_prefix</code> instructs the client how to download the resource, and <code>resource_identifier</code> is the identifier used to download the asset. The following are supported: <ol> <li> <p>A direct link: \"direct:\" <li> <p>An asset hosted on the Synapse platform: \"synapse:\" <p>If a URL is given without a source prefix, it will be treated as a direct download link.</p> Source code in <code>cli/medperf/commands/mlcube/mlcube.py</code> <pre><code>@app.command(\"submit\")\n@clean_except\ndef submit(\nname: str = typer.Option(..., \"--name\", \"-n\", help=\"Name of the container\"),\nmlcube_file: str = typer.Option(\n...,\n\"--container-config-file\",\n\"-m\",\nhelp=\"Identifier to download the container config file. See the description above\",\n),\nmlcube_hash: str = typer.Option(\n\"\", \"--container-config-hash\", help=\"hash of container config file\"\n),\nparameters_file: str = typer.Option(\n\"\",\n\"--parameters-file\",\n\"-p\",\nhelp=\"Identifier to download the parameters file. See the description above\",\n),\nparameters_hash: str = typer.Option(\n\"\", \"--parameters-hash\", help=\"hash of parameters file\"\n),\nadditional_file: str = typer.Option(\n\"\",\n\"--additional-file\",\n\"-a\",\nhelp=\"Identifier to download the additional files tarball. See the description above\",\n),\nadditional_hash: str = typer.Option(\n\"\", \"--additional-hash\", help=\"hash of additional file\"\n),\nimage_hash: str = typer.Option(\"\", \"--image-hash\", help=\"hash of image file\"),\noperational: bool = typer.Option(\nFalse,\n\"--operational\",\nhelp=\"Submit the container as OPERATIONAL\",\n),\n):\n\"\"\"Submits a new container to the platform.\\n\n    The following assets:\\n\n        - container config file\\n\n        - parameters_file\\n\n        - additional_file\\n\n        - image_file\\n\n    are expected to be given in the following format: &lt;source_prefix:resource_identifier&gt;\n    where `source_prefix` instructs the client how to download the resource, and `resource_identifier`\n    is the identifier used to download the asset. The following are supported:\\n\n    1. A direct link: \"direct:&lt;URL&gt;\"\\n\n    2. An asset hosted on the Synapse platform: \"synapse:&lt;synapse ID&gt;\"\\n\\n\n    If a URL is given without a source prefix, it will be treated as a direct download link.\n    \"\"\"\nmlcube_info = {\n\"name\": name,\n\"git_mlcube_url\": mlcube_file,\n\"git_mlcube_hash\": mlcube_hash,\n\"git_parameters_url\": parameters_file,\n\"parameters_hash\": parameters_hash,\n\"image_hash\": image_hash,\n\"additional_files_tarball_url\": additional_file,\n\"additional_files_tarball_hash\": additional_hash,\n\"state\": \"OPERATION\" if operational else \"DEVELOPMENT\",\n}\nSubmitCube.run(mlcube_info)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/mlcube/mlcube/#commands.mlcube.mlcube.view","title":"<code>view(entity_id=typer.Argument(None, help='Container ID'), format=typer.Option('yaml', '-f', '--format', help='Format to display contents. Available formats: [yaml, json]'), unregistered=typer.Option(False, '--unregistered', help='Display unregistered containers if container ID is not provided'), mine=typer.Option(False, '--mine', help='Display current-user containers if container ID is not provided'), output=typer.Option(None, '--output', '-o', help='Output file to store contents. If not provided, the output will be displayed'))</code>","text":"<p>Displays the information of one or more containers</p> Source code in <code>cli/medperf/commands/mlcube/mlcube.py</code> <pre><code>@app.command(\"view\")\n@clean_except\ndef view(\nentity_id: Optional[int] = typer.Argument(None, help=\"Container ID\"),\nformat: str = typer.Option(\n\"yaml\",\n\"-f\",\n\"--format\",\nhelp=\"Format to display contents. Available formats: [yaml, json]\",\n),\nunregistered: bool = typer.Option(\nFalse,\n\"--unregistered\",\nhelp=\"Display unregistered containers if container ID is not provided\",\n),\nmine: bool = typer.Option(\nFalse,\n\"--mine\",\nhelp=\"Display current-user containers if container ID is not provided\",\n),\noutput: str = typer.Option(\nNone,\n\"--output\",\n\"-o\",\nhelp=\"Output file to store contents. If not provided, the output will be displayed\",\n),\n):\n\"\"\"Displays the information of one or more containers\"\"\"\nEntityView.run(entity_id, Cube, format, unregistered, mine, output)\n</code></pre>"},{"location":"reference/commands/mlcube/run_test/","title":"Run test","text":""},{"location":"reference/commands/mlcube/run_test/#commands.mlcube.run_test.run_mlcube","title":"<code>run_mlcube(cube_path, task, parameters_file_path=None, additional_files_path=None, output_logs=None, timeout=None, mounts={}, env={}, ports=[], disable_network=True, download=False)</code>","text":"<p>Dev utility command</p> Source code in <code>cli/medperf/commands/mlcube/run_test.py</code> <pre><code>def run_mlcube(\ncube_path: str,\ntask: str,\nparameters_file_path: str = None,\nadditional_files_path: str = None,\noutput_logs: str = None,\ntimeout: int = None,\nmounts: dict = {},\nenv: dict = {},\nports: list = [],\ndisable_network: bool = True,\ndownload: bool = False,\n):\n\"\"\"Dev utility command\"\"\"\nc = Cube(name=\"test\", for_test=True, git_mlcube_url=\"https://example.com\")\nc.cube_path = cube_path\nc.params_path = parameters_file_path or c.params_path\nc.additiona_files_folder_path = (\nadditional_files_path or c.additiona_files_folder_path\n)\nif download:\nc.download_run_files()\nc.run(task, output_logs, timeout, mounts, env, ports, disable_network)\n</code></pre>"},{"location":"reference/commands/mlcube/submit/","title":"Submit","text":""},{"location":"reference/commands/mlcube/submit/#commands.mlcube.submit.SubmitCube","title":"<code>SubmitCube</code>","text":"Source code in <code>cli/medperf/commands/mlcube/submit.py</code> <pre><code>class SubmitCube:\n@classmethod\ndef run(cls, submit_info: dict):\n\"\"\"Submits a new cube to the medperf platform\n        Args:\n            submit_info (dict): Dictionary containing the cube information.\n        \"\"\"\nui = config.ui\nsubmission = cls(submit_info)\nwith ui.interactive():\nui.text = \"Validating Container can be downloaded\"\nsubmission.download()\nui.text = \"Submitting Container to MedPerf\"\nupdated_cube_dict = submission.upload()\nsubmission.to_permanent_path(updated_cube_dict)\nsubmission.write(updated_cube_dict)\nreturn submission.cube.id\ndef __init__(self, submit_info: dict):\nself.comms = config.comms\nself.ui = config.ui\nself.cube = Cube(**submit_info)\nconfig.tmp_paths.append(self.cube.path)\ndef download(self):\nself.cube.download_config_files()\nself.cube.download_run_files()\ndef upload(self):\nupdated_body = self.cube.upload()\nreturn updated_body\ndef to_permanent_path(self, cube_dict):\n\"\"\"Renames the temporary cube submission to a permanent one using the uid of\n        the registered cube\n        \"\"\"\nold_cube_loc = self.cube.path\nupdated_cube = Cube(**cube_dict)\nnew_cube_loc = updated_cube.path\nremove_path(new_cube_loc)\nos.rename(old_cube_loc, new_cube_loc)\ndef write(self, updated_cube_dict):\nself.cube = Cube(**updated_cube_dict)\nself.cube.write()\n</code></pre>"},{"location":"reference/commands/mlcube/submit/#commands.mlcube.submit.SubmitCube.run","title":"<code>run(submit_info)</code>  <code>classmethod</code>","text":"<p>Submits a new cube to the medperf platform</p> <p>Parameters:</p> Name Type Description Default <code>submit_info</code> <code>dict</code> <p>Dictionary containing the cube information.</p> required Source code in <code>cli/medperf/commands/mlcube/submit.py</code> <pre><code>@classmethod\ndef run(cls, submit_info: dict):\n\"\"\"Submits a new cube to the medperf platform\n    Args:\n        submit_info (dict): Dictionary containing the cube information.\n    \"\"\"\nui = config.ui\nsubmission = cls(submit_info)\nwith ui.interactive():\nui.text = \"Validating Container can be downloaded\"\nsubmission.download()\nui.text = \"Submitting Container to MedPerf\"\nupdated_cube_dict = submission.upload()\nsubmission.to_permanent_path(updated_cube_dict)\nsubmission.write(updated_cube_dict)\nreturn submission.cube.id\n</code></pre>"},{"location":"reference/commands/mlcube/submit/#commands.mlcube.submit.SubmitCube.to_permanent_path","title":"<code>to_permanent_path(cube_dict)</code>","text":"<p>Renames the temporary cube submission to a permanent one using the uid of the registered cube</p> Source code in <code>cli/medperf/commands/mlcube/submit.py</code> <pre><code>def to_permanent_path(self, cube_dict):\n\"\"\"Renames the temporary cube submission to a permanent one using the uid of\n    the registered cube\n    \"\"\"\nold_cube_loc = self.cube.path\nupdated_cube = Cube(**cube_dict)\nnew_cube_loc = updated_cube.path\nremove_path(new_cube_loc)\nos.rename(old_cube_loc, new_cube_loc)\n</code></pre>"},{"location":"reference/commands/training/close_event/","title":"Close event","text":""},{"location":"reference/commands/training/close_event/#commands.training.close_event.CloseEvent","title":"<code>CloseEvent</code>","text":"<p>Used for both event cancellation (with custom report path) and for event closing (with the expected report path generated by the aggregator)</p> Source code in <code>cli/medperf/commands/training/close_event.py</code> <pre><code>class CloseEvent:\n\"\"\"Used for both event cancellation (with custom report path) and for event closing\n    (with the expected report path generated by the aggregator)\"\"\"\n@classmethod\ndef run(cls, training_exp_id: int, report_path: str = None, approval: bool = False):\nsubmission = cls(training_exp_id, report_path, approval)\nsubmission.prepare()\nsubmission.validate()\nsubmission.read_report()\nsubmission.submit()\nsubmission.write()\ndef __init__(self, training_exp_id: int, report_path: str, approval: bool):\nself.training_exp_id = training_exp_id\nself.approved = approval\nself.report_path = report_path\ndef prepare(self):\nself.training_exp = TrainingExp.get(self.training_exp_id)\nself.event = TrainingEvent.from_experiment(self.training_exp_id)\nself.report_path = self.report_path or self.event.get_latest_report_path()\ndef validate(self):\nif self.event.finished:\nraise InvalidArgumentError(\"This experiment has already finished\")\nif not os.path.exists(self.report_path):\nraise InvalidArgumentError(f\"Report {self.report_path} does not exist.\")\ndef read_report(self):\nwith open(self.report_path) as f:\nself.report = yaml.safe_load(f)\ndef submit(self):\nself.event.report = self.report\nbody = {\"finished\": True, \"report\": self.report}\ndict_pretty_print(self.report)\nmsg = (\nf\"You are about to close the event of training experiment {self.training_exp.name}.\"\n\" This will be the submitted report. Do you confirm? [Y/n] \"\n)\nself.approved = self.approved or approval_prompt(msg)\nif self.approved:\nconfig.comms.update_training_event(self.event.id, body)\nreturn\nraise CleanExit(\"Event closing cancelled\")\ndef write(self):\nself.event.write()\n</code></pre>"},{"location":"reference/commands/training/get_experiment_status/","title":"Get experiment status","text":""},{"location":"reference/commands/training/get_experiment_status/#commands.training.get_experiment_status.GetExperimentStatus","title":"<code>GetExperimentStatus</code>","text":"Source code in <code>cli/medperf/commands/training/get_experiment_status.py</code> <pre><code>class GetExperimentStatus:\n@classmethod\ndef run(cls, training_exp_id: int, silent: bool = False):\n\"\"\"Starts the aggregation server of a training experiment\n        Args:\n            training_exp_id (int): Training experiment UID.\n        \"\"\"\nexecution = cls(training_exp_id)\nexecution.prepare()\nexecution.prepare_plan()\nexecution.prepare_pki_assets()\nwith config.ui.interactive():\nexecution.prepare_admin_cube()\nexecution.get_experiment_status()\nif not silent:\nexecution.print_experiment_status()\nexecution.store_status()\ndef __init__(self, training_exp_id: int) -&gt; None:\nself.training_exp_id = training_exp_id\nself.ui = config.ui\ndef prepare(self):\nself.training_exp = TrainingExp.get(self.training_exp_id)\nself.ui.print(f\"Training Experiment: {self.training_exp.name}\")\nself.user_email: str = get_medperf_user_data()[\"email\"]\nself.status_output = generate_tmp_path()\nself.temp_dir = generate_tmp_path()\ndef prepare_plan(self):\nself.training_exp.prepare_plan()\ndef prepare_pki_assets(self):\nca = CA.from_experiment(self.training_exp_id)\ntrust(ca)\nself.admin_pki_assets = get_pki_assets_path(self.user_email, ca.name)\nself.ca = ca\ndef prepare_admin_cube(self):\nself.cube = self.__get_cube(self.training_exp.fl_admin_mlcube, \"FL Admin\")\ndef __get_cube(self, uid: int, name: str) -&gt; Cube:\nself.ui.text = (\n\"Retrieving and setting up training Container. This may take some time.\"\n)\ncube = Cube.get(uid)\ncube.download_run_files()\nself.ui.print(f\"&gt; Container '{name}' download complete\")\nreturn cube\ndef get_experiment_status(self):\nenv = {\"MEDPERF_ADMIN_PARTICIPANT_CN\": self.user_email}\nmounts = {\n\"node_cert_folder\": self.admin_pki_assets,\n\"ca_cert_folder\": self.ca.pki_assets,\n\"plan_path\": self.training_exp.plan_path,\n\"output_status_file\": self.status_output,\n\"temp_dir\": self.temp_dir,\n}\nself.ui.text = \"Getting training experiment status\"\nself.cube.run(\ntask=\"get_experiment_status\", mounts=mounts, env=env, disable_network=False\n)\ndef print_experiment_status(self):\nwith open(self.status_output) as f:\ncontents = yaml.safe_load(f)\ndict_pretty_print(contents, skip_none_values=False)\ndef store_status(self):\nnew_status_path = self.training_exp.status_path\nremove_path(new_status_path)\nos.rename(self.status_output, new_status_path)\n</code></pre>"},{"location":"reference/commands/training/get_experiment_status/#commands.training.get_experiment_status.GetExperimentStatus.run","title":"<code>run(training_exp_id, silent=False)</code>  <code>classmethod</code>","text":"<p>Starts the aggregation server of a training experiment</p> <p>Parameters:</p> Name Type Description Default <code>training_exp_id</code> <code>int</code> <p>Training experiment UID.</p> required Source code in <code>cli/medperf/commands/training/get_experiment_status.py</code> <pre><code>@classmethod\ndef run(cls, training_exp_id: int, silent: bool = False):\n\"\"\"Starts the aggregation server of a training experiment\n    Args:\n        training_exp_id (int): Training experiment UID.\n    \"\"\"\nexecution = cls(training_exp_id)\nexecution.prepare()\nexecution.prepare_plan()\nexecution.prepare_pki_assets()\nwith config.ui.interactive():\nexecution.prepare_admin_cube()\nexecution.get_experiment_status()\nif not silent:\nexecution.print_experiment_status()\nexecution.store_status()\n</code></pre>"},{"location":"reference/commands/training/set_plan/","title":"Set plan","text":""},{"location":"reference/commands/training/set_plan/#commands.training.set_plan.SetPlan","title":"<code>SetPlan</code>","text":"Source code in <code>cli/medperf/commands/training/set_plan.py</code> <pre><code>class SetPlan:\n@classmethod\ndef run(\ncls, training_exp_id: int, training_config_path: str, approval: bool = False\n):\n\"\"\"Creates and submits the training plan\n        Args:\n            training_exp_id (int): training experiment\n            training_config_path (str): path to a training config file\n            approval (bool): skip approval\n        \"\"\"\nplanset = cls(training_exp_id, training_config_path, approval)\nplanset.validate()\nplanset.prepare()\nplanset.create_plan()\nplanset.update()\nplanset.write()\ndef __init__(self, training_exp_id: int, training_config_path: str, approval: bool):\nself.ui = config.ui\nself.training_exp_id = training_exp_id\nself.training_config_path = os.path.abspath(training_config_path)\nself.approved = approval\nself.plan_out_path = generate_tmp_path()\ndef validate(self):\nif not os.path.exists(self.training_config_path):\nraise InvalidArgumentError(\"Provided training config path does not exist\")\ndef prepare(self):\nself.training_exp = TrainingExp.get(self.training_exp_id)\nself.aggregator = Aggregator.from_experiment(self.training_exp_id)\nself.mlcube = self.__get_cube(self.training_exp.fl_mlcube, \"FL\")\nself.aggregator.prepare_config()\ndef __get_cube(self, uid: int, name: str) -&gt; Cube:\nself.ui.text = f\"Retrieving container '{name}'\"\ncube = Cube.get(uid)\ncube.download_run_files()\nself.ui.print(f\"&gt; Container '{name}' download complete\")\nreturn cube\ndef create_plan(self):\n\"\"\"Auto-generates dataset UIDs for both input and output paths\"\"\"\nmounts = {\n\"training_config_path\": self.training_config_path,\n\"aggregator_config_path\": self.aggregator.config_path,\n\"plan_path\": self.plan_out_path,\n}\nself.mlcube.run(\"generate_plan\", mounts=mounts)\ndef update(self):\nwith open(self.plan_out_path) as f:\nplan = yaml.safe_load(f)\nself.training_exp.plan = plan\nbody = {\"plan\": plan}\ndict_pretty_print(body)\nmsg = (\n\"This is the training plan that will be submitted and used by the participants.\"\n\" Do you confirm?[Y/n] \"\n)\nself.approved = self.approved or approval_prompt(msg)\nif self.approved:\nconfig.comms.update_training_exp(self.training_exp.id, body)\nreturn\nraise CleanExit(\"Setting the training plan was cancelled\")\ndef write(self) -&gt; str:\n\"\"\"Writes the registration into disk\n        Args:\n            filename (str, optional): name of the file. Defaults to config.reg_file.\n        \"\"\"\nself.training_exp.write()\n</code></pre>"},{"location":"reference/commands/training/set_plan/#commands.training.set_plan.SetPlan.create_plan","title":"<code>create_plan()</code>","text":"<p>Auto-generates dataset UIDs for both input and output paths</p> Source code in <code>cli/medperf/commands/training/set_plan.py</code> <pre><code>def create_plan(self):\n\"\"\"Auto-generates dataset UIDs for both input and output paths\"\"\"\nmounts = {\n\"training_config_path\": self.training_config_path,\n\"aggregator_config_path\": self.aggregator.config_path,\n\"plan_path\": self.plan_out_path,\n}\nself.mlcube.run(\"generate_plan\", mounts=mounts)\n</code></pre>"},{"location":"reference/commands/training/set_plan/#commands.training.set_plan.SetPlan.run","title":"<code>run(training_exp_id, training_config_path, approval=False)</code>  <code>classmethod</code>","text":"<p>Creates and submits the training plan</p> <p>Parameters:</p> Name Type Description Default <code>training_exp_id</code> <code>int</code> <p>training experiment</p> required <code>training_config_path</code> <code>str</code> <p>path to a training config file</p> required <code>approval</code> <code>bool</code> <p>skip approval</p> <code>False</code> Source code in <code>cli/medperf/commands/training/set_plan.py</code> <pre><code>@classmethod\ndef run(\ncls, training_exp_id: int, training_config_path: str, approval: bool = False\n):\n\"\"\"Creates and submits the training plan\n    Args:\n        training_exp_id (int): training experiment\n        training_config_path (str): path to a training config file\n        approval (bool): skip approval\n    \"\"\"\nplanset = cls(training_exp_id, training_config_path, approval)\nplanset.validate()\nplanset.prepare()\nplanset.create_plan()\nplanset.update()\nplanset.write()\n</code></pre>"},{"location":"reference/commands/training/set_plan/#commands.training.set_plan.SetPlan.write","title":"<code>write()</code>","text":"<p>Writes the registration into disk</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>name of the file. Defaults to config.reg_file.</p> required Source code in <code>cli/medperf/commands/training/set_plan.py</code> <pre><code>def write(self) -&gt; str:\n\"\"\"Writes the registration into disk\n    Args:\n        filename (str, optional): name of the file. Defaults to config.reg_file.\n    \"\"\"\nself.training_exp.write()\n</code></pre>"},{"location":"reference/commands/training/start_event/","title":"Start event","text":""},{"location":"reference/commands/training/submit/","title":"Submit","text":""},{"location":"reference/commands/training/submit/#commands.training.submit.SubmitTrainingExp","title":"<code>SubmitTrainingExp</code>","text":"Source code in <code>cli/medperf/commands/training/submit.py</code> <pre><code>class SubmitTrainingExp:\n@classmethod\ndef run(cls, training_exp_info: dict):\n\"\"\"Submits a new cube to the medperf platform\n        Args:\n            benchmark_info (dict): benchmark information\n                expected keys:\n                    name (str): benchmark name\n                    description (str): benchmark description\n                    docs_url (str): benchmark documentation url\n                    demo_url (str): benchmark demo dataset url\n                    demo_hash (str): benchmark demo dataset hash\n                    data_preparation_mlcube (int): benchmark data preparation mlcube uid\n                    reference_model_mlcube (int): benchmark reference model mlcube uid\n                    evaluator_mlcube (int): benchmark data evaluator mlcube uid\n        \"\"\"\nui = config.ui\nsubmission = cls(training_exp_info)\nwith ui.interactive():\nui.text = \"Getting FL Container\"\nsubmission.get_fl_mlcube()\nui.text = \"Getting FL admin Container\"\nsubmission.get_fl_admin_mlcube()\nui.print(\"&gt; Completed retrieving FL Container\")\nui.text = \"Submitting TrainingExp to MedPerf\"\nupdated_benchmark_body = submission.submit()\nui.print(\"Uploaded\")\nsubmission.write(updated_benchmark_body)\ndef __init__(self, training_exp_info: dict):\nself.ui = config.ui\nself.training_exp = TrainingExp(**training_exp_info)\nconfig.tmp_paths.append(self.training_exp.path)\ndef get_fl_mlcube(self):\nmlcube_id = self.training_exp.fl_mlcube\nCube.get(mlcube_id)\ndef get_fl_admin_mlcube(self):\nmlcube_id = self.training_exp.fl_admin_mlcube\nif mlcube_id:\nCube.get(mlcube_id)\ndef submit(self):\nupdated_body = self.training_exp.upload()\nreturn updated_body\ndef write(self, updated_body):\nremove_path(self.training_exp.path)\ntraining_exp = TrainingExp(**updated_body)\ntraining_exp.write()\n</code></pre>"},{"location":"reference/commands/training/submit/#commands.training.submit.SubmitTrainingExp.run","title":"<code>run(training_exp_info)</code>  <code>classmethod</code>","text":"<p>Submits a new cube to the medperf platform</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_info</code> <code>dict</code> <p>benchmark information expected keys:     name (str): benchmark name     description (str): benchmark description     docs_url (str): benchmark documentation url     demo_url (str): benchmark demo dataset url     demo_hash (str): benchmark demo dataset hash     data_preparation_mlcube (int): benchmark data preparation mlcube uid     reference_model_mlcube (int): benchmark reference model mlcube uid     evaluator_mlcube (int): benchmark data evaluator mlcube uid</p> required Source code in <code>cli/medperf/commands/training/submit.py</code> <pre><code>@classmethod\ndef run(cls, training_exp_info: dict):\n\"\"\"Submits a new cube to the medperf platform\n    Args:\n        benchmark_info (dict): benchmark information\n            expected keys:\n                name (str): benchmark name\n                description (str): benchmark description\n                docs_url (str): benchmark documentation url\n                demo_url (str): benchmark demo dataset url\n                demo_hash (str): benchmark demo dataset hash\n                data_preparation_mlcube (int): benchmark data preparation mlcube uid\n                reference_model_mlcube (int): benchmark reference model mlcube uid\n                evaluator_mlcube (int): benchmark data evaluator mlcube uid\n    \"\"\"\nui = config.ui\nsubmission = cls(training_exp_info)\nwith ui.interactive():\nui.text = \"Getting FL Container\"\nsubmission.get_fl_mlcube()\nui.text = \"Getting FL admin Container\"\nsubmission.get_fl_admin_mlcube()\nui.print(\"&gt; Completed retrieving FL Container\")\nui.text = \"Submitting TrainingExp to MedPerf\"\nupdated_benchmark_body = submission.submit()\nui.print(\"Uploaded\")\nsubmission.write(updated_benchmark_body)\n</code></pre>"},{"location":"reference/commands/training/training/","title":"Training","text":""},{"location":"reference/commands/training/training/#commands.training.training.cancel_event","title":"<code>cancel_event(training_exp_id=typer.Option(..., '--training_exp_id', '-t', help='UID of the desired benchmark'), report_path=typer.Option(..., '--report-path', '-r', help='report path'), approval=typer.Option(False, '-y', help='Skip approval step'))</code>","text":"<p>Runs the benchmark execution step for a given benchmark, prepared dataset and model</p> Source code in <code>cli/medperf/commands/training/training.py</code> <pre><code>@app.command(\"cancel_event\")\n@clean_except\ndef cancel_event(\ntraining_exp_id: int = typer.Option(\n..., \"--training_exp_id\", \"-t\", help=\"UID of the desired benchmark\"\n),\nreport_path: str = typer.Option(..., \"--report-path\", \"-r\", help=\"report path\"),\napproval: bool = typer.Option(False, \"-y\", help=\"Skip approval step\"),\n):\n\"\"\"Runs the benchmark execution step for a given benchmark, prepared dataset and model\"\"\"\nCloseEvent.run(training_exp_id, report_path=report_path, approval=approval)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/training/training/#commands.training.training.close_event","title":"<code>close_event(training_exp_id=typer.Option(..., '--training_exp_id', '-t', help='UID of the desired benchmark'), approval=typer.Option(False, '-y', help='Skip approval step'))</code>","text":"<p>Runs the benchmark execution step for a given benchmark, prepared dataset and model</p> Source code in <code>cli/medperf/commands/training/training.py</code> <pre><code>@app.command(\"close_event\")\n@clean_except\ndef close_event(\ntraining_exp_id: int = typer.Option(\n..., \"--training_exp_id\", \"-t\", help=\"UID of the desired benchmark\"\n),\napproval: bool = typer.Option(False, \"-y\", help=\"Skip approval step\"),\n):\n\"\"\"Runs the benchmark execution step for a given benchmark, prepared dataset and model\"\"\"\nCloseEvent.run(training_exp_id, approval=approval)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/training/training/#commands.training.training.get_experiment_status","title":"<code>get_experiment_status(training_exp_id=typer.Option(..., '--training_exp_id', '-t', help='UID of the desired benchmark'), silent=typer.Option(False, '--silent', help=\"don't print\"))</code>","text":"<p>Runs the benchmark execution step for a given benchmark, prepared dataset and model</p> Source code in <code>cli/medperf/commands/training/training.py</code> <pre><code>@app.command(\"get_experiment_status\")\n@clean_except\ndef get_experiment_status(\ntraining_exp_id: int = typer.Option(\n..., \"--training_exp_id\", \"-t\", help=\"UID of the desired benchmark\"\n),\nsilent: bool = typer.Option(False, \"--silent\", help=\"don't print\"),\n):\n\"\"\"Runs the benchmark execution step for a given benchmark, prepared dataset and model\"\"\"\nGetExperimentStatus.run(training_exp_id, silent)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/training/training/#commands.training.training.list","title":"<code>list(unregistered=typer.Option(False, '--unregistered', help='Get unregistered exps'), mine=typer.Option(False, '--mine', help='Get current-user exps'))</code>","text":"<p>List experiments stored locally and remotely from the user</p> Source code in <code>cli/medperf/commands/training/training.py</code> <pre><code>@app.command(\"ls\")\n@clean_except\ndef list(\nunregistered: bool = typer.Option(\nFalse, \"--unregistered\", help=\"Get unregistered exps\"\n),\nmine: bool = typer.Option(False, \"--mine\", help=\"Get current-user exps\"),\n):\n\"\"\"List experiments stored locally and remotely from the user\"\"\"\nEntityList.run(\nTrainingExp,\nfields=[\"UID\", \"Name\", \"State\", \"Approval Status\", \"Registered\"],\nunregistered=unregistered,\nmine_only=mine,\n)\n</code></pre>"},{"location":"reference/commands/training/training/#commands.training.training.set_plan","title":"<code>set_plan(training_exp_id=typer.Option(..., '--training_exp_id', '-t', help='UID of the desired benchmark'), training_config_path=typer.Option(..., '--config-path', '-c', help='config path'), approval=typer.Option(False, '-y', help='Skip approval step'))</code>","text":"<p>Runs the benchmark execution step for a given benchmark, prepared dataset and model</p> Source code in <code>cli/medperf/commands/training/training.py</code> <pre><code>@app.command(\"set_plan\")\n@clean_except\ndef set_plan(\ntraining_exp_id: int = typer.Option(\n..., \"--training_exp_id\", \"-t\", help=\"UID of the desired benchmark\"\n),\ntraining_config_path: str = typer.Option(\n..., \"--config-path\", \"-c\", help=\"config path\"\n),\napproval: bool = typer.Option(False, \"-y\", help=\"Skip approval step\"),\n):\n\"\"\"Runs the benchmark execution step for a given benchmark, prepared dataset and model\"\"\"\nSetPlan.run(training_exp_id, training_config_path, approval)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/training/training/#commands.training.training.start_event","title":"<code>start_event(training_exp_id=typer.Option(..., '--training_exp_id', '-t', help='UID of the desired benchmark'), name=typer.Option(..., '--name', '-n', help='Name of the benchmark'), participants_list_file=typer.Option(None, '--participants_list_file', '-p', help='Name of the benchmark'), approval=typer.Option(False, '-y', help='Skip approval step'))</code>","text":"<p>Runs the benchmark execution step for a given benchmark, prepared dataset and model</p> Source code in <code>cli/medperf/commands/training/training.py</code> <pre><code>@app.command(\"start_event\")\n@clean_except\ndef start_event(\ntraining_exp_id: int = typer.Option(\n..., \"--training_exp_id\", \"-t\", help=\"UID of the desired benchmark\"\n),\nname: str = typer.Option(..., \"--name\", \"-n\", help=\"Name of the benchmark\"),\nparticipants_list_file: str = typer.Option(\nNone, \"--participants_list_file\", \"-p\", help=\"Name of the benchmark\"\n),\napproval: bool = typer.Option(False, \"-y\", help=\"Skip approval step\"),\n):\n\"\"\"Runs the benchmark execution step for a given benchmark, prepared dataset and model\"\"\"\nStartEvent.run(training_exp_id, name, participants_list_file, approval)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/training/training/#commands.training.training.submit","title":"<code>submit(name=typer.Option(..., '--name', '-n', help='Name of the benchmark'), description=typer.Option(..., '--description', '-d', help='Description of the benchmark'), docs_url=typer.Option('', '--docs-url', '-u', help='URL to documentation'), prep_mlcube=typer.Option(..., '--prep-container', '-p', help='prep container UID'), fl_mlcube=typer.Option(..., '--fl-container', '-m', help='FL container UID'), fl_admin_mlcube=typer.Option(None, '--fl-admin-container', '-a', help='FL admin interface container'), operational=typer.Option(False, '--operational', help='Submit the experiment as OPERATIONAL'))</code>","text":"<p>Submits a new benchmark to the platform</p> Source code in <code>cli/medperf/commands/training/training.py</code> <pre><code>@app.command(\"submit\")\n@clean_except\ndef submit(\nname: str = typer.Option(..., \"--name\", \"-n\", help=\"Name of the benchmark\"),\ndescription: str = typer.Option(\n..., \"--description\", \"-d\", help=\"Description of the benchmark\"\n),\ndocs_url: str = typer.Option(\"\", \"--docs-url\", \"-u\", help=\"URL to documentation\"),\nprep_mlcube: int = typer.Option(\n..., \"--prep-container\", \"-p\", help=\"prep container UID\"\n),\nfl_mlcube: int = typer.Option(..., \"--fl-container\", \"-m\", help=\"FL container UID\"),\nfl_admin_mlcube: int = typer.Option(\nNone, \"--fl-admin-container\", \"-a\", help=\"FL admin interface container\"\n),\noperational: bool = typer.Option(\nFalse,\n\"--operational\",\nhelp=\"Submit the experiment as OPERATIONAL\",\n),\n):\n\"\"\"Submits a new benchmark to the platform\"\"\"\ntraining_exp_info = {\n\"name\": name,\n\"description\": description,\n\"docs_url\": docs_url,\n\"fl_mlcube\": fl_mlcube,\n\"fl_admin_mlcube\": fl_admin_mlcube,\n\"demo_dataset_tarball_url\": \"link\",\n\"demo_dataset_tarball_hash\": \"hash\",\n\"demo_dataset_generated_uid\": \"uid\",\n\"data_preparation_mlcube\": prep_mlcube,\n\"state\": \"OPERATION\" if operational else \"DEVELOPMENT\",\n}\nSubmitTrainingExp.run(training_exp_info)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/training/training/#commands.training.training.update_plan","title":"<code>update_plan(training_exp_id=typer.Option(..., '--training_exp_id', '-t', help='UID of the desired benchmark'), field_name=typer.Option(..., '--field_name', '-f', help='UID of the desired benchmark'), value=typer.Option(..., '--value', '-v', help='UID of the desired benchmark'))</code>","text":"<p>Runtime-update of a scalar field of the training plan</p> Source code in <code>cli/medperf/commands/training/training.py</code> <pre><code>@app.command(\"update_plan\")\n@clean_except\ndef update_plan(\ntraining_exp_id: int = typer.Option(\n..., \"--training_exp_id\", \"-t\", help=\"UID of the desired benchmark\"\n),\nfield_name: str = typer.Option(\n..., \"--field_name\", \"-f\", help=\"UID of the desired benchmark\"\n),\nvalue: str = typer.Option(\n..., \"--value\", \"-v\", help=\"UID of the desired benchmark\"\n),\n):\n\"\"\"Runtime-update of a scalar field of the training plan\"\"\"\nUpdatePlan.run(training_exp_id, field_name, value)\nconfig.ui.print(\"\u2705 Done!\")\n</code></pre>"},{"location":"reference/commands/training/training/#commands.training.training.view","title":"<code>view(entity_id=typer.Argument(None, help='Benchmark ID'), format=typer.Option('yaml', '-f', '--format', help='Format to display contents. Available formats: [yaml, json]'), unregistered=typer.Option(False, '--unregistered', help='Display unregistered benchmarks if benchmark ID is not provided'), mine=typer.Option(False, '--mine', help='Display current-user benchmarks if benchmark ID is not provided'), output=typer.Option(None, '--output', '-o', help='Output file to store contents. If not provided, the output will be displayed'))</code>","text":"<p>Displays the information of one or more benchmarks</p> Source code in <code>cli/medperf/commands/training/training.py</code> <pre><code>@app.command(\"view\")\n@clean_except\ndef view(\nentity_id: Optional[int] = typer.Argument(None, help=\"Benchmark ID\"),\nformat: str = typer.Option(\n\"yaml\",\n\"-f\",\n\"--format\",\nhelp=\"Format to display contents. Available formats: [yaml, json]\",\n),\nunregistered: bool = typer.Option(\nFalse,\n\"--unregistered\",\nhelp=\"Display unregistered benchmarks if benchmark ID is not provided\",\n),\nmine: bool = typer.Option(\nFalse,\n\"--mine\",\nhelp=\"Display current-user benchmarks if benchmark ID is not provided\",\n),\noutput: str = typer.Option(\nNone,\n\"--output\",\n\"-o\",\nhelp=\"Output file to store contents. If not provided, the output will be displayed\",\n),\n):\n\"\"\"Displays the information of one or more benchmarks\"\"\"\nEntityView.run(entity_id, TrainingExp, format, unregistered, mine, output)\n</code></pre>"},{"location":"reference/commands/training/update_plan/","title":"Update plan","text":""},{"location":"reference/commands/training/update_plan/#commands.training.update_plan.UpdatePlan","title":"<code>UpdatePlan</code>","text":"Source code in <code>cli/medperf/commands/training/update_plan.py</code> <pre><code>class UpdatePlan:\n@classmethod\ndef run(cls, training_exp_id: int, field_name: str, field_value: str):\n\"\"\"Starts the aggregation server of a training experiment\n        Args:\n            training_exp_id (int): Training experiment UID.\n        \"\"\"\nexecution = cls(training_exp_id, field_name, field_value)\nexecution.prepare()\nexecution.prepare_plan()\nexecution.prepare_pki_assets()\nwith config.ui.interactive():\nexecution.prepare_admin_cube()\nexecution.update_plan()\ndef __init__(self, training_exp_id: int, field_name: str, field_value: str) -&gt; None:\nself.training_exp_id = training_exp_id\nself.field_name = field_name\nself.field_value = field_value\nself.ui = config.ui\ndef prepare(self):\nself.training_exp = TrainingExp.get(self.training_exp_id)\nself.ui.print(f\"Training Experiment: {self.training_exp.name}\")\nself.user_email: str = get_medperf_user_data()[\"email\"]\nself.temp_dir = generate_tmp_path()\ndef prepare_plan(self):\nself.training_exp.prepare_plan()\ndef prepare_pki_assets(self):\nca = CA.from_experiment(self.training_exp_id)\ntrust(ca)\nself.admin_pki_assets = get_pki_assets_path(self.user_email, ca.name)\nself.ca = ca\ndef prepare_admin_cube(self):\nself.cube = self.__get_cube(self.training_exp.fl_admin_mlcube, \"FL Admin\")\ndef __get_cube(self, uid: int, name: str) -&gt; Cube:\nself.ui.text = (\n\"Retrieving and setting up training Container. This may take some time.\"\n)\ncube = Cube.get(uid)\ncube.download_run_files()\nself.ui.print(f\"&gt; Contaier '{name}' download complete\")\nreturn cube\ndef update_plan(self):\nenv = {\n\"MEDPERF_ADMIN_PARTICIPANT_CN\": self.user_email,\n\"MEDPERF_UPDATE_FIELD_NAME\": self.field_name,\n\"MEDPERF_UPDATE_FIELD_VALUE\": self.field_value,\n}\nmounts = {\n\"node_cert_folder\": self.admin_pki_assets,\n\"ca_cert_folder\": self.ca.pki_assets,\n\"plan_path\": self.training_exp.plan_path,\n\"temp_dir\": self.temp_dir,\n}\nself.ui.text = \"Updating plan\"\nself.cube.run(task=\"update_plan\", mounts=mounts, env=env, disable_network=False)\n</code></pre>"},{"location":"reference/commands/training/update_plan/#commands.training.update_plan.UpdatePlan.run","title":"<code>run(training_exp_id, field_name, field_value)</code>  <code>classmethod</code>","text":"<p>Starts the aggregation server of a training experiment</p> <p>Parameters:</p> Name Type Description Default <code>training_exp_id</code> <code>int</code> <p>Training experiment UID.</p> required Source code in <code>cli/medperf/commands/training/update_plan.py</code> <pre><code>@classmethod\ndef run(cls, training_exp_id: int, field_name: str, field_value: str):\n\"\"\"Starts the aggregation server of a training experiment\n    Args:\n        training_exp_id (int): Training experiment UID.\n    \"\"\"\nexecution = cls(training_exp_id, field_name, field_value)\nexecution.prepare()\nexecution.prepare_plan()\nexecution.prepare_pki_assets()\nwith config.ui.interactive():\nexecution.prepare_admin_cube()\nexecution.update_plan()\n</code></pre>"},{"location":"reference/comms/factory/","title":"Factory","text":""},{"location":"reference/comms/interface/","title":"Interface","text":""},{"location":"reference/comms/interface/#comms.interface.Comms","title":"<code>Comms</code>","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>class Comms(ABC):\n@abstractmethod\ndef __init__(self, source: str):\n\"\"\"Create an instance of a communication object.\n        Args:\n            source (str): location of the communication source. Where messages are going to be sent.\n            ui (UI): Implementation of the UI interface.\n            token (str, Optional): authentication token to be used throughout communication. Defaults to None.\n        \"\"\"\n</code></pre>"},{"location":"reference/comms/interface/#comms.interface.Comms.__init__","title":"<code>__init__(source)</code>  <code>abstractmethod</code>","text":"<p>Create an instance of a communication object.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>location of the communication source. Where messages are going to be sent.</p> required <code>ui</code> <code>UI</code> <p>Implementation of the UI interface.</p> required <code>token</code> <code>(str, Optional)</code> <p>authentication token to be used throughout communication. Defaults to None.</p> required Source code in <code>cli/medperf/comms/interface.py</code> <pre><code>@abstractmethod\ndef __init__(self, source: str):\n\"\"\"Create an instance of a communication object.\n    Args:\n        source (str): location of the communication source. Where messages are going to be sent.\n        ui (UI): Implementation of the UI interface.\n        token (str, Optional): authentication token to be used throughout communication. Defaults to None.\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/rest/","title":"Rest","text":""},{"location":"reference/comms/rest/#comms.rest.REST","title":"<code>REST</code>","text":"<p>             Bases: <code>Comms</code></p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>class REST(Comms):\ndef __init__(self, source: str):\nself.server_url = self.parse_url(source)\nself.cert = config.certificate\nif self.cert is None:\n# No certificate provided, default to normal verification\nself.cert = True\n@classmethod\ndef parse_url(cls, url: str) -&gt; str:\n\"\"\"Parse the source URL so that it can be used by the comms implementation.\n        It should handle protocols and versioning to be able to communicate with the API.\n        Args:\n            url (str): base URL\n        Returns:\n            str: parsed URL with protocol and version\n        \"\"\"\nurl_sections = url.split(\"://\")\napi_path = f\"/api/v{config.major_version}\"\n# Remove protocol if passed\nif len(url_sections) &gt; 1:\nurl = \"\".join(url_sections[1:])\nreturn f\"https://{url}{api_path}\"\ndef __auth_get(self, url, **kwargs):\nreturn self.__auth_req(url, requests.get, **kwargs)\ndef __auth_post(self, url, **kwargs):\nreturn self.__auth_req(url, requests.post, **kwargs)\ndef __auth_put(self, url, **kwargs):\nreturn self.__auth_req(url, requests.put, **kwargs)\ndef __auth_req(self, url, req_func, **kwargs):\ntoken = config.auth.access_token\nreturn self.__req(\nurl, req_func, headers={\"Authorization\": f\"Bearer {token}\"}, **kwargs\n)\ndef __req(self, url, req_func, **kwargs):\nlogging.debug(f\"Calling {req_func}: {url}\")\nif \"json\" in kwargs:\nlogging.debug(f\"Passing JSON contents: {kwargs['json']}\")\nkwargs[\"json\"] = sanitize_json(kwargs[\"json\"])\ntry:\nreturn req_func(url, verify=self.cert, **kwargs)\nexcept requests.exceptions.SSLError as e:\nlogging.error(f\"Couldn't connect to {self.server_url}: {e}\")\nraise CommunicationError(\n\"Couldn't connect to server through HTTPS. If running locally, \"\n\"remember to provide the server certificate through --certificate\"\n)\ndef __get_list(\nself,\nurl,\nnum_elements=None,\npage_size=config.default_page_size,\noffset=0,\nbinary_reduction=False,\nfilters={},\nerror_msg: str = \"\",\n):\n\"\"\"Retrieves a list of elements from a URL by iterating over pages until num_elements is obtained.\n        If num_elements is None, then iterates until all elements have been retrieved.\n        If binary_reduction is enabled, errors are assumed to be related to response size. In that case,\n        the page_size is reduced by half until a successful response is obtained or until page_size can't be\n        reduced anymore.\n        Args:\n            url (str): The url to retrieve elements from\n            num_elements (int, optional): The desired number of elements to be retrieved. Defaults to None.\n            page_size (int, optional): Starting page size. Defaults to config.default_page_size.\n            start_limit (int, optional): The starting position for element retrieval. Defaults to 0.\n            binary_reduction (bool, optional): Wether to handle errors by halfing the page size. Defaults to False.\n        Returns:\n            List[dict]: A list of dictionaries representing the retrieved elements.\n        \"\"\"\nel_list = []\nif num_elements is None:\nnum_elements = float(\"inf\")\nwhile len(el_list) &lt; num_elements:\nfilters.update({\"limit\": page_size, \"offset\": offset})\nquery_str = \"&amp;\".join([f\"{k}={v}\" for k, v in filters.items()])\npaginated_url = f\"{url}?{query_str}\"\nres = self.__auth_get(paginated_url)\nif res.status_code != 200:\nif not binary_reduction:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRetrievalError(f\"{error_msg}: {details}\")\nlog_response_error(res, warn=True)\ndetails = format_errors_dict(res.json())\nif page_size &lt;= 1:\nlogging.debug(\n\"Could not retrieve list. Minimum page size achieved without success\"\n)\nraise CommunicationRetrievalError(f\"{error_msg}: {details}\")\npage_size = page_size // 2\ncontinue\nelse:\ndata = res.json()\nel_list += data[\"results\"]\noffset += len(data[\"results\"])\nif data[\"next\"] is None:\nbreak\nif isinstance(num_elements, int):\nreturn el_list[:num_elements]\nreturn el_list\ndef __get(self, url: str, error_msg: str) -&gt; dict:\n\"\"\"self.__auth_get with error handling\"\"\"\nres = self.__auth_get(url)\nif res.status_code != 200:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRetrievalError(f\"{error_msg}: {details}\")\nreturn res.json()\ndef __post(self, url: str, json: dict, error_msg: str) -&gt; int:\n\"\"\"self.__auth_post with error handling\"\"\"\nres = self.__auth_post(url, json=json)\nif res.status_code != 201:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRetrievalError(f\"{error_msg}: {details}\")\nreturn res.json()\ndef __put(self, url: str, json: dict, error_msg: str):\n\"\"\"self.__auth_put with error handling\"\"\"\nres = self.__auth_put(url, json=json)\nif res.status_code != 200:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRequestError(f\"{error_msg}: {details}\")\ndef get_current_user(self):\n\"\"\"Retrieve the currently-authenticated user information\"\"\"\nurl = f\"{self.server_url}/me/\"\nerror_msg = \"Could not get current user\"\nreturn self.__get(url, error_msg)\n# get object\ndef get_user(self, user_id: int) -&gt; dict:\n\"\"\"Retrieves the specified user. This will only return if\n        the current user has permission to view the requested user,\n        either by being himself, an admin or an owner of a data preparation\n        mlcube used by the requested user\n        Args:\n            user_id (int): User UID\n        Returns:\n            dict: Requested user information\n        \"\"\"\nurl = f\"{self.server_url}/users/{user_id}/\"\nerror_msg = \"Could not retrieve user\"\nreturn self.__get(url, error_msg)\ndef get_benchmark(self, benchmark_uid: int) -&gt; dict:\n\"\"\"Retrieves the benchmark specification file from the server\n        Args:\n            benchmark_uid (int): uid for the desired benchmark\n        Returns:\n            dict: benchmark specification\n        \"\"\"\nurl = f\"{self.server_url}/benchmarks/{benchmark_uid}\"\nerror_msg = \"Could not retrieve benchmark\"\nreturn self.__get(url, error_msg)\ndef get_cube_metadata(self, cube_uid: int) -&gt; dict:\n\"\"\"Retrieves metadata about the specified cube\n        Args:\n            cube_uid (int): UID of the desired cube.\n        Returns:\n            dict: Dictionary containing url and hashes for the cube files\n        \"\"\"\nurl = f\"{self.server_url}/mlcubes/{cube_uid}/\"\nerror_msg = \"Could not retrieve mlcube\"\nreturn self.__get(url, error_msg)\ndef get_dataset(self, dset_uid: int) -&gt; dict:\n\"\"\"Retrieves a specific dataset\n        Args:\n            dset_uid (int): Dataset UID\n        Returns:\n            dict: Dataset metadata\n        \"\"\"\nurl = f\"{self.server_url}/datasets/{dset_uid}/\"\nerror_msg = \"Could not retrieve dataset\"\nreturn self.__get(url, error_msg)\ndef get_execution(self, execution_uid: int) -&gt; dict:\n\"\"\"Retrieves a specific execution data\n        Args:\n            execution_uid (int): Execution UID\n        Returns:\n            dict: Execution metadata\n        \"\"\"\nurl = f\"{self.server_url}/results/{execution_uid}/\"\nerror_msg = \"Could not retrieve execution\"\nreturn self.__get(url, error_msg)\ndef get_training_exp(self, training_exp_id: int) -&gt; dict:\n\"\"\"Retrieves the training_exp specification file from the server\n        Args:\n            training_exp_id (int): uid for the desired training_exp\n        Returns:\n            dict: training_exp specification\n        \"\"\"\nurl = f\"{self.server_url}/training/{training_exp_id}/\"\nerror_msg = \"Could not retrieve training experiment\"\nreturn self.__get(url, error_msg)\ndef get_aggregator(self, aggregator_id: int) -&gt; dict:\n\"\"\"Retrieves the aggregator specification file from the server\n        Args:\n            benchmark_uid (int): uid for the desired benchmark\n        Returns:\n            dict: benchmark specification\n        \"\"\"\nurl = f\"{self.server_url}/aggregators/{aggregator_id}\"\nerror_msg = \"Could not retrieve aggregator\"\nreturn self.__get(url, error_msg)\ndef get_ca(self, ca_id: int) -&gt; dict:\n\"\"\"Retrieves the aggregator specification file from the server\n        Args:\n            benchmark_uid (int): uid for the desired benchmark\n        Returns:\n            dict: benchmark specification\n        \"\"\"\nurl = f\"{self.server_url}/cas/{ca_id}\"\nerror_msg = \"Could not retrieve ca\"\nreturn self.__get(url, error_msg)\ndef get_training_event(self, event_id: int) -&gt; dict:\n\"\"\"Retrieves the aggregator specification file from the server\n        Args:\n            benchmark_uid (int): uid for the desired benchmark\n        Returns:\n            dict: benchmark specification\n        \"\"\"\nurl = f\"{self.server_url}/training/events/{event_id}\"\nerror_msg = \"Could not retrieve training event\"\nreturn self.__get(url, error_msg)\n# get object of an object\ndef get_experiment_event(self, training_exp_id: int) -&gt; dict:\n\"\"\"Retrieves the training experiment's event object from the server\n        Args:\n            training_exp_id (int): uid for the training experiment\n        Returns:\n            dict: event specification\n        \"\"\"\nurl = f\"{self.server_url}/training/{training_exp_id}/event/\"\nerror_msg = \"Could not retrieve training experiment event\"\nreturn self.__get(url, error_msg)\ndef get_experiment_aggregator(self, training_exp_id: int) -&gt; dict:\n\"\"\"Retrieves the training experiment's aggregator object from the server\n        Args:\n            training_exp_id (int): uid for the training experiment\n        Returns:\n            dict: aggregator specification\n        \"\"\"\nurl = f\"{self.server_url}/training/{training_exp_id}/aggregator/\"\nerror_msg = \"Could not retrieve training experiment aggregator\"\nreturn self.__get(url, error_msg)\ndef get_experiment_ca(self, training_exp_id: int) -&gt; dict:\n\"\"\"Retrieves the training experiment's ca object from the server\n        Args:\n            training_exp_id (int): uid for the training experiment\n        Returns:\n            dict: ca specification\n        \"\"\"\nurl = f\"{self.server_url}/training/{training_exp_id}/ca/\"\nerror_msg = \"Could not retrieve training experiment ca\"\nreturn self.__get(url, error_msg)\n# get list\ndef get_benchmarks(self, filters={}) -&gt; List[dict]:\n\"\"\"Retrieves all benchmarks in the platform.\n        Returns:\n            List[dict]: all benchmarks information.\n        \"\"\"\nurl = f\"{self.server_url}/benchmarks/\"\nerror_msg = \"Could not retrieve benchmarks\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\ndef get_cubes(self, filters={}) -&gt; List[dict]:\n\"\"\"Retrieves all MLCubes in the platform\n        Returns:\n            List[dict]: List containing the data of all MLCubes\n        \"\"\"\nurl = f\"{self.server_url}/mlcubes/\"\nerror_msg = \"Could not retrieve mlcubes\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\ndef get_datasets(self, filters={}) -&gt; List[dict]:\n\"\"\"Retrieves all datasets in the platform\n        Returns:\n            List[dict]: List of data from all datasets\n        \"\"\"\nurl = f\"{self.server_url}/datasets/\"\nerror_msg = \"Could not retrieve datasets\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\ndef get_executions(self, filters={}) -&gt; List[dict]:\n\"\"\"Retrieves all executions\n        Returns:\n            List[dict]: List of executions\n        \"\"\"\nurl = f\"{self.server_url}/results/\"\nerror_msg = \"Could not retrieve executions\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\ndef get_training_exps(self, filters={}) -&gt; List[dict]:\n\"\"\"Retrieves all training_exps\n        Returns:\n            List[dict]: List of training_exps\n        \"\"\"\nurl = f\"{self.server_url}/training/\"\nerror_msg = \"Could not retrieve training experiments\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\ndef get_aggregators(self, filters={}) -&gt; List[dict]:\n\"\"\"Retrieves all aggregators\n        Returns:\n            List[dict]: List of aggregators\n        \"\"\"\nurl = f\"{self.server_url}/aggregators/\"\nerror_msg = \"Could not retrieve aggregators\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\ndef get_cas(self, filters={}) -&gt; List[dict]:\n\"\"\"Retrieves all cas\n        Returns:\n            List[dict]: List of cas\n        \"\"\"\nurl = f\"{self.server_url}/cas/\"\nerror_msg = \"Could not retrieve cas\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\ndef get_training_events(self, filters={}) -&gt; List[dict]:\n\"\"\"Retrieves all training events\n        Returns:\n            List[dict]: List of training events\n        \"\"\"\nurl = f\"{self.server_url}/training/events/\"\nerror_msg = \"Could not retrieve training events\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n# get user list\ndef get_user_cubes(self, filters={}) -&gt; List[dict]:\n\"\"\"Retrieves metadata from all cubes registered by the user\n        Returns:\n            List[dict]: List of dictionaries containing the mlcubes registration information\n        \"\"\"\nurl = f\"{self.server_url}/me/mlcubes/\"\nerror_msg = \"Could not retrieve user mlcubes\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\ndef get_user_datasets(self, filters={}) -&gt; dict:\n\"\"\"Retrieves all datasets registered by the user\n        Returns:\n            dict: dictionary with the contents of each dataset registration query\n        \"\"\"\nurl = f\"{self.server_url}/me/datasets/\"\nerror_msg = \"Could not retrieve user datasets\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\ndef get_user_benchmarks(self, filters={}) -&gt; List[dict]:\n\"\"\"Retrieves all benchmarks created by the user\n        Returns:\n            List[dict]: Benchmarks data\n        \"\"\"\nurl = f\"{self.server_url}/me/benchmarks/\"\nerror_msg = \"Could not retrieve user benchmarks\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\ndef get_user_executions(self, filters={}) -&gt; dict:\n\"\"\"Retrieves all executions registered by the user\n        Returns:\n            dict: dictionary with the contents of each execution registration query\n        \"\"\"\nurl = f\"{self.server_url}/me/results/\"\nerror_msg = \"Could not retrieve user executions\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\ndef get_user_training_exps(self, filters={}) -&gt; dict:\n\"\"\"Retrieves all training_exps registered by the user\n        Returns:\n            dict: dictionary with the contents of each result registration query\n        \"\"\"\nurl = f\"{self.server_url}/me/training/\"\nerror_msg = \"Could not retrieve user training experiments\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\ndef get_user_aggregators(self, filters={}) -&gt; dict:\n\"\"\"Retrieves all aggregators registered by the user\n        Returns:\n            dict: dictionary with the contents of each result registration query\n        \"\"\"\nurl = f\"{self.server_url}/me/aggregators/\"\nerror_msg = \"Could not retrieve user aggregators\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\ndef get_user_cas(self, filters={}) -&gt; dict:\n\"\"\"Retrieves all cas registered by the user\n        Returns:\n            dict: dictionary with the contents of each result registration query\n        \"\"\"\nurl = f\"{self.server_url}/me/cas/\"\nerror_msg = \"Could not retrieve user cas\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\ndef get_user_training_events(self, filters={}) -&gt; dict:\n\"\"\"Retrieves all training events registered by the user\n        Returns:\n            dict: dictionary with the contents of each result registration query\n        \"\"\"\nurl = f\"{self.server_url}/me/training/events/\"\nerror_msg = \"Could not retrieve user training events\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n# get user associations list\ndef get_user_benchmarks_datasets_associations(self, filters={}) -&gt; List[dict]:\n\"\"\"Get all dataset associations related to the current user\n        Returns:\n            List[dict]: List containing all associations information\n        \"\"\"\nurl = f\"{self.server_url}/me/datasets/associations/\"\nerror_msg = \"Could not retrieve user datasets benchmark associations\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\ndef get_user_benchmarks_models_associations(self, filters={}) -&gt; List[dict]:\n\"\"\"Get all cube associations related to the current user\n        Returns:\n            List[dict]: List containing all associations information\n        \"\"\"\nurl = f\"{self.server_url}/me/mlcubes/associations/\"\nerror_msg = \"Could not retrieve user mlcubes benchmark associations\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\ndef get_user_training_datasets_associations(self, filters={}) -&gt; List[dict]:\n\"\"\"Get all training dataset associations related to the current user\n        Returns:\n            List[dict]: List containing all associations information\n        \"\"\"\nurl = f\"{self.server_url}/me/datasets/training_associations/\"\nerror_msg = \"Could not retrieve user datasets training associations\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\ndef get_user_training_aggregators_associations(self, filters={}) -&gt; List[dict]:\n\"\"\"Get all aggregator associations related to the current user\n        Returns:\n            List[dict]: List containing all associations information\n        \"\"\"\nurl = f\"{self.server_url}/me/aggregators/training_associations/\"\nerror_msg = \"Could not retrieve user aggregators training associations\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\ndef get_user_training_cas_associations(self, filters={}) -&gt; List[dict]:\n\"\"\"Get all ca associations related to the current user\n        Returns:\n            List[dict]: List containing all associations information\n        \"\"\"\nurl = f\"{self.server_url}/me/cas/training_associations/\"\nerror_msg = \"Could not retrieve user cas training associations\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n# upload\ndef upload_benchmark(self, benchmark_dict: dict) -&gt; int:\n\"\"\"Uploads a new benchmark to the server.\n        Args:\n            benchmark_dict (dict): benchmark_data to be uploaded\n        Returns:\n            int: UID of newly created benchmark\n        \"\"\"\nurl = f\"{self.server_url}/benchmarks/\"\nerror_msg = \"could not upload benchmark\"\nreturn self.__post(url, json=benchmark_dict, error_msg=error_msg)\ndef upload_mlcube(self, mlcube_body: dict) -&gt; int:\n\"\"\"Uploads an MLCube instance to the platform\n        Args:\n            mlcube_body (dict): Dictionary containing all the relevant data for creating mlcubes\n        Returns:\n            int: id of the created mlcube instance on the platform\n        \"\"\"\nurl = f\"{self.server_url}/mlcubes/\"\nerror_msg = \"could not upload mlcube\"\nreturn self.__post(url, json=mlcube_body, error_msg=error_msg)\ndef upload_dataset(self, reg_dict: dict) -&gt; int:\n\"\"\"Uploads registration data to the server, under the sha name of the file.\n        Args:\n            reg_dict (dict): Dictionary containing registration information.\n        Returns:\n            int: id of the created dataset registration.\n        \"\"\"\nurl = f\"{self.server_url}/datasets/\"\nerror_msg = \"could not upload dataset\"\nreturn self.__post(url, json=reg_dict, error_msg=error_msg)\ndef upload_execution(self, executions_dict: dict) -&gt; int:\n\"\"\"Uploads execution to the server.\n        Args:\n            executions_dict (dict): Dictionary containing executions information.\n        Returns:\n            dict: generated executions entry\n        \"\"\"\nurl = f\"{self.server_url}/results/\"\nerror_msg = \"could not upload execution\"\nreturn self.__post(url, json=executions_dict, error_msg=error_msg)\ndef update_execution(self, execution_id: int, data: dict) -&gt; dict:\n\"\"\"Updates an execution object\n        Args:\n            execution_id (int): Execution ID\n            data (dict): Execution data. Can be a partial update\n        Returns:\n            dict: Updated description of the execution\n        \"\"\"\nurl = f\"{self.server_url}/results/{execution_id}/\"\nerror_msg = \"Could not update execution\"\nreturn self.__put(url, json=data, error_msg=error_msg)\ndef upload_training_exp(self, training_exp_dict: dict) -&gt; int:\n\"\"\"Uploads a new training_exp to the server.\n        Args:\n            training_exp_dict (dict): training_exp to be uploaded\n        Returns:\n            dict: newly created training_exp\n        \"\"\"\nurl = f\"{self.server_url}/training/\"\nerror_msg = \"could not upload training experiment\"\nreturn self.__post(url, json=training_exp_dict, error_msg=error_msg)\ndef upload_aggregator(self, aggregator_dict: dict) -&gt; int:\n\"\"\"Uploads a new aggregator to the server.\n        Args:\n            benchmark_dict (dict): benchmark_data to be uploaded\n        Returns:\n            int: UID of newly created benchmark\n        \"\"\"\nurl = f\"{self.server_url}/aggregators/\"\nerror_msg = \"could not upload aggregator\"\nreturn self.__post(url, json=aggregator_dict, error_msg=error_msg)\ndef upload_ca(self, ca_dict: dict) -&gt; int:\n\"\"\"Uploads a new ca to the server.\n        Args:\n            benchmark_dict (dict): benchmark_data to be uploaded\n        Returns:\n            int: UID of newly created benchmark\n        \"\"\"\nurl = f\"{self.server_url}/cas/\"\nerror_msg = \"could not upload ca\"\nreturn self.__post(url, json=ca_dict, error_msg=error_msg)\ndef upload_training_event(self, trainnig_event_dict: dict) -&gt; int:\n\"\"\"Uploads a new training event to the server.\n        Args:\n            benchmark_dict (dict): benchmark_data to be uploaded\n        Returns:\n            int: UID of newly created benchmark\n        \"\"\"\nurl = f\"{self.server_url}/training/events/\"\nerror_msg = \"could not upload training event\"\nreturn self.__post(url, json=trainnig_event_dict, error_msg=error_msg)\n# Association creation\ndef associate_benchmark_dataset(\nself, data_uid: int, benchmark_uid: int, metadata: dict = {}\n):\n\"\"\"Create a Dataset Benchmark association\n        Args:\n            data_uid (int): Registered dataset UID\n            benchmark_uid (int): Benchmark UID\n            metadata (dict, optional): Additional metadata. Defaults to {}.\n        \"\"\"\nurl = f\"{self.server_url}/datasets/benchmarks/\"\ndata = {\n\"dataset\": data_uid,\n\"benchmark\": benchmark_uid,\n\"approval_status\": Status.PENDING.value,\n\"metadata\": metadata,\n}\nerror_msg = \"Could not associate dataset to benchmark\"\nreturn self.__post(url, json=data, error_msg=error_msg)\ndef associate_benchmark_model(\nself, cube_uid: int, benchmark_uid: int, metadata: dict = {}\n):\n\"\"\"Create an MLCube-Benchmark association\n        Args:\n            cube_uid (int): MLCube UID\n            benchmark_uid (int): Benchmark UID\n            metadata (dict, optional): Additional metadata. Defaults to {}.\n        \"\"\"\nurl = f\"{self.server_url}/mlcubes/benchmarks/\"\ndata = {\n\"approval_status\": Status.PENDING.value,\n\"model_mlcube\": cube_uid,\n\"benchmark\": benchmark_uid,\n\"metadata\": metadata,\n}\nerror_msg = \"Could not associate mlcube to benchmark\"\nreturn self.__post(url, json=data, error_msg=error_msg)\ndef associate_training_dataset(self, data_uid: int, training_exp_id: int):\n\"\"\"Create a Dataset experiment association\n        Args:\n            data_uid (int): Registered dataset UID\n            benchmark_uid (int): Benchmark UID\n            metadata (dict, optional): Additional metadata. Defaults to {}.\n        \"\"\"\nurl = f\"{self.server_url}/datasets/training/\"\ndata = {\n\"dataset\": data_uid,\n\"training_exp\": training_exp_id,\n\"approval_status\": Status.PENDING.value,\n}\nerror_msg = \"Could not associate dataset to training_exp\"\nreturn self.__post(url, json=data, error_msg=error_msg)\ndef associate_training_aggregator(self, aggregator_id: int, training_exp_id: int):\n\"\"\"Create a aggregator experiment association\n        Args:\n            aggregator_id (int): Registered aggregator UID\n            training_exp_id (int): training experiment UID\n        \"\"\"\nurl = f\"{self.server_url}/aggregators/training/\"\ndata = {\n\"aggregator\": aggregator_id,\n\"training_exp\": training_exp_id,\n\"approval_status\": Status.PENDING.value,\n}\nerror_msg = \"Could not associate aggregator to training_exp\"\nreturn self.__post(url, json=data, error_msg=error_msg)\ndef associate_training_ca(self, ca_id: int, training_exp_id: int):\n\"\"\"Create a ca experiment association\n        Args:\n            ca_id (int): Registered ca UID\n            training_exp_id (int): training experiment UID\n        \"\"\"\nurl = f\"{self.server_url}/cas/training/\"\ndata = {\n\"ca\": ca_id,\n\"training_exp\": training_exp_id,\n\"approval_status\": Status.PENDING.value,\n}\nerror_msg = \"Could not associate ca to training_exp\"\nreturn self.__post(url, json=data, error_msg=error_msg)\n# updates associations\ndef update_benchmark_dataset_association(\nself, benchmark_uid: int, dataset_uid: int, data: str\n):\n\"\"\"Approves a dataset association\n        Args:\n            dataset_uid (int): Dataset UID\n            benchmark_uid (int): Benchmark UID\n            status (str): Approval status to set for the association\n        \"\"\"\nurl = f\"{self.server_url}/datasets/{dataset_uid}/benchmarks/{benchmark_uid}/\"\nerror_msg = f\"Could not update association: dataset {dataset_uid}, benchmark {benchmark_uid}\"\nself.__put(url, json=data, error_msg=error_msg)\ndef update_benchmark_model_association(\nself, benchmark_uid: int, mlcube_uid: int, data: dict\n):\n\"\"\"Approves an mlcube association\n        Args:\n            mlcube_uid (int): Dataset UID\n            benchmark_uid (int): Benchmark UID\n            status (str): Approval status to set for the association\n        \"\"\"\nurl = f\"{self.server_url}/mlcubes/{mlcube_uid}/benchmarks/{benchmark_uid}/\"\nerror_msg = (\nf\"Could update association: mlcube {mlcube_uid}, benchmark {benchmark_uid}\"\n)\nself.__put(url, json=data, error_msg=error_msg)\ndef update_training_aggregator_association(\nself, training_exp_id: int, aggregator_id: int, data: dict\n):\n\"\"\"Approves a aggregator association\n        Args:\n            dataset_uid (int): Dataset UID\n            benchmark_uid (int): Benchmark UID\n            status (str): Approval status to set for the association\n        \"\"\"\nurl = (\nf\"{self.server_url}/aggregators/{aggregator_id}/training/{training_exp_id}/\"\n)\nerror_msg = (\n\"Could not update association: aggregator\"\nf\" {aggregator_id}, training_exp {training_exp_id}\"\n)\nself.__put(url, json=data, error_msg=error_msg)\ndef update_training_dataset_association(\nself, training_exp_id: int, dataset_uid: int, data: dict\n):\n\"\"\"Approves a training dataset association\n        Args:\n            dataset_uid (int): Dataset UID\n            benchmark_uid (int): Benchmark UID\n            status (str): Approval status to set for the association\n        \"\"\"\nurl = f\"{self.server_url}/datasets/{dataset_uid}/training/{training_exp_id}/\"\nerror_msg = (\n\"Could not approve association: dataset\"\nf\"{dataset_uid}, training_exp {training_exp_id}\"\n)\nself.__put(url, json=data, error_msg=error_msg)\ndef update_training_ca_association(\nself, training_exp_id: int, ca_uid: int, data: dict\n):\n\"\"\"Approves a training ca association\n        Args:\n            dataset_uid (int): Dataset UID\n            benchmark_uid (int): Benchmark UID\n            status (str): Approval status to set for the association\n        \"\"\"\nurl = f\"{self.server_url}/cas/{ca_uid}/training/{training_exp_id}/\"\nerror_msg = (\n\"Could not update association: ca\"\nf\"{ca_uid}, training_exp {training_exp_id}\"\n)\nself.__put(url, json=data, error_msg=error_msg)\n# update objects\ndef update_dataset(self, dataset_id: int, data: dict):\nurl = f\"{self.server_url}/datasets/{dataset_id}/\"\nerror_msg = \"Could not update dataset\"\nreturn self.__put(url, json=data, error_msg=error_msg)\ndef update_training_exp(self, training_exp_id: int, data: dict):\nurl = f\"{self.server_url}/training/{training_exp_id}/\"\nerror_msg = \"Could not update training experiment\"\nreturn self.__put(url, json=data, error_msg=error_msg)\ndef update_training_event(self, training_event_id: int, data: dict):\nurl = f\"{self.server_url}/training/events/{training_event_id}/\"\nerror_msg = \"Could not update training event\"\nreturn self.__put(url, json=data, error_msg=error_msg)\ndef update_benchmark(self, benchmark_id: int, data: dict):\nurl = f\"{self.server_url}/benchmarks/{benchmark_id}/\"\nerror_msg = \"Could not update benchmark\"\nreturn self.__put(url, json=data, error_msg=error_msg)\n# misc\ndef get_benchmark_executions(self, benchmark_id: int, filters={}) -&gt; dict:\n\"\"\"Retrieves all executions for a given benchmark\n        Args:\n            benchmark_id (int): benchmark ID to retrieve executions from\n        Returns:\n            dict: dictionary with the contents of each execution in the specified benchmark\n        \"\"\"\nurl = f\"{self.server_url}/benchmarks/{benchmark_id}/results/\"\nerror_msg = \"Could not get benchmark executions\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\ndef get_mlcube_datasets(self, mlcube_id: int, filters={}) -&gt; dict:\n\"\"\"Retrieves all datasets that have the specified mlcube as the prep mlcube\n        Args:\n            mlcube_id (int): mlcube ID to retrieve datasets from\n        Returns:\n            dict: dictionary with the contents of each dataset\n        \"\"\"\nurl = f\"{self.server_url}/mlcubes/{mlcube_id}/datasets/\"\nerror_msg = \"Could not get mlcube datasets\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\ndef get_training_datasets_associations(\nself, training_exp_id: int, filters={}\n) -&gt; dict:\n\"\"\"Retrieves all datasets for a given training_exp\n        Args:\n            benchmark_id (int): benchmark ID to retrieve results from\n        Returns:\n            dict: dictionary with the contents of each result in the specified benchmark\n        \"\"\"\nurl = f\"{self.server_url}/training/{training_exp_id}/datasets\"\nerror_msg = \"Could not get training experiment datasets associations\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\ndef get_benchmark_models_associations(\nself, benchmark_uid: int, filters={}\n) -&gt; List[int]:\n\"\"\"Retrieves all the model associations of a benchmark.\n        Args:\n            benchmark_uid (int): UID of the desired benchmark\n        Returns:\n            list[int]: List of benchmark model associations\n        \"\"\"\nurl = f\"{self.server_url}/benchmarks/{benchmark_uid}/models\"\nerror_msg = \"Could not get benchmark models associations\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\ndef get_training_datasets_with_users(\nself, training_exp_id: int, filters={}\n) -&gt; dict:\n\"\"\"Retrieves all datasets for a given training_exp and their owner information\n        Args:\n            training_exp_id (int): training exp ID\n        Returns:\n            dict: dictionary with the contents of dataset IDs and owner info\n        \"\"\"\nurl = f\"{self.server_url}/training/{training_exp_id}/participants_info/\"\nerror_msg = \"Could not get training experiment participants info\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\ndef get_benchmark_datasets_with_users(self, benchmark_id: int, filters={}) -&gt; dict:\n\"\"\"Retrieves all datasets for a given benchmark and their owner information\n        Args:\n            benchmark_id (int): benchmark ID\n        Returns:\n            dict: dictionary with the contents of dataset IDs and owner info\n        \"\"\"\nurl = f\"{self.server_url}/benchmarks/{benchmark_id}/participants_info/\"\nerror_msg = \"Could not get benchmark participants info\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.__get","title":"<code>__get(url, error_msg)</code>","text":"<p>self.__auth_get with error handling</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def __get(self, url: str, error_msg: str) -&gt; dict:\n\"\"\"self.__auth_get with error handling\"\"\"\nres = self.__auth_get(url)\nif res.status_code != 200:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRetrievalError(f\"{error_msg}: {details}\")\nreturn res.json()\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.__get_list","title":"<code>__get_list(url, num_elements=None, page_size=config.default_page_size, offset=0, binary_reduction=False, filters={}, error_msg='')</code>","text":"<p>Retrieves a list of elements from a URL by iterating over pages until num_elements is obtained. If num_elements is None, then iterates until all elements have been retrieved. If binary_reduction is enabled, errors are assumed to be related to response size. In that case, the page_size is reduced by half until a successful response is obtained or until page_size can't be reduced anymore.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The url to retrieve elements from</p> required <code>num_elements</code> <code>int</code> <p>The desired number of elements to be retrieved. Defaults to None.</p> <code>None</code> <code>page_size</code> <code>int</code> <p>Starting page size. Defaults to config.default_page_size.</p> <code>config.default_page_size</code> <code>start_limit</code> <code>int</code> <p>The starting position for element retrieval. Defaults to 0.</p> required <code>binary_reduction</code> <code>bool</code> <p>Wether to handle errors by halfing the page size. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>List[dict]: A list of dictionaries representing the retrieved elements.</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def __get_list(\nself,\nurl,\nnum_elements=None,\npage_size=config.default_page_size,\noffset=0,\nbinary_reduction=False,\nfilters={},\nerror_msg: str = \"\",\n):\n\"\"\"Retrieves a list of elements from a URL by iterating over pages until num_elements is obtained.\n    If num_elements is None, then iterates until all elements have been retrieved.\n    If binary_reduction is enabled, errors are assumed to be related to response size. In that case,\n    the page_size is reduced by half until a successful response is obtained or until page_size can't be\n    reduced anymore.\n    Args:\n        url (str): The url to retrieve elements from\n        num_elements (int, optional): The desired number of elements to be retrieved. Defaults to None.\n        page_size (int, optional): Starting page size. Defaults to config.default_page_size.\n        start_limit (int, optional): The starting position for element retrieval. Defaults to 0.\n        binary_reduction (bool, optional): Wether to handle errors by halfing the page size. Defaults to False.\n    Returns:\n        List[dict]: A list of dictionaries representing the retrieved elements.\n    \"\"\"\nel_list = []\nif num_elements is None:\nnum_elements = float(\"inf\")\nwhile len(el_list) &lt; num_elements:\nfilters.update({\"limit\": page_size, \"offset\": offset})\nquery_str = \"&amp;\".join([f\"{k}={v}\" for k, v in filters.items()])\npaginated_url = f\"{url}?{query_str}\"\nres = self.__auth_get(paginated_url)\nif res.status_code != 200:\nif not binary_reduction:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRetrievalError(f\"{error_msg}: {details}\")\nlog_response_error(res, warn=True)\ndetails = format_errors_dict(res.json())\nif page_size &lt;= 1:\nlogging.debug(\n\"Could not retrieve list. Minimum page size achieved without success\"\n)\nraise CommunicationRetrievalError(f\"{error_msg}: {details}\")\npage_size = page_size // 2\ncontinue\nelse:\ndata = res.json()\nel_list += data[\"results\"]\noffset += len(data[\"results\"])\nif data[\"next\"] is None:\nbreak\nif isinstance(num_elements, int):\nreturn el_list[:num_elements]\nreturn el_list\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.__post","title":"<code>__post(url, json, error_msg)</code>","text":"<p>self.__auth_post with error handling</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def __post(self, url: str, json: dict, error_msg: str) -&gt; int:\n\"\"\"self.__auth_post with error handling\"\"\"\nres = self.__auth_post(url, json=json)\nif res.status_code != 201:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRetrievalError(f\"{error_msg}: {details}\")\nreturn res.json()\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.__put","title":"<code>__put(url, json, error_msg)</code>","text":"<p>self.__auth_put with error handling</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def __put(self, url: str, json: dict, error_msg: str):\n\"\"\"self.__auth_put with error handling\"\"\"\nres = self.__auth_put(url, json=json)\nif res.status_code != 200:\nlog_response_error(res)\ndetails = format_errors_dict(res.json())\nraise CommunicationRequestError(f\"{error_msg}: {details}\")\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.associate_benchmark_dataset","title":"<code>associate_benchmark_dataset(data_uid, benchmark_uid, metadata={})</code>","text":"<p>Create a Dataset Benchmark association</p> <p>Parameters:</p> Name Type Description Default <code>data_uid</code> <code>int</code> <p>Registered dataset UID</p> required <code>benchmark_uid</code> <code>int</code> <p>Benchmark UID</p> required <code>metadata</code> <code>dict</code> <p>Additional metadata. Defaults to {}.</p> <code>{}</code> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def associate_benchmark_dataset(\nself, data_uid: int, benchmark_uid: int, metadata: dict = {}\n):\n\"\"\"Create a Dataset Benchmark association\n    Args:\n        data_uid (int): Registered dataset UID\n        benchmark_uid (int): Benchmark UID\n        metadata (dict, optional): Additional metadata. Defaults to {}.\n    \"\"\"\nurl = f\"{self.server_url}/datasets/benchmarks/\"\ndata = {\n\"dataset\": data_uid,\n\"benchmark\": benchmark_uid,\n\"approval_status\": Status.PENDING.value,\n\"metadata\": metadata,\n}\nerror_msg = \"Could not associate dataset to benchmark\"\nreturn self.__post(url, json=data, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.associate_benchmark_model","title":"<code>associate_benchmark_model(cube_uid, benchmark_uid, metadata={})</code>","text":"<p>Create an MLCube-Benchmark association</p> <p>Parameters:</p> Name Type Description Default <code>cube_uid</code> <code>int</code> <p>MLCube UID</p> required <code>benchmark_uid</code> <code>int</code> <p>Benchmark UID</p> required <code>metadata</code> <code>dict</code> <p>Additional metadata. Defaults to {}.</p> <code>{}</code> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def associate_benchmark_model(\nself, cube_uid: int, benchmark_uid: int, metadata: dict = {}\n):\n\"\"\"Create an MLCube-Benchmark association\n    Args:\n        cube_uid (int): MLCube UID\n        benchmark_uid (int): Benchmark UID\n        metadata (dict, optional): Additional metadata. Defaults to {}.\n    \"\"\"\nurl = f\"{self.server_url}/mlcubes/benchmarks/\"\ndata = {\n\"approval_status\": Status.PENDING.value,\n\"model_mlcube\": cube_uid,\n\"benchmark\": benchmark_uid,\n\"metadata\": metadata,\n}\nerror_msg = \"Could not associate mlcube to benchmark\"\nreturn self.__post(url, json=data, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.associate_training_aggregator","title":"<code>associate_training_aggregator(aggregator_id, training_exp_id)</code>","text":"<p>Create a aggregator experiment association</p> <p>Parameters:</p> Name Type Description Default <code>aggregator_id</code> <code>int</code> <p>Registered aggregator UID</p> required <code>training_exp_id</code> <code>int</code> <p>training experiment UID</p> required Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def associate_training_aggregator(self, aggregator_id: int, training_exp_id: int):\n\"\"\"Create a aggregator experiment association\n    Args:\n        aggregator_id (int): Registered aggregator UID\n        training_exp_id (int): training experiment UID\n    \"\"\"\nurl = f\"{self.server_url}/aggregators/training/\"\ndata = {\n\"aggregator\": aggregator_id,\n\"training_exp\": training_exp_id,\n\"approval_status\": Status.PENDING.value,\n}\nerror_msg = \"Could not associate aggregator to training_exp\"\nreturn self.__post(url, json=data, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.associate_training_ca","title":"<code>associate_training_ca(ca_id, training_exp_id)</code>","text":"<p>Create a ca experiment association</p> <p>Parameters:</p> Name Type Description Default <code>ca_id</code> <code>int</code> <p>Registered ca UID</p> required <code>training_exp_id</code> <code>int</code> <p>training experiment UID</p> required Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def associate_training_ca(self, ca_id: int, training_exp_id: int):\n\"\"\"Create a ca experiment association\n    Args:\n        ca_id (int): Registered ca UID\n        training_exp_id (int): training experiment UID\n    \"\"\"\nurl = f\"{self.server_url}/cas/training/\"\ndata = {\n\"ca\": ca_id,\n\"training_exp\": training_exp_id,\n\"approval_status\": Status.PENDING.value,\n}\nerror_msg = \"Could not associate ca to training_exp\"\nreturn self.__post(url, json=data, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.associate_training_dataset","title":"<code>associate_training_dataset(data_uid, training_exp_id)</code>","text":"<p>Create a Dataset experiment association</p> <p>Parameters:</p> Name Type Description Default <code>data_uid</code> <code>int</code> <p>Registered dataset UID</p> required <code>benchmark_uid</code> <code>int</code> <p>Benchmark UID</p> required <code>metadata</code> <code>dict</code> <p>Additional metadata. Defaults to {}.</p> required Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def associate_training_dataset(self, data_uid: int, training_exp_id: int):\n\"\"\"Create a Dataset experiment association\n    Args:\n        data_uid (int): Registered dataset UID\n        benchmark_uid (int): Benchmark UID\n        metadata (dict, optional): Additional metadata. Defaults to {}.\n    \"\"\"\nurl = f\"{self.server_url}/datasets/training/\"\ndata = {\n\"dataset\": data_uid,\n\"training_exp\": training_exp_id,\n\"approval_status\": Status.PENDING.value,\n}\nerror_msg = \"Could not associate dataset to training_exp\"\nreturn self.__post(url, json=data, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_aggregator","title":"<code>get_aggregator(aggregator_id)</code>","text":"<p>Retrieves the aggregator specification file from the server</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>uid for the desired benchmark</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>benchmark specification</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_aggregator(self, aggregator_id: int) -&gt; dict:\n\"\"\"Retrieves the aggregator specification file from the server\n    Args:\n        benchmark_uid (int): uid for the desired benchmark\n    Returns:\n        dict: benchmark specification\n    \"\"\"\nurl = f\"{self.server_url}/aggregators/{aggregator_id}\"\nerror_msg = \"Could not retrieve aggregator\"\nreturn self.__get(url, error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_aggregators","title":"<code>get_aggregators(filters={})</code>","text":"<p>Retrieves all aggregators</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List of aggregators</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_aggregators(self, filters={}) -&gt; List[dict]:\n\"\"\"Retrieves all aggregators\n    Returns:\n        List[dict]: List of aggregators\n    \"\"\"\nurl = f\"{self.server_url}/aggregators/\"\nerror_msg = \"Could not retrieve aggregators\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_benchmark","title":"<code>get_benchmark(benchmark_uid)</code>","text":"<p>Retrieves the benchmark specification file from the server</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>uid for the desired benchmark</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>benchmark specification</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_benchmark(self, benchmark_uid: int) -&gt; dict:\n\"\"\"Retrieves the benchmark specification file from the server\n    Args:\n        benchmark_uid (int): uid for the desired benchmark\n    Returns:\n        dict: benchmark specification\n    \"\"\"\nurl = f\"{self.server_url}/benchmarks/{benchmark_uid}\"\nerror_msg = \"Could not retrieve benchmark\"\nreturn self.__get(url, error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_benchmark_datasets_with_users","title":"<code>get_benchmark_datasets_with_users(benchmark_id, filters={})</code>","text":"<p>Retrieves all datasets for a given benchmark and their owner information</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_id</code> <code>int</code> <p>benchmark ID</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dictionary with the contents of dataset IDs and owner info</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_benchmark_datasets_with_users(self, benchmark_id: int, filters={}) -&gt; dict:\n\"\"\"Retrieves all datasets for a given benchmark and their owner information\n    Args:\n        benchmark_id (int): benchmark ID\n    Returns:\n        dict: dictionary with the contents of dataset IDs and owner info\n    \"\"\"\nurl = f\"{self.server_url}/benchmarks/{benchmark_id}/participants_info/\"\nerror_msg = \"Could not get benchmark participants info\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_benchmark_executions","title":"<code>get_benchmark_executions(benchmark_id, filters={})</code>","text":"<p>Retrieves all executions for a given benchmark</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_id</code> <code>int</code> <p>benchmark ID to retrieve executions from</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dictionary with the contents of each execution in the specified benchmark</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_benchmark_executions(self, benchmark_id: int, filters={}) -&gt; dict:\n\"\"\"Retrieves all executions for a given benchmark\n    Args:\n        benchmark_id (int): benchmark ID to retrieve executions from\n    Returns:\n        dict: dictionary with the contents of each execution in the specified benchmark\n    \"\"\"\nurl = f\"{self.server_url}/benchmarks/{benchmark_id}/results/\"\nerror_msg = \"Could not get benchmark executions\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_benchmark_models_associations","title":"<code>get_benchmark_models_associations(benchmark_uid, filters={})</code>","text":"<p>Retrieves all the model associations of a benchmark.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>UID of the desired benchmark</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>list[int]: List of benchmark model associations</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_benchmark_models_associations(\nself, benchmark_uid: int, filters={}\n) -&gt; List[int]:\n\"\"\"Retrieves all the model associations of a benchmark.\n    Args:\n        benchmark_uid (int): UID of the desired benchmark\n    Returns:\n        list[int]: List of benchmark model associations\n    \"\"\"\nurl = f\"{self.server_url}/benchmarks/{benchmark_uid}/models\"\nerror_msg = \"Could not get benchmark models associations\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_benchmarks","title":"<code>get_benchmarks(filters={})</code>","text":"<p>Retrieves all benchmarks in the platform.</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: all benchmarks information.</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_benchmarks(self, filters={}) -&gt; List[dict]:\n\"\"\"Retrieves all benchmarks in the platform.\n    Returns:\n        List[dict]: all benchmarks information.\n    \"\"\"\nurl = f\"{self.server_url}/benchmarks/\"\nerror_msg = \"Could not retrieve benchmarks\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_ca","title":"<code>get_ca(ca_id)</code>","text":"<p>Retrieves the aggregator specification file from the server</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>uid for the desired benchmark</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>benchmark specification</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_ca(self, ca_id: int) -&gt; dict:\n\"\"\"Retrieves the aggregator specification file from the server\n    Args:\n        benchmark_uid (int): uid for the desired benchmark\n    Returns:\n        dict: benchmark specification\n    \"\"\"\nurl = f\"{self.server_url}/cas/{ca_id}\"\nerror_msg = \"Could not retrieve ca\"\nreturn self.__get(url, error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_cas","title":"<code>get_cas(filters={})</code>","text":"<p>Retrieves all cas</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List of cas</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_cas(self, filters={}) -&gt; List[dict]:\n\"\"\"Retrieves all cas\n    Returns:\n        List[dict]: List of cas\n    \"\"\"\nurl = f\"{self.server_url}/cas/\"\nerror_msg = \"Could not retrieve cas\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_cube_metadata","title":"<code>get_cube_metadata(cube_uid)</code>","text":"<p>Retrieves metadata about the specified cube</p> <p>Parameters:</p> Name Type Description Default <code>cube_uid</code> <code>int</code> <p>UID of the desired cube.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing url and hashes for the cube files</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_cube_metadata(self, cube_uid: int) -&gt; dict:\n\"\"\"Retrieves metadata about the specified cube\n    Args:\n        cube_uid (int): UID of the desired cube.\n    Returns:\n        dict: Dictionary containing url and hashes for the cube files\n    \"\"\"\nurl = f\"{self.server_url}/mlcubes/{cube_uid}/\"\nerror_msg = \"Could not retrieve mlcube\"\nreturn self.__get(url, error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_cubes","title":"<code>get_cubes(filters={})</code>","text":"<p>Retrieves all MLCubes in the platform</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List containing the data of all MLCubes</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_cubes(self, filters={}) -&gt; List[dict]:\n\"\"\"Retrieves all MLCubes in the platform\n    Returns:\n        List[dict]: List containing the data of all MLCubes\n    \"\"\"\nurl = f\"{self.server_url}/mlcubes/\"\nerror_msg = \"Could not retrieve mlcubes\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_current_user","title":"<code>get_current_user()</code>","text":"<p>Retrieve the currently-authenticated user information</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_current_user(self):\n\"\"\"Retrieve the currently-authenticated user information\"\"\"\nurl = f\"{self.server_url}/me/\"\nerror_msg = \"Could not get current user\"\nreturn self.__get(url, error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_dataset","title":"<code>get_dataset(dset_uid)</code>","text":"<p>Retrieves a specific dataset</p> <p>Parameters:</p> Name Type Description Default <code>dset_uid</code> <code>int</code> <p>Dataset UID</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dataset metadata</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_dataset(self, dset_uid: int) -&gt; dict:\n\"\"\"Retrieves a specific dataset\n    Args:\n        dset_uid (int): Dataset UID\n    Returns:\n        dict: Dataset metadata\n    \"\"\"\nurl = f\"{self.server_url}/datasets/{dset_uid}/\"\nerror_msg = \"Could not retrieve dataset\"\nreturn self.__get(url, error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_datasets","title":"<code>get_datasets(filters={})</code>","text":"<p>Retrieves all datasets in the platform</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List of data from all datasets</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_datasets(self, filters={}) -&gt; List[dict]:\n\"\"\"Retrieves all datasets in the platform\n    Returns:\n        List[dict]: List of data from all datasets\n    \"\"\"\nurl = f\"{self.server_url}/datasets/\"\nerror_msg = \"Could not retrieve datasets\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_execution","title":"<code>get_execution(execution_uid)</code>","text":"<p>Retrieves a specific execution data</p> <p>Parameters:</p> Name Type Description Default <code>execution_uid</code> <code>int</code> <p>Execution UID</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Execution metadata</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_execution(self, execution_uid: int) -&gt; dict:\n\"\"\"Retrieves a specific execution data\n    Args:\n        execution_uid (int): Execution UID\n    Returns:\n        dict: Execution metadata\n    \"\"\"\nurl = f\"{self.server_url}/results/{execution_uid}/\"\nerror_msg = \"Could not retrieve execution\"\nreturn self.__get(url, error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_executions","title":"<code>get_executions(filters={})</code>","text":"<p>Retrieves all executions</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List of executions</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_executions(self, filters={}) -&gt; List[dict]:\n\"\"\"Retrieves all executions\n    Returns:\n        List[dict]: List of executions\n    \"\"\"\nurl = f\"{self.server_url}/results/\"\nerror_msg = \"Could not retrieve executions\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_experiment_aggregator","title":"<code>get_experiment_aggregator(training_exp_id)</code>","text":"<p>Retrieves the training experiment's aggregator object from the server</p> <p>Parameters:</p> Name Type Description Default <code>training_exp_id</code> <code>int</code> <p>uid for the training experiment</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>aggregator specification</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_experiment_aggregator(self, training_exp_id: int) -&gt; dict:\n\"\"\"Retrieves the training experiment's aggregator object from the server\n    Args:\n        training_exp_id (int): uid for the training experiment\n    Returns:\n        dict: aggregator specification\n    \"\"\"\nurl = f\"{self.server_url}/training/{training_exp_id}/aggregator/\"\nerror_msg = \"Could not retrieve training experiment aggregator\"\nreturn self.__get(url, error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_experiment_ca","title":"<code>get_experiment_ca(training_exp_id)</code>","text":"<p>Retrieves the training experiment's ca object from the server</p> <p>Parameters:</p> Name Type Description Default <code>training_exp_id</code> <code>int</code> <p>uid for the training experiment</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>ca specification</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_experiment_ca(self, training_exp_id: int) -&gt; dict:\n\"\"\"Retrieves the training experiment's ca object from the server\n    Args:\n        training_exp_id (int): uid for the training experiment\n    Returns:\n        dict: ca specification\n    \"\"\"\nurl = f\"{self.server_url}/training/{training_exp_id}/ca/\"\nerror_msg = \"Could not retrieve training experiment ca\"\nreturn self.__get(url, error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_experiment_event","title":"<code>get_experiment_event(training_exp_id)</code>","text":"<p>Retrieves the training experiment's event object from the server</p> <p>Parameters:</p> Name Type Description Default <code>training_exp_id</code> <code>int</code> <p>uid for the training experiment</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>event specification</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_experiment_event(self, training_exp_id: int) -&gt; dict:\n\"\"\"Retrieves the training experiment's event object from the server\n    Args:\n        training_exp_id (int): uid for the training experiment\n    Returns:\n        dict: event specification\n    \"\"\"\nurl = f\"{self.server_url}/training/{training_exp_id}/event/\"\nerror_msg = \"Could not retrieve training experiment event\"\nreturn self.__get(url, error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_mlcube_datasets","title":"<code>get_mlcube_datasets(mlcube_id, filters={})</code>","text":"<p>Retrieves all datasets that have the specified mlcube as the prep mlcube</p> <p>Parameters:</p> Name Type Description Default <code>mlcube_id</code> <code>int</code> <p>mlcube ID to retrieve datasets from</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dictionary with the contents of each dataset</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_mlcube_datasets(self, mlcube_id: int, filters={}) -&gt; dict:\n\"\"\"Retrieves all datasets that have the specified mlcube as the prep mlcube\n    Args:\n        mlcube_id (int): mlcube ID to retrieve datasets from\n    Returns:\n        dict: dictionary with the contents of each dataset\n    \"\"\"\nurl = f\"{self.server_url}/mlcubes/{mlcube_id}/datasets/\"\nerror_msg = \"Could not get mlcube datasets\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_training_datasets_associations","title":"<code>get_training_datasets_associations(training_exp_id, filters={})</code>","text":"<p>Retrieves all datasets for a given training_exp</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_id</code> <code>int</code> <p>benchmark ID to retrieve results from</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dictionary with the contents of each result in the specified benchmark</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_training_datasets_associations(\nself, training_exp_id: int, filters={}\n) -&gt; dict:\n\"\"\"Retrieves all datasets for a given training_exp\n    Args:\n        benchmark_id (int): benchmark ID to retrieve results from\n    Returns:\n        dict: dictionary with the contents of each result in the specified benchmark\n    \"\"\"\nurl = f\"{self.server_url}/training/{training_exp_id}/datasets\"\nerror_msg = \"Could not get training experiment datasets associations\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_training_datasets_with_users","title":"<code>get_training_datasets_with_users(training_exp_id, filters={})</code>","text":"<p>Retrieves all datasets for a given training_exp and their owner information</p> <p>Parameters:</p> Name Type Description Default <code>training_exp_id</code> <code>int</code> <p>training exp ID</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dictionary with the contents of dataset IDs and owner info</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_training_datasets_with_users(\nself, training_exp_id: int, filters={}\n) -&gt; dict:\n\"\"\"Retrieves all datasets for a given training_exp and their owner information\n    Args:\n        training_exp_id (int): training exp ID\n    Returns:\n        dict: dictionary with the contents of dataset IDs and owner info\n    \"\"\"\nurl = f\"{self.server_url}/training/{training_exp_id}/participants_info/\"\nerror_msg = \"Could not get training experiment participants info\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_training_event","title":"<code>get_training_event(event_id)</code>","text":"<p>Retrieves the aggregator specification file from the server</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>uid for the desired benchmark</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>benchmark specification</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_training_event(self, event_id: int) -&gt; dict:\n\"\"\"Retrieves the aggregator specification file from the server\n    Args:\n        benchmark_uid (int): uid for the desired benchmark\n    Returns:\n        dict: benchmark specification\n    \"\"\"\nurl = f\"{self.server_url}/training/events/{event_id}\"\nerror_msg = \"Could not retrieve training event\"\nreturn self.__get(url, error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_training_events","title":"<code>get_training_events(filters={})</code>","text":"<p>Retrieves all training events</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List of training events</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_training_events(self, filters={}) -&gt; List[dict]:\n\"\"\"Retrieves all training events\n    Returns:\n        List[dict]: List of training events\n    \"\"\"\nurl = f\"{self.server_url}/training/events/\"\nerror_msg = \"Could not retrieve training events\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_training_exp","title":"<code>get_training_exp(training_exp_id)</code>","text":"<p>Retrieves the training_exp specification file from the server</p> <p>Parameters:</p> Name Type Description Default <code>training_exp_id</code> <code>int</code> <p>uid for the desired training_exp</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>training_exp specification</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_training_exp(self, training_exp_id: int) -&gt; dict:\n\"\"\"Retrieves the training_exp specification file from the server\n    Args:\n        training_exp_id (int): uid for the desired training_exp\n    Returns:\n        dict: training_exp specification\n    \"\"\"\nurl = f\"{self.server_url}/training/{training_exp_id}/\"\nerror_msg = \"Could not retrieve training experiment\"\nreturn self.__get(url, error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_training_exps","title":"<code>get_training_exps(filters={})</code>","text":"<p>Retrieves all training_exps</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List of training_exps</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_training_exps(self, filters={}) -&gt; List[dict]:\n\"\"\"Retrieves all training_exps\n    Returns:\n        List[dict]: List of training_exps\n    \"\"\"\nurl = f\"{self.server_url}/training/\"\nerror_msg = \"Could not retrieve training experiments\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_user","title":"<code>get_user(user_id)</code>","text":"<p>Retrieves the specified user. This will only return if the current user has permission to view the requested user, either by being himself, an admin or an owner of a data preparation mlcube used by the requested user</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>int</code> <p>User UID</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Requested user information</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_user(self, user_id: int) -&gt; dict:\n\"\"\"Retrieves the specified user. This will only return if\n    the current user has permission to view the requested user,\n    either by being himself, an admin or an owner of a data preparation\n    mlcube used by the requested user\n    Args:\n        user_id (int): User UID\n    Returns:\n        dict: Requested user information\n    \"\"\"\nurl = f\"{self.server_url}/users/{user_id}/\"\nerror_msg = \"Could not retrieve user\"\nreturn self.__get(url, error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_user_aggregators","title":"<code>get_user_aggregators(filters={})</code>","text":"<p>Retrieves all aggregators registered by the user</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dictionary with the contents of each result registration query</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_user_aggregators(self, filters={}) -&gt; dict:\n\"\"\"Retrieves all aggregators registered by the user\n    Returns:\n        dict: dictionary with the contents of each result registration query\n    \"\"\"\nurl = f\"{self.server_url}/me/aggregators/\"\nerror_msg = \"Could not retrieve user aggregators\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_user_benchmarks","title":"<code>get_user_benchmarks(filters={})</code>","text":"<p>Retrieves all benchmarks created by the user</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: Benchmarks data</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_user_benchmarks(self, filters={}) -&gt; List[dict]:\n\"\"\"Retrieves all benchmarks created by the user\n    Returns:\n        List[dict]: Benchmarks data\n    \"\"\"\nurl = f\"{self.server_url}/me/benchmarks/\"\nerror_msg = \"Could not retrieve user benchmarks\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_user_benchmarks_datasets_associations","title":"<code>get_user_benchmarks_datasets_associations(filters={})</code>","text":"<p>Get all dataset associations related to the current user</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List containing all associations information</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_user_benchmarks_datasets_associations(self, filters={}) -&gt; List[dict]:\n\"\"\"Get all dataset associations related to the current user\n    Returns:\n        List[dict]: List containing all associations information\n    \"\"\"\nurl = f\"{self.server_url}/me/datasets/associations/\"\nerror_msg = \"Could not retrieve user datasets benchmark associations\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_user_benchmarks_models_associations","title":"<code>get_user_benchmarks_models_associations(filters={})</code>","text":"<p>Get all cube associations related to the current user</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List containing all associations information</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_user_benchmarks_models_associations(self, filters={}) -&gt; List[dict]:\n\"\"\"Get all cube associations related to the current user\n    Returns:\n        List[dict]: List containing all associations information\n    \"\"\"\nurl = f\"{self.server_url}/me/mlcubes/associations/\"\nerror_msg = \"Could not retrieve user mlcubes benchmark associations\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_user_cas","title":"<code>get_user_cas(filters={})</code>","text":"<p>Retrieves all cas registered by the user</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dictionary with the contents of each result registration query</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_user_cas(self, filters={}) -&gt; dict:\n\"\"\"Retrieves all cas registered by the user\n    Returns:\n        dict: dictionary with the contents of each result registration query\n    \"\"\"\nurl = f\"{self.server_url}/me/cas/\"\nerror_msg = \"Could not retrieve user cas\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_user_cubes","title":"<code>get_user_cubes(filters={})</code>","text":"<p>Retrieves metadata from all cubes registered by the user</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List of dictionaries containing the mlcubes registration information</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_user_cubes(self, filters={}) -&gt; List[dict]:\n\"\"\"Retrieves metadata from all cubes registered by the user\n    Returns:\n        List[dict]: List of dictionaries containing the mlcubes registration information\n    \"\"\"\nurl = f\"{self.server_url}/me/mlcubes/\"\nerror_msg = \"Could not retrieve user mlcubes\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_user_datasets","title":"<code>get_user_datasets(filters={})</code>","text":"<p>Retrieves all datasets registered by the user</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dictionary with the contents of each dataset registration query</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_user_datasets(self, filters={}) -&gt; dict:\n\"\"\"Retrieves all datasets registered by the user\n    Returns:\n        dict: dictionary with the contents of each dataset registration query\n    \"\"\"\nurl = f\"{self.server_url}/me/datasets/\"\nerror_msg = \"Could not retrieve user datasets\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_user_executions","title":"<code>get_user_executions(filters={})</code>","text":"<p>Retrieves all executions registered by the user</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dictionary with the contents of each execution registration query</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_user_executions(self, filters={}) -&gt; dict:\n\"\"\"Retrieves all executions registered by the user\n    Returns:\n        dict: dictionary with the contents of each execution registration query\n    \"\"\"\nurl = f\"{self.server_url}/me/results/\"\nerror_msg = \"Could not retrieve user executions\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_user_training_aggregators_associations","title":"<code>get_user_training_aggregators_associations(filters={})</code>","text":"<p>Get all aggregator associations related to the current user</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List containing all associations information</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_user_training_aggregators_associations(self, filters={}) -&gt; List[dict]:\n\"\"\"Get all aggregator associations related to the current user\n    Returns:\n        List[dict]: List containing all associations information\n    \"\"\"\nurl = f\"{self.server_url}/me/aggregators/training_associations/\"\nerror_msg = \"Could not retrieve user aggregators training associations\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_user_training_cas_associations","title":"<code>get_user_training_cas_associations(filters={})</code>","text":"<p>Get all ca associations related to the current user</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List containing all associations information</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_user_training_cas_associations(self, filters={}) -&gt; List[dict]:\n\"\"\"Get all ca associations related to the current user\n    Returns:\n        List[dict]: List containing all associations information\n    \"\"\"\nurl = f\"{self.server_url}/me/cas/training_associations/\"\nerror_msg = \"Could not retrieve user cas training associations\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_user_training_datasets_associations","title":"<code>get_user_training_datasets_associations(filters={})</code>","text":"<p>Get all training dataset associations related to the current user</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List containing all associations information</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_user_training_datasets_associations(self, filters={}) -&gt; List[dict]:\n\"\"\"Get all training dataset associations related to the current user\n    Returns:\n        List[dict]: List containing all associations information\n    \"\"\"\nurl = f\"{self.server_url}/me/datasets/training_associations/\"\nerror_msg = \"Could not retrieve user datasets training associations\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_user_training_events","title":"<code>get_user_training_events(filters={})</code>","text":"<p>Retrieves all training events registered by the user</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dictionary with the contents of each result registration query</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_user_training_events(self, filters={}) -&gt; dict:\n\"\"\"Retrieves all training events registered by the user\n    Returns:\n        dict: dictionary with the contents of each result registration query\n    \"\"\"\nurl = f\"{self.server_url}/me/training/events/\"\nerror_msg = \"Could not retrieve user training events\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.get_user_training_exps","title":"<code>get_user_training_exps(filters={})</code>","text":"<p>Retrieves all training_exps registered by the user</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dictionary with the contents of each result registration query</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def get_user_training_exps(self, filters={}) -&gt; dict:\n\"\"\"Retrieves all training_exps registered by the user\n    Returns:\n        dict: dictionary with the contents of each result registration query\n    \"\"\"\nurl = f\"{self.server_url}/me/training/\"\nerror_msg = \"Could not retrieve user training experiments\"\nreturn self.__get_list(url, filters=filters, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.parse_url","title":"<code>parse_url(url)</code>  <code>classmethod</code>","text":"<p>Parse the source URL so that it can be used by the comms implementation. It should handle protocols and versioning to be able to communicate with the API.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>base URL</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>parsed URL with protocol and version</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>@classmethod\ndef parse_url(cls, url: str) -&gt; str:\n\"\"\"Parse the source URL so that it can be used by the comms implementation.\n    It should handle protocols and versioning to be able to communicate with the API.\n    Args:\n        url (str): base URL\n    Returns:\n        str: parsed URL with protocol and version\n    \"\"\"\nurl_sections = url.split(\"://\")\napi_path = f\"/api/v{config.major_version}\"\n# Remove protocol if passed\nif len(url_sections) &gt; 1:\nurl = \"\".join(url_sections[1:])\nreturn f\"https://{url}{api_path}\"\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.update_benchmark_dataset_association","title":"<code>update_benchmark_dataset_association(benchmark_uid, dataset_uid, data)</code>","text":"<p>Approves a dataset association</p> <p>Parameters:</p> Name Type Description Default <code>dataset_uid</code> <code>int</code> <p>Dataset UID</p> required <code>benchmark_uid</code> <code>int</code> <p>Benchmark UID</p> required <code>status</code> <code>str</code> <p>Approval status to set for the association</p> required Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def update_benchmark_dataset_association(\nself, benchmark_uid: int, dataset_uid: int, data: str\n):\n\"\"\"Approves a dataset association\n    Args:\n        dataset_uid (int): Dataset UID\n        benchmark_uid (int): Benchmark UID\n        status (str): Approval status to set for the association\n    \"\"\"\nurl = f\"{self.server_url}/datasets/{dataset_uid}/benchmarks/{benchmark_uid}/\"\nerror_msg = f\"Could not update association: dataset {dataset_uid}, benchmark {benchmark_uid}\"\nself.__put(url, json=data, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.update_benchmark_model_association","title":"<code>update_benchmark_model_association(benchmark_uid, mlcube_uid, data)</code>","text":"<p>Approves an mlcube association</p> <p>Parameters:</p> Name Type Description Default <code>mlcube_uid</code> <code>int</code> <p>Dataset UID</p> required <code>benchmark_uid</code> <code>int</code> <p>Benchmark UID</p> required <code>status</code> <code>str</code> <p>Approval status to set for the association</p> required Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def update_benchmark_model_association(\nself, benchmark_uid: int, mlcube_uid: int, data: dict\n):\n\"\"\"Approves an mlcube association\n    Args:\n        mlcube_uid (int): Dataset UID\n        benchmark_uid (int): Benchmark UID\n        status (str): Approval status to set for the association\n    \"\"\"\nurl = f\"{self.server_url}/mlcubes/{mlcube_uid}/benchmarks/{benchmark_uid}/\"\nerror_msg = (\nf\"Could update association: mlcube {mlcube_uid}, benchmark {benchmark_uid}\"\n)\nself.__put(url, json=data, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.update_execution","title":"<code>update_execution(execution_id, data)</code>","text":"<p>Updates an execution object</p> <p>Parameters:</p> Name Type Description Default <code>execution_id</code> <code>int</code> <p>Execution ID</p> required <code>data</code> <code>dict</code> <p>Execution data. Can be a partial update</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Updated description of the execution</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def update_execution(self, execution_id: int, data: dict) -&gt; dict:\n\"\"\"Updates an execution object\n    Args:\n        execution_id (int): Execution ID\n        data (dict): Execution data. Can be a partial update\n    Returns:\n        dict: Updated description of the execution\n    \"\"\"\nurl = f\"{self.server_url}/results/{execution_id}/\"\nerror_msg = \"Could not update execution\"\nreturn self.__put(url, json=data, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.update_training_aggregator_association","title":"<code>update_training_aggregator_association(training_exp_id, aggregator_id, data)</code>","text":"<p>Approves a aggregator association</p> <p>Parameters:</p> Name Type Description Default <code>dataset_uid</code> <code>int</code> <p>Dataset UID</p> required <code>benchmark_uid</code> <code>int</code> <p>Benchmark UID</p> required <code>status</code> <code>str</code> <p>Approval status to set for the association</p> required Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def update_training_aggregator_association(\nself, training_exp_id: int, aggregator_id: int, data: dict\n):\n\"\"\"Approves a aggregator association\n    Args:\n        dataset_uid (int): Dataset UID\n        benchmark_uid (int): Benchmark UID\n        status (str): Approval status to set for the association\n    \"\"\"\nurl = (\nf\"{self.server_url}/aggregators/{aggregator_id}/training/{training_exp_id}/\"\n)\nerror_msg = (\n\"Could not update association: aggregator\"\nf\" {aggregator_id}, training_exp {training_exp_id}\"\n)\nself.__put(url, json=data, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.update_training_ca_association","title":"<code>update_training_ca_association(training_exp_id, ca_uid, data)</code>","text":"<p>Approves a training ca association</p> <p>Parameters:</p> Name Type Description Default <code>dataset_uid</code> <code>int</code> <p>Dataset UID</p> required <code>benchmark_uid</code> <code>int</code> <p>Benchmark UID</p> required <code>status</code> <code>str</code> <p>Approval status to set for the association</p> required Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def update_training_ca_association(\nself, training_exp_id: int, ca_uid: int, data: dict\n):\n\"\"\"Approves a training ca association\n    Args:\n        dataset_uid (int): Dataset UID\n        benchmark_uid (int): Benchmark UID\n        status (str): Approval status to set for the association\n    \"\"\"\nurl = f\"{self.server_url}/cas/{ca_uid}/training/{training_exp_id}/\"\nerror_msg = (\n\"Could not update association: ca\"\nf\"{ca_uid}, training_exp {training_exp_id}\"\n)\nself.__put(url, json=data, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.update_training_dataset_association","title":"<code>update_training_dataset_association(training_exp_id, dataset_uid, data)</code>","text":"<p>Approves a training dataset association</p> <p>Parameters:</p> Name Type Description Default <code>dataset_uid</code> <code>int</code> <p>Dataset UID</p> required <code>benchmark_uid</code> <code>int</code> <p>Benchmark UID</p> required <code>status</code> <code>str</code> <p>Approval status to set for the association</p> required Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def update_training_dataset_association(\nself, training_exp_id: int, dataset_uid: int, data: dict\n):\n\"\"\"Approves a training dataset association\n    Args:\n        dataset_uid (int): Dataset UID\n        benchmark_uid (int): Benchmark UID\n        status (str): Approval status to set for the association\n    \"\"\"\nurl = f\"{self.server_url}/datasets/{dataset_uid}/training/{training_exp_id}/\"\nerror_msg = (\n\"Could not approve association: dataset\"\nf\"{dataset_uid}, training_exp {training_exp_id}\"\n)\nself.__put(url, json=data, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.upload_aggregator","title":"<code>upload_aggregator(aggregator_dict)</code>","text":"<p>Uploads a new aggregator to the server.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_dict</code> <code>dict</code> <p>benchmark_data to be uploaded</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>UID of newly created benchmark</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def upload_aggregator(self, aggregator_dict: dict) -&gt; int:\n\"\"\"Uploads a new aggregator to the server.\n    Args:\n        benchmark_dict (dict): benchmark_data to be uploaded\n    Returns:\n        int: UID of newly created benchmark\n    \"\"\"\nurl = f\"{self.server_url}/aggregators/\"\nerror_msg = \"could not upload aggregator\"\nreturn self.__post(url, json=aggregator_dict, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.upload_benchmark","title":"<code>upload_benchmark(benchmark_dict)</code>","text":"<p>Uploads a new benchmark to the server.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_dict</code> <code>dict</code> <p>benchmark_data to be uploaded</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>UID of newly created benchmark</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def upload_benchmark(self, benchmark_dict: dict) -&gt; int:\n\"\"\"Uploads a new benchmark to the server.\n    Args:\n        benchmark_dict (dict): benchmark_data to be uploaded\n    Returns:\n        int: UID of newly created benchmark\n    \"\"\"\nurl = f\"{self.server_url}/benchmarks/\"\nerror_msg = \"could not upload benchmark\"\nreturn self.__post(url, json=benchmark_dict, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.upload_ca","title":"<code>upload_ca(ca_dict)</code>","text":"<p>Uploads a new ca to the server.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_dict</code> <code>dict</code> <p>benchmark_data to be uploaded</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>UID of newly created benchmark</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def upload_ca(self, ca_dict: dict) -&gt; int:\n\"\"\"Uploads a new ca to the server.\n    Args:\n        benchmark_dict (dict): benchmark_data to be uploaded\n    Returns:\n        int: UID of newly created benchmark\n    \"\"\"\nurl = f\"{self.server_url}/cas/\"\nerror_msg = \"could not upload ca\"\nreturn self.__post(url, json=ca_dict, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.upload_dataset","title":"<code>upload_dataset(reg_dict)</code>","text":"<p>Uploads registration data to the server, under the sha name of the file.</p> <p>Parameters:</p> Name Type Description Default <code>reg_dict</code> <code>dict</code> <p>Dictionary containing registration information.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>id of the created dataset registration.</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def upload_dataset(self, reg_dict: dict) -&gt; int:\n\"\"\"Uploads registration data to the server, under the sha name of the file.\n    Args:\n        reg_dict (dict): Dictionary containing registration information.\n    Returns:\n        int: id of the created dataset registration.\n    \"\"\"\nurl = f\"{self.server_url}/datasets/\"\nerror_msg = \"could not upload dataset\"\nreturn self.__post(url, json=reg_dict, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.upload_execution","title":"<code>upload_execution(executions_dict)</code>","text":"<p>Uploads execution to the server.</p> <p>Parameters:</p> Name Type Description Default <code>executions_dict</code> <code>dict</code> <p>Dictionary containing executions information.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>int</code> <p>generated executions entry</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def upload_execution(self, executions_dict: dict) -&gt; int:\n\"\"\"Uploads execution to the server.\n    Args:\n        executions_dict (dict): Dictionary containing executions information.\n    Returns:\n        dict: generated executions entry\n    \"\"\"\nurl = f\"{self.server_url}/results/\"\nerror_msg = \"could not upload execution\"\nreturn self.__post(url, json=executions_dict, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.upload_mlcube","title":"<code>upload_mlcube(mlcube_body)</code>","text":"<p>Uploads an MLCube instance to the platform</p> <p>Parameters:</p> Name Type Description Default <code>mlcube_body</code> <code>dict</code> <p>Dictionary containing all the relevant data for creating mlcubes</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>id of the created mlcube instance on the platform</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def upload_mlcube(self, mlcube_body: dict) -&gt; int:\n\"\"\"Uploads an MLCube instance to the platform\n    Args:\n        mlcube_body (dict): Dictionary containing all the relevant data for creating mlcubes\n    Returns:\n        int: id of the created mlcube instance on the platform\n    \"\"\"\nurl = f\"{self.server_url}/mlcubes/\"\nerror_msg = \"could not upload mlcube\"\nreturn self.__post(url, json=mlcube_body, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.upload_training_event","title":"<code>upload_training_event(trainnig_event_dict)</code>","text":"<p>Uploads a new training event to the server.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_dict</code> <code>dict</code> <p>benchmark_data to be uploaded</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>UID of newly created benchmark</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def upload_training_event(self, trainnig_event_dict: dict) -&gt; int:\n\"\"\"Uploads a new training event to the server.\n    Args:\n        benchmark_dict (dict): benchmark_data to be uploaded\n    Returns:\n        int: UID of newly created benchmark\n    \"\"\"\nurl = f\"{self.server_url}/training/events/\"\nerror_msg = \"could not upload training event\"\nreturn self.__post(url, json=trainnig_event_dict, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/rest/#comms.rest.REST.upload_training_exp","title":"<code>upload_training_exp(training_exp_dict)</code>","text":"<p>Uploads a new training_exp to the server.</p> <p>Parameters:</p> Name Type Description Default <code>training_exp_dict</code> <code>dict</code> <p>training_exp to be uploaded</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>int</code> <p>newly created training_exp</p> Source code in <code>cli/medperf/comms/rest.py</code> <pre><code>def upload_training_exp(self, training_exp_dict: dict) -&gt; int:\n\"\"\"Uploads a new training_exp to the server.\n    Args:\n        training_exp_dict (dict): training_exp to be uploaded\n    Returns:\n        dict: newly created training_exp\n    \"\"\"\nurl = f\"{self.server_url}/training/\"\nerror_msg = \"could not upload training experiment\"\nreturn self.__post(url, json=training_exp_dict, error_msg=error_msg)\n</code></pre>"},{"location":"reference/comms/auth/auth0/","title":"Auth0","text":""},{"location":"reference/comms/auth/auth0/#comms.auth.auth0.Auth0","title":"<code>Auth0</code>","text":"<p>             Bases: <code>Auth</code></p> Source code in <code>cli/medperf/comms/auth/auth0.py</code> <pre><code>class Auth0(Auth):\ndef __init__(self):\nself.domain = config.auth_domain\nself.client_id = config.auth_client_id\nself.audience = config.auth_audience\nself._lock = threading.Lock()\ndef login(self, email):\n\"\"\"Retrieves and stores an access token/refresh token pair from the auth0\n        backend using the device authorization flow.\n        Args:\n            email (str): user email. This will be used to validate that the received\n                         id_token contains the same email address.\n        \"\"\"\ndevice_code_response = self.__request_device_code()\ndevice_code = device_code_response[\"device_code\"]\nuser_code = device_code_response[\"user_code\"]\nverification_uri_complete = device_code_response[\"verification_uri_complete\"]\ninterval = device_code_response[\"interval\"]\nconfig.ui.print(\n\"\\nPlease go to the following link to complete your login request:\\n\\t\"\n)\nconfig.ui.print_url(verification_uri_complete)\nconfig.ui.print(\n\"\\n\\nMake sure that you will be presented with the following code:\\n\\t\"\n)\nconfig.ui.print_code(user_code)\nconfig.ui.print(\"\\n\\n\")\nconfig.ui.print_warning(\n\"Keep this terminal open until you complete your login request. \"\n\"The command will exit on its own once you complete the request. \"\n\"If you wish to stop the login request anyway, press Ctrl+C.\"\n)\ntoken_response, token_issued_at = self.__get_device_access_token(\ndevice_code, interval\n)\naccess_token = token_response[\"access_token\"]\nid_token = token_response[\"id_token\"]\nrefresh_token = token_response[\"refresh_token\"]\ntoken_expires_in = token_response[\"expires_in\"]\nid_token_payload = verify_token(id_token)\nself.__check_token_email(id_token_payload, email)\nset_credentials(\naccess_token,\nrefresh_token,\nid_token_payload,\ntoken_issued_at,\ntoken_expires_in,\nlogin_event=True,\n)\ndef __request_device_code(self):\n\"\"\"Get a device code from the auth0 backend to be used for the authorization process\"\"\"\nurl = f\"https://{self.domain}/oauth/device/code\"\nheaders = {\"content-type\": \"application/x-www-form-urlencoded\"}\nbody = {\n\"client_id\": self.client_id,\n\"audience\": self.audience,\n\"scope\": \"offline_access openid email\",\n}\nres = requests.post(url=url, headers=headers, data=body)\nif res.status_code != 200:\nself.__raise_errors(res, \"Login\")\nreturn res.json()\ndef __get_device_access_token(self, device_code, polling_interval):\n\"\"\"Get the access token from the auth0 backend associated with\n        the device code requested before. This function will keep polling\n        the access token until the user completes the browser flow part\n        of the authorization process.\n        Args:\n            device_code (str): A temporary device code requested by `__request_device_code`\n            polling_interval (float): number of seconds to wait between each two polling requests\n        Returns:\n            json_res (dict): the response of the successful request, containg the access/refresh tokens pair\n            token_issued_at (float): the timestamp when the access token was issued\n        \"\"\"\nurl = f\"https://{self.domain}/oauth/token\"\nheaders = {\"content-type\": \"application/x-www-form-urlencoded\"}\nbody = {\n\"grant_type\": \"urn:ietf:params:oauth:grant-type:device_code\",\n\"device_code\": device_code,\n\"client_id\": self.client_id,\n}\nwhile True:\ntime.sleep(polling_interval)\ntoken_issued_at = time.time()\nres = requests.post(url=url, headers=headers, data=body)\nif res.status_code == 200:\njson_res = res.json()\nreturn json_res, token_issued_at\ntry:\njson_res = res.json()\nexcept requests.exceptions.JSONDecodeError:\njson_res = {}\nerror = json_res.get(\"error\", None)\nif error not in [\"slow_down\", \"authorization_pending\"]:\nself.__raise_errors(res, \"Login\")\ndef __check_token_email(self, id_token_payload, email):\n\"\"\"Checks if the email provided by the user in the terminal matches the\n        email found in the recieved id token.\"\"\"\nemail_in_token = id_token_payload[\"email\"]\nif email.lower() != email_in_token:\nraise CommunicationError(\n\"The email provided in the terminal does not match the one provided during login\"\n)\ndef logout(self):\n\"\"\"Logs out the user by revoking their refresh token and deleting the\n        stored tokens.\"\"\"\ntry:\ncreds = read_credentials()\nexcept AuthenticationError:\nreturn\nrefresh_token = creds[\"refresh_token\"]\nurl = f\"https://{self.domain}/oauth/revoke\"\nheaders = {\"content-type\": \"application/json\"}\nbody = {\n\"client_id\": self.client_id,\n\"token\": refresh_token,\n}\nres = requests.post(url=url, headers=headers, json=body)\nif res.status_code != 200:\nself.__raise_errors(res, \"Logout\")\ndelete_credentials()\n@property\ndef access_token(self):\n\"\"\"Thread and process-safe access token retrieval\"\"\"\n# In case of multiple threads are using the same connection object,\n# keep the thread lock, otherwise the database will throw\n# errors of starting a transaction within a transaction.\n# In case of each thread is using a different connection object,\n# keep the thread lock to avoid the OperationalError when\n# multiple threads want to access the database.\nwith self._lock:\n# TODO: This is temporary. Use a cleaner solution.\ndb = sqlite3.connect(config.tokens_db, isolation_level=None, timeout=60)\ntry:\ndb.execute(\"BEGIN EXCLUSIVE TRANSACTION\")\nexcept sqlite3.OperationalError:\nmsg = \"Another process is using the database. Try again later\"\nraise CommunicationError(msg)\ntoken = self._access_token\n# Sqlite will automatically execute COMMIT and close the connection\n# if an exception is raised during the retrieval of the access token.\ndb.execute(\"COMMIT\")\ndb.close()\nreturn token\n@property\ndef _access_token(self):\n\"\"\"Reads and returns an access token of the currently logged\n        in user to be used for authorizing requests to the MedPerf server.\n        Refresh the token if necessary.\n        Returns:\n            access_token (str): the access token\n        \"\"\"\ncreds = read_credentials()\naccess_token = creds[\"access_token\"]\nrefresh_token = creds[\"refresh_token\"]\ntoken_expires_in = creds[\"token_expires_in\"]\ntoken_issued_at = creds[\"token_issued_at\"]\nlogged_in_at = creds[\"logged_in_at\"]\n# token_issued_at and expires_in are for the access token\nsliding_expiration_time = (\ntoken_issued_at + token_expires_in - config.token_expiration_leeway\n)\nabsolute_expiration_time = (\nlogged_in_at\n+ config.token_absolute_expiry\n- config.refresh_token_expiration_leeway\n)\ncurrent_time = time.time()\nif current_time &lt; sliding_expiration_time:\n# Access token not expired. No need to refresh.\nreturn access_token\n# So we need to refresh.\nif current_time &gt; absolute_expiration_time:\n# Expired refresh token. Force logout and ask the user to re-authenticate\nlogging.debug(\nf\"Refresh token expired: {absolute_expiration_time=} &lt;&gt; {current_time=}\"\n)\nself.logout()\nraise AuthenticationError(\"Token expired. Please re-authenticate\")\n# Expired access token and not expired refresh token. Refresh.\naccess_token = self.__refresh_access_token(refresh_token)\nreturn access_token\ndef __refresh_access_token(self, refresh_token):\n\"\"\"Retrieve and store a new access token using a refresh token.\n        A new refresh token will also be retrieved and stored.\n        Args:\n            refresh_token (str): the refresh token\n        Returns:\n            access_token (str): the new access token\n        \"\"\"\nurl = f\"https://{self.domain}/oauth/token\"\nheaders = {\"content-type\": \"application/x-www-form-urlencoded\"}\nbody = {\n\"grant_type\": \"refresh_token\",\n\"client_id\": self.client_id,\n\"refresh_token\": refresh_token,\n}\ntoken_issued_at = time.time()\nlogging.debug(\"Refreshing access token.\")\nres = requests.post(url=url, headers=headers, data=body)\nif res.status_code != 200:\nself.__raise_errors(res, \"Token refresh\")\njson_res = res.json()\naccess_token = json_res[\"access_token\"]\nid_token = json_res[\"id_token\"]\nrefresh_token = json_res[\"refresh_token\"]\ntoken_expires_in = json_res[\"expires_in\"]\nid_token_payload = verify_token(id_token)\nset_credentials(\naccess_token,\nrefresh_token,\nid_token_payload,\ntoken_issued_at,\ntoken_expires_in,\n)\nreturn access_token\ndef __raise_errors(self, res, action):\n\"\"\"log the failed request's response and raise errors.\n        Args:\n            res (requests.Response): the response of a failed request\n            action (str): a string for more informative error display\n            to the user.\n        \"\"\"\nlog_response_error(res)\nif res.status_code == 429:\nraise CommunicationError(\"Too many requests. Try again later.\")\ntry:\njson_res = res.json()\nexcept requests.exceptions.JSONDecodeError:\njson_res = {}\ndescription = json_res.get(\"error_description\", \"\")\nmsg = f\"{action} failed.\"\nif description:\nmsg += f\" {description}\"\nraise CommunicationError(msg)\n</code></pre>"},{"location":"reference/comms/auth/auth0/#comms.auth.auth0.Auth0.access_token","title":"<code>access_token</code>  <code>property</code>","text":"<p>Thread and process-safe access token retrieval</p>"},{"location":"reference/comms/auth/auth0/#comms.auth.auth0.Auth0.__check_token_email","title":"<code>__check_token_email(id_token_payload, email)</code>","text":"<p>Checks if the email provided by the user in the terminal matches the email found in the recieved id token.</p> Source code in <code>cli/medperf/comms/auth/auth0.py</code> <pre><code>def __check_token_email(self, id_token_payload, email):\n\"\"\"Checks if the email provided by the user in the terminal matches the\n    email found in the recieved id token.\"\"\"\nemail_in_token = id_token_payload[\"email\"]\nif email.lower() != email_in_token:\nraise CommunicationError(\n\"The email provided in the terminal does not match the one provided during login\"\n)\n</code></pre>"},{"location":"reference/comms/auth/auth0/#comms.auth.auth0.Auth0.__get_device_access_token","title":"<code>__get_device_access_token(device_code, polling_interval)</code>","text":"<p>Get the access token from the auth0 backend associated with the device code requested before. This function will keep polling the access token until the user completes the browser flow part of the authorization process.</p> <p>Parameters:</p> Name Type Description Default <code>device_code</code> <code>str</code> <p>A temporary device code requested by <code>__request_device_code</code></p> required <code>polling_interval</code> <code>float</code> <p>number of seconds to wait between each two polling requests</p> required <p>Returns:</p> Name Type Description <code>json_res</code> <code>dict</code> <p>the response of the successful request, containg the access/refresh tokens pair</p> <code>token_issued_at</code> <code>float</code> <p>the timestamp when the access token was issued</p> Source code in <code>cli/medperf/comms/auth/auth0.py</code> <pre><code>def __get_device_access_token(self, device_code, polling_interval):\n\"\"\"Get the access token from the auth0 backend associated with\n    the device code requested before. This function will keep polling\n    the access token until the user completes the browser flow part\n    of the authorization process.\n    Args:\n        device_code (str): A temporary device code requested by `__request_device_code`\n        polling_interval (float): number of seconds to wait between each two polling requests\n    Returns:\n        json_res (dict): the response of the successful request, containg the access/refresh tokens pair\n        token_issued_at (float): the timestamp when the access token was issued\n    \"\"\"\nurl = f\"https://{self.domain}/oauth/token\"\nheaders = {\"content-type\": \"application/x-www-form-urlencoded\"}\nbody = {\n\"grant_type\": \"urn:ietf:params:oauth:grant-type:device_code\",\n\"device_code\": device_code,\n\"client_id\": self.client_id,\n}\nwhile True:\ntime.sleep(polling_interval)\ntoken_issued_at = time.time()\nres = requests.post(url=url, headers=headers, data=body)\nif res.status_code == 200:\njson_res = res.json()\nreturn json_res, token_issued_at\ntry:\njson_res = res.json()\nexcept requests.exceptions.JSONDecodeError:\njson_res = {}\nerror = json_res.get(\"error\", None)\nif error not in [\"slow_down\", \"authorization_pending\"]:\nself.__raise_errors(res, \"Login\")\n</code></pre>"},{"location":"reference/comms/auth/auth0/#comms.auth.auth0.Auth0.__raise_errors","title":"<code>__raise_errors(res, action)</code>","text":"<p>log the failed request's response and raise errors.</p> <p>Parameters:</p> Name Type Description Default <code>res</code> <code>requests.Response</code> <p>the response of a failed request</p> required <code>action</code> <code>str</code> <p>a string for more informative error display</p> required Source code in <code>cli/medperf/comms/auth/auth0.py</code> <pre><code>def __raise_errors(self, res, action):\n\"\"\"log the failed request's response and raise errors.\n    Args:\n        res (requests.Response): the response of a failed request\n        action (str): a string for more informative error display\n        to the user.\n    \"\"\"\nlog_response_error(res)\nif res.status_code == 429:\nraise CommunicationError(\"Too many requests. Try again later.\")\ntry:\njson_res = res.json()\nexcept requests.exceptions.JSONDecodeError:\njson_res = {}\ndescription = json_res.get(\"error_description\", \"\")\nmsg = f\"{action} failed.\"\nif description:\nmsg += f\" {description}\"\nraise CommunicationError(msg)\n</code></pre>"},{"location":"reference/comms/auth/auth0/#comms.auth.auth0.Auth0.__refresh_access_token","title":"<code>__refresh_access_token(refresh_token)</code>","text":"<p>Retrieve and store a new access token using a refresh token. A new refresh token will also be retrieved and stored.</p> <p>Parameters:</p> Name Type Description Default <code>refresh_token</code> <code>str</code> <p>the refresh token</p> required <p>Returns:</p> Name Type Description <code>access_token</code> <code>str</code> <p>the new access token</p> Source code in <code>cli/medperf/comms/auth/auth0.py</code> <pre><code>def __refresh_access_token(self, refresh_token):\n\"\"\"Retrieve and store a new access token using a refresh token.\n    A new refresh token will also be retrieved and stored.\n    Args:\n        refresh_token (str): the refresh token\n    Returns:\n        access_token (str): the new access token\n    \"\"\"\nurl = f\"https://{self.domain}/oauth/token\"\nheaders = {\"content-type\": \"application/x-www-form-urlencoded\"}\nbody = {\n\"grant_type\": \"refresh_token\",\n\"client_id\": self.client_id,\n\"refresh_token\": refresh_token,\n}\ntoken_issued_at = time.time()\nlogging.debug(\"Refreshing access token.\")\nres = requests.post(url=url, headers=headers, data=body)\nif res.status_code != 200:\nself.__raise_errors(res, \"Token refresh\")\njson_res = res.json()\naccess_token = json_res[\"access_token\"]\nid_token = json_res[\"id_token\"]\nrefresh_token = json_res[\"refresh_token\"]\ntoken_expires_in = json_res[\"expires_in\"]\nid_token_payload = verify_token(id_token)\nset_credentials(\naccess_token,\nrefresh_token,\nid_token_payload,\ntoken_issued_at,\ntoken_expires_in,\n)\nreturn access_token\n</code></pre>"},{"location":"reference/comms/auth/auth0/#comms.auth.auth0.Auth0.__request_device_code","title":"<code>__request_device_code()</code>","text":"<p>Get a device code from the auth0 backend to be used for the authorization process</p> Source code in <code>cli/medperf/comms/auth/auth0.py</code> <pre><code>def __request_device_code(self):\n\"\"\"Get a device code from the auth0 backend to be used for the authorization process\"\"\"\nurl = f\"https://{self.domain}/oauth/device/code\"\nheaders = {\"content-type\": \"application/x-www-form-urlencoded\"}\nbody = {\n\"client_id\": self.client_id,\n\"audience\": self.audience,\n\"scope\": \"offline_access openid email\",\n}\nres = requests.post(url=url, headers=headers, data=body)\nif res.status_code != 200:\nself.__raise_errors(res, \"Login\")\nreturn res.json()\n</code></pre>"},{"location":"reference/comms/auth/auth0/#comms.auth.auth0.Auth0.login","title":"<code>login(email)</code>","text":"<p>Retrieves and stores an access token/refresh token pair from the auth0 backend using the device authorization flow.</p> <p>Parameters:</p> Name Type Description Default <code>email</code> <code>str</code> <p>user email. This will be used to validate that the received          id_token contains the same email address.</p> required Source code in <code>cli/medperf/comms/auth/auth0.py</code> <pre><code>def login(self, email):\n\"\"\"Retrieves and stores an access token/refresh token pair from the auth0\n    backend using the device authorization flow.\n    Args:\n        email (str): user email. This will be used to validate that the received\n                     id_token contains the same email address.\n    \"\"\"\ndevice_code_response = self.__request_device_code()\ndevice_code = device_code_response[\"device_code\"]\nuser_code = device_code_response[\"user_code\"]\nverification_uri_complete = device_code_response[\"verification_uri_complete\"]\ninterval = device_code_response[\"interval\"]\nconfig.ui.print(\n\"\\nPlease go to the following link to complete your login request:\\n\\t\"\n)\nconfig.ui.print_url(verification_uri_complete)\nconfig.ui.print(\n\"\\n\\nMake sure that you will be presented with the following code:\\n\\t\"\n)\nconfig.ui.print_code(user_code)\nconfig.ui.print(\"\\n\\n\")\nconfig.ui.print_warning(\n\"Keep this terminal open until you complete your login request. \"\n\"The command will exit on its own once you complete the request. \"\n\"If you wish to stop the login request anyway, press Ctrl+C.\"\n)\ntoken_response, token_issued_at = self.__get_device_access_token(\ndevice_code, interval\n)\naccess_token = token_response[\"access_token\"]\nid_token = token_response[\"id_token\"]\nrefresh_token = token_response[\"refresh_token\"]\ntoken_expires_in = token_response[\"expires_in\"]\nid_token_payload = verify_token(id_token)\nself.__check_token_email(id_token_payload, email)\nset_credentials(\naccess_token,\nrefresh_token,\nid_token_payload,\ntoken_issued_at,\ntoken_expires_in,\nlogin_event=True,\n)\n</code></pre>"},{"location":"reference/comms/auth/auth0/#comms.auth.auth0.Auth0.logout","title":"<code>logout()</code>","text":"<p>Logs out the user by revoking their refresh token and deleting the stored tokens.</p> Source code in <code>cli/medperf/comms/auth/auth0.py</code> <pre><code>def logout(self):\n\"\"\"Logs out the user by revoking their refresh token and deleting the\n    stored tokens.\"\"\"\ntry:\ncreds = read_credentials()\nexcept AuthenticationError:\nreturn\nrefresh_token = creds[\"refresh_token\"]\nurl = f\"https://{self.domain}/oauth/revoke\"\nheaders = {\"content-type\": \"application/json\"}\nbody = {\n\"client_id\": self.client_id,\n\"token\": refresh_token,\n}\nres = requests.post(url=url, headers=headers, json=body)\nif res.status_code != 200:\nself.__raise_errors(res, \"Logout\")\ndelete_credentials()\n</code></pre>"},{"location":"reference/comms/auth/interface/","title":"Interface","text":""},{"location":"reference/comms/auth/interface/#comms.auth.interface.Auth","title":"<code>Auth</code>","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>cli/medperf/comms/auth/interface.py</code> <pre><code>class Auth(ABC):\n@abstractmethod\ndef __init__(self):\n\"\"\"Initialize the class\"\"\"\n@abstractmethod\ndef login(self, email):\n\"\"\"Log in a user\"\"\"\n@abstractmethod\ndef logout(self):\n\"\"\"Log out a user\"\"\"\n@property\n@abstractmethod\ndef access_token(self):\n\"\"\"An access token to authorize requests to the MedPerf server\"\"\"\n</code></pre>"},{"location":"reference/comms/auth/interface/#comms.auth.interface.Auth.access_token","title":"<code>access_token</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>An access token to authorize requests to the MedPerf server</p>"},{"location":"reference/comms/auth/interface/#comms.auth.interface.Auth.__init__","title":"<code>__init__()</code>  <code>abstractmethod</code>","text":"<p>Initialize the class</p> Source code in <code>cli/medperf/comms/auth/interface.py</code> <pre><code>@abstractmethod\ndef __init__(self):\n\"\"\"Initialize the class\"\"\"\n</code></pre>"},{"location":"reference/comms/auth/interface/#comms.auth.interface.Auth.login","title":"<code>login(email)</code>  <code>abstractmethod</code>","text":"<p>Log in a user</p> Source code in <code>cli/medperf/comms/auth/interface.py</code> <pre><code>@abstractmethod\ndef login(self, email):\n\"\"\"Log in a user\"\"\"\n</code></pre>"},{"location":"reference/comms/auth/interface/#comms.auth.interface.Auth.logout","title":"<code>logout()</code>  <code>abstractmethod</code>","text":"<p>Log out a user</p> Source code in <code>cli/medperf/comms/auth/interface.py</code> <pre><code>@abstractmethod\ndef logout(self):\n\"\"\"Log out a user\"\"\"\n</code></pre>"},{"location":"reference/comms/auth/local/","title":"Local","text":""},{"location":"reference/comms/auth/local/#comms.auth.local.Local","title":"<code>Local</code>","text":"<p>             Bases: <code>Auth</code></p> Source code in <code>cli/medperf/comms/auth/local.py</code> <pre><code>class Local(Auth):\ndef __init__(self):\nwith open(config.local_tokens_path) as f:\nself.tokens = json.load(f)\ndef login(self, email):\n\"\"\"Retrieves and stores an access token from a local store json file.\n        Args:\n            email (str): user email.\n        \"\"\"\ntry:\naccess_token = self.tokens[email]\nexcept KeyError:\nraise InvalidArgumentError(\n\"The provided email does not exist for testing. \"\n\"Make sure you activated the right profile.\"\n)\nrefresh_token = \"refresh token\"\nid_token_payload = {\"email\": email}\ntoken_issued_at = 0\ntoken_expires_in = 10**10\nset_credentials(\naccess_token,\nrefresh_token,\nid_token_payload,\ntoken_issued_at,\ntoken_expires_in,\nlogin_event=True,\n)\ndef logout(self):\n\"\"\"Logs out the user by deleting the stored tokens.\"\"\"\ndelete_credentials()\n@property\ndef access_token(self):\n\"\"\"Reads and returns an access token of the currently logged\n        in user to be used for authorizing requests to the MedPerf server.\n        Returns:\n            access_token (str): the access token\n        \"\"\"\ncreds = read_credentials()\naccess_token = creds[\"access_token\"]\nreturn access_token\n</code></pre>"},{"location":"reference/comms/auth/local/#comms.auth.local.Local.access_token","title":"<code>access_token</code>  <code>property</code>","text":"<p>Reads and returns an access token of the currently logged in user to be used for authorizing requests to the MedPerf server.</p> <p>Returns:</p> Name Type Description <code>access_token</code> <code>str</code> <p>the access token</p>"},{"location":"reference/comms/auth/local/#comms.auth.local.Local.login","title":"<code>login(email)</code>","text":"<p>Retrieves and stores an access token from a local store json file.</p> <p>Parameters:</p> Name Type Description Default <code>email</code> <code>str</code> <p>user email.</p> required Source code in <code>cli/medperf/comms/auth/local.py</code> <pre><code>def login(self, email):\n\"\"\"Retrieves and stores an access token from a local store json file.\n    Args:\n        email (str): user email.\n    \"\"\"\ntry:\naccess_token = self.tokens[email]\nexcept KeyError:\nraise InvalidArgumentError(\n\"The provided email does not exist for testing. \"\n\"Make sure you activated the right profile.\"\n)\nrefresh_token = \"refresh token\"\nid_token_payload = {\"email\": email}\ntoken_issued_at = 0\ntoken_expires_in = 10**10\nset_credentials(\naccess_token,\nrefresh_token,\nid_token_payload,\ntoken_issued_at,\ntoken_expires_in,\nlogin_event=True,\n)\n</code></pre>"},{"location":"reference/comms/auth/local/#comms.auth.local.Local.logout","title":"<code>logout()</code>","text":"<p>Logs out the user by deleting the stored tokens.</p> Source code in <code>cli/medperf/comms/auth/local.py</code> <pre><code>def logout(self):\n\"\"\"Logs out the user by deleting the stored tokens.\"\"\"\ndelete_credentials()\n</code></pre>"},{"location":"reference/comms/auth/token_verifier/","title":"Token verifier","text":"<p>This module defines a wrapper around the existing token verifier in auth0-python library. The library is designed to cache public keys in memory. Since our client is ephemeral, we wrapped the library's <code>JwksFetcher</code> to cache keys in the filesystem storage, and wrapped the library's signature verifier to use this new <code>JwksFetcher</code></p>"},{"location":"reference/comms/entity_resources/resources/","title":"Resources","text":"<p>This module downloads files from the internet. It provides a set of functions to download common files that are necessary for workflow executions and are not on the MedPerf server. An example of such files is model weights of a Model MLCube.</p> <p>This module takes care of validating the integrity of the downloaded file if a hash was specified when requesting the file. It also returns the hash of the downloaded file, which can be the original specified hash or the calculated hash of the freshly downloaded file if no hash was specified.</p> <p>Additionally, to avoid unnecessary downloads, an existing file will not be re-downloaded.</p>"},{"location":"reference/comms/entity_resources/resources/#comms.entity_resources.resources.get_benchmark_demo_dataset","title":"<code>get_benchmark_demo_dataset(url, expected_hash=None)</code>","text":"<p>Downloads and extracts a demo dataset. If the hash is provided, the file's integrity will be checked upon download.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL where the compressed demo dataset file can be downloaded.</p> required <code>expected_hash</code> <code>str</code> <p>expected hash of the downloaded file</p> <code>None</code> <p>Returns:</p> Name Type Description <code>output_path</code> <code>str</code> <p>location where the uncompressed demo dataset is stored locally.</p> <code>hash_value</code> <code>str</code> <p>The hash of the downloaded tarball file</p> Source code in <code>cli/medperf/comms/entity_resources/resources.py</code> <pre><code>def get_benchmark_demo_dataset(url: str, expected_hash: str = None) -&gt; str:\n\"\"\"Downloads and extracts a demo dataset. If the hash is provided,\n    the file's integrity will be checked upon download.\n    Args:\n        url (str): URL where the compressed demo dataset file can be downloaded.\n        expected_hash (str, optional): expected hash of the downloaded file\n    Returns:\n        output_path (str): location where the uncompressed demo dataset is stored locally.\n        hash_value (str): The hash of the downloaded tarball file\n    \"\"\"\n# TODO: at some point maybe it is better to download demo datasets in\n# their benchmark folder. Doing this, we should then modify\n# the compatibility test command and remove the option of directly passing\n# demo datasets. This would look cleaner.\n# Possible cons: if multiple benchmarks use the same demo dataset.\ndemo_storage = config.demo_datasets_folder\nif expected_hash:\n# If the folder exists, return\ndemo_dataset_folder = os.path.join(demo_storage, expected_hash)\nif os.path.exists(demo_dataset_folder):\nreturn demo_dataset_folder, expected_hash\n# make sure files are uncompressed while in tmp storage, to avoid any clutter\n# objects if uncompression fails for some reason.\ntmp_output_folder = generate_tmp_path()\noutput_tarball_path = os.path.join(tmp_output_folder, config.tarball_filename)\nhash_value = download_resource(url, output_tarball_path, expected_hash)\nuntar(output_tarball_path)\ndemo_dataset_folder = os.path.join(demo_storage, hash_value)\nif os.path.exists(demo_dataset_folder):\n# handle the possibility of having clutter uncompressed files\nremove_path(demo_dataset_folder)\nos.rename(tmp_output_folder, demo_dataset_folder)\nreturn demo_dataset_folder, hash_value\n</code></pre>"},{"location":"reference/comms/entity_resources/resources/#comms.entity_resources.resources.get_cube","title":"<code>get_cube(url, cube_path, expected_hash=None)</code>","text":"<p>Downloads and writes a cube mlcube.yaml file</p> Source code in <code>cli/medperf/comms/entity_resources/resources.py</code> <pre><code>def get_cube(url: str, cube_path: str, expected_hash: str = None):\n\"\"\"Downloads and writes a cube mlcube.yaml file\"\"\"\noutput_path = os.path.join(cube_path, config.cube_filename)\nreturn _get_regular_file(url, output_path, expected_hash)\n</code></pre>"},{"location":"reference/comms/entity_resources/resources/#comms.entity_resources.resources.get_cube_additional","title":"<code>get_cube_additional(url, cube_path, expected_tarball_hash=None)</code>","text":"<p>Retrieves additional files of an MLCube. The additional files will be in a compressed tarball file. The function will additionally extract this file.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL where the additional_files.tar.gz file can be downloaded.</p> required <code>cube_path</code> <code>str</code> <p>Cube location.</p> required <code>expected_tarball_hash</code> <code>str</code> <p>expected hash of tarball file</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tarball_hash</code> <code>str</code> <p>The hash of the downloaded tarball file</p> Source code in <code>cli/medperf/comms/entity_resources/resources.py</code> <pre><code>def get_cube_additional(\nurl: str,\ncube_path: str,\nexpected_tarball_hash: str = None,\n) -&gt; str:\n\"\"\"Retrieves additional files of an MLCube. The additional files\n    will be in a compressed tarball file. The function will additionally\n    extract this file.\n    Args:\n        url (str): URL where the additional_files.tar.gz file can be downloaded.\n        cube_path (str): Cube location.\n        expected_tarball_hash (str, optional): expected hash of tarball file\n    Returns:\n        tarball_hash (str): The hash of the downloaded tarball file\n    \"\"\"\nadditional_files_folder = os.path.join(cube_path, config.additional_path)\nmlcube_cache_file = os.path.join(cube_path, config.mlcube_cache_file)\nif not _should_get_cube_additional(\nadditional_files_folder, expected_tarball_hash, mlcube_cache_file\n):\nreturn additional_files_folder, expected_tarball_hash\n# Download the additional files. Make sure files are extracted in tmp storage\n# to avoid any clutter objects if uncompression fails for some reason.\ntmp_output_folder = generate_tmp_path()\noutput_tarball_path = os.path.join(tmp_output_folder, config.tarball_filename)\ntarball_hash = download_resource(url, output_tarball_path, expected_tarball_hash)\nuntar(output_tarball_path)\nparent_folder = os.path.dirname(os.path.normpath(additional_files_folder))\nos.makedirs(parent_folder, exist_ok=True)\nif os.path.exists(additional_files_folder):\n# handle the possibility of having clutter uncompressed files\nremove_path(additional_files_folder)\nos.rename(tmp_output_folder, additional_files_folder)\n# Store the downloaded tarball hash to be used later for verifying that the\n# local cache is up to date\nwith open(mlcube_cache_file, \"w\") as f:  # assumes parent folder already exists\ncontents = {\"additional_files_cached_hash\": tarball_hash}\nyaml.dump(contents, f)\nreturn additional_files_folder, tarball_hash\n</code></pre>"},{"location":"reference/comms/entity_resources/resources/#comms.entity_resources.resources.get_cube_image","title":"<code>get_cube_image(url, hash_value=None)</code>","text":"<p>Retrieves and stores the image file from the server. Stores images on a shared location, and retrieves a cached image by hash if found locally. Creates a symbolic link to the cube storage.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL where the image file can be downloaded.</p> required <code>cube_path</code> <code>str</code> <p>Path to cube.</p> required <code>hash_value</code> <code>(str, Optional)</code> <p>File hash to store under shared storage. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>image_cube_file</code> <code>str</code> <p>Location where the image file is stored locally.</p> <code>hash_value</code> <code>str</code> <p>The hash of the downloaded file</p> Source code in <code>cli/medperf/comms/entity_resources/resources.py</code> <pre><code>def get_cube_image(url: str, hash_value: str = None) -&gt; str:\n\"\"\"Retrieves and stores the image file from the server. Stores images\n    on a shared location, and retrieves a cached image by hash if found locally.\n    Creates a symbolic link to the cube storage.\n    Args:\n        url (str): URL where the image file can be downloaded.\n        cube_path (str): Path to cube.\n        hash_value (str, Optional): File hash to store under shared storage. Defaults to None.\n    Returns:\n        image_cube_file: Location where the image file is stored locally.\n        hash_value (str): The hash of the downloaded file\n    \"\"\"\nif hash_value:\noutput_path = os.path.join(config.images_folder, hash_value)\nreturn _get_regular_file(url, output_path, hash_value)\n# No hash provided, we need to download the file\ntmp_output_path = generate_tmp_path()\nhash_value = download_resource(url, tmp_output_path)\nimage_path = os.path.join(config.images_folder, hash_value)\nshutil.move(tmp_output_path, image_path)\nreturn image_path, hash_value\n</code></pre>"},{"location":"reference/comms/entity_resources/resources/#comms.entity_resources.resources.get_cube_params","title":"<code>get_cube_params(url, cube_path, expected_hash=None)</code>","text":"<p>Downloads and writes a cube parameters.yaml file</p> Source code in <code>cli/medperf/comms/entity_resources/resources.py</code> <pre><code>def get_cube_params(url: str, cube_path: str, expected_hash: str = None):\n\"\"\"Downloads and writes a cube parameters.yaml file\"\"\"\noutput_path = os.path.join(cube_path, config.workspace_path, config.params_filename)\nreturn _get_regular_file(url, output_path, expected_hash)\n</code></pre>"},{"location":"reference/comms/entity_resources/utils/","title":"Utils","text":""},{"location":"reference/comms/entity_resources/utils/#comms.entity_resources.utils.__parse_resource","title":"<code>__parse_resource(resource)</code>","text":"<p>Parses a resource string and returns its identifier and the source class it can be downloaded from. The function iterates over all supported sources and checks which one accepts this resource. A resource is a string that should match a certain pattern to be downloaded by a certain resource.</p> <p>If the resource pattern does not correspond to any supported source, the function raises an <code>InvalidArgumentError</code></p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>str</code> <p>The resource string. Must be in the form : required Source code in <code>cli/medperf/comms/entity_resources/utils.py</code> <pre><code>def __parse_resource(resource: str):\n\"\"\"Parses a resource string and returns its identifier and the source class\n    it can be downloaded from.\n    The function iterates over all supported sources and checks which one accepts\n    this resource. A resource is a string that should match a certain pattern to be\n    downloaded by a certain resource.\n    If the resource pattern does not correspond to any supported source, the\n    function raises an `InvalidArgumentError`\n    Args:\n        resource (str): The resource string. Must be in the form &lt;source_prefix&gt;:&lt;resource_identifier&gt;\n        or a url. The later case will be interpreted as a direct download link.\n    \"\"\"\nfor source_class in supported_sources:\nresource_identifier = source_class.validate_resource(resource)\nif resource_identifier:\nreturn source_class, resource_identifier\n# In this case the input format is not compatible with any source\nmsg = f\"\"\"Invalid resource input: {resource}. A Resource must be a url or\n    in the following format: '&lt;source_prefix&gt;:&lt;resource_identifier&gt;'. Run\n    `medperf container submit --help` for more details.\"\"\"\nraise InvalidArgumentError(msg)\n</code></pre>"},{"location":"reference/comms/entity_resources/utils/#comms.entity_resources.utils.download_resource","title":"<code>download_resource(resource, output_path, expected_hash=None)</code>","text":"<p>Downloads a resource/file from the internet. Passing a hash is optional. If hash is provided, the downloaded file's hash will be checked and an error will be raised if it is incorrect.</p> <p>Upon success, the function returns the hash of the downloaded file.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>str</code> <p>The resource string. Must be in the form : required <code>output_path</code> <code>str</code> <p>The path to download the resource to</p> required <code>expected_hash</code> <code>(optional, str)</code> <p>The expected hash of the file to be downloaded</p> <code>None</code> <p>Returns:</p> Type Description <p>The hash of the downloaded file (or existing file)</p> Source code in <code>cli/medperf/comms/entity_resources/utils.py</code> <pre><code>def download_resource(\nresource: str, output_path: str, expected_hash: Optional[str] = None\n):\n\"\"\"Downloads a resource/file from the internet. Passing a hash is optional.\n    If hash is provided, the downloaded file's hash will be checked and an error\n    will be raised if it is incorrect.\n    Upon success, the function returns the hash of the downloaded file.\n    Args:\n        resource (str): The resource string. Must be in the form &lt;source_prefix&gt;:&lt;resource_identifier&gt;\n        or a url.\n        output_path (str): The path to download the resource to\n        expected_hash (optional, str): The expected hash of the file to be downloaded\n    Returns:\n        The hash of the downloaded file (or existing file)\n    \"\"\"\ntmp_output_path = tmp_download_resource(resource)\ncalculated_hash = get_file_hash(tmp_output_path)\nif expected_hash and calculated_hash != expected_hash:\nlogging.debug(f\"{resource}: Expected {expected_hash}, found {calculated_hash}.\")\nraise InvalidEntityError(f\"Hash mismatch: {resource}\")\nto_permanent_path(tmp_output_path, output_path)\nreturn calculated_hash\n</code></pre>"},{"location":"reference/comms/entity_resources/utils/#comms.entity_resources.utils.tmp_download_resource","title":"<code>tmp_download_resource(resource)</code>","text":"<p>Downloads a resource to the temporary storage.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>str</code> <p>The resource string. Must be in the form : required <p>Returns:</p> Name Type Description <code>tmp_output_path</code> <code>str</code> <p>The location where the resource was downloaded</p> Source code in <code>cli/medperf/comms/entity_resources/utils.py</code> <pre><code>def tmp_download_resource(resource):\n\"\"\"Downloads a resource to the temporary storage.\n    Args:\n        resource (str): The resource string. Must be in the form &lt;source_prefix&gt;:&lt;resource_identifier&gt;\n        or a url.\n    Returns:\n        tmp_output_path (str): The location where the resource was downloaded\n    \"\"\"\ntmp_output_path = generate_tmp_path()\nsource_class, resource_identifier = __parse_resource(resource)\nsource = source_class()\nsource.authenticate()\nsource.download(resource_identifier, tmp_output_path)\nreturn tmp_output_path\n</code></pre>"},{"location":"reference/comms/entity_resources/utils/#comms.entity_resources.utils.to_permanent_path","title":"<code>to_permanent_path(tmp_output_path, output_path)</code>","text":"<p>Writes a file from the temporary storage to the desired output path.</p> Source code in <code>cli/medperf/comms/entity_resources/utils.py</code> <pre><code>def to_permanent_path(tmp_output_path, output_path):\n\"\"\"Writes a file from the temporary storage to the desired output path.\"\"\"\noutput_folder = os.path.dirname(os.path.abspath(output_path))\nos.makedirs(output_folder, exist_ok=True)\nos.rename(tmp_output_path, output_path)\n</code></pre>"},{"location":"reference/comms/entity_resources/sources/direct/","title":"Direct","text":""},{"location":"reference/comms/entity_resources/sources/direct/#comms.entity_resources.sources.direct.DirectLinkSource","title":"<code>DirectLinkSource</code>","text":"<p>             Bases: <code>BaseSource</code></p> Source code in <code>cli/medperf/comms/entity_resources/sources/direct.py</code> <pre><code>class DirectLinkSource(BaseSource):\nprefix = \"direct:\"\n@classmethod\ndef validate_resource(cls, value: str):\n\"\"\"This class expects a resource string of the form\n        `direct:&lt;URL&gt;` or only a URL.\n        Args:\n            resource (str): the resource string\n        Returns:\n            (str|None): The URL if the pattern matches, else None\n        \"\"\"\nprefix = cls.prefix\nif value.startswith(prefix):\nprefix_len = len(prefix)\nvalue = value[prefix_len:]\nif validators.url(value):\nreturn value\ndef __init__(self):\npass\ndef authenticate(self):\npass\ndef __download_once(self, resource_identifier: str, output_path: str):\n\"\"\"Downloads a direct-download-link file by streaming its contents. source:\n        https://stackoverflow.com/questions/16694907/download-large-file-in-python-with-requests\n        \"\"\"\nwith requests.get(resource_identifier, stream=True) as res:\nif res.status_code != 200:\nlog_response_error(res)\nmsg = (\n\"There was a problem retrieving the specified file at \"\n+ resource_identifier\n)\nraise CommunicationRetrievalError(msg)\nwith open(output_path, \"wb\") as f:\nfor chunk in res.iter_content(chunk_size=config.ddl_stream_chunk_size):\n# NOTE: if the response is chunk-encoded, this may not work\n# check whether this is common.\nf.write(chunk)\ndef download(self, resource_identifier: str, output_path: str):\n\"\"\"Downloads a direct-download-link file with multiple attempts. This is\n        done due to facing transient network failure from some direct download\n        link servers.\"\"\"\nattempt = 0\nwhile attempt &lt; config.ddl_max_redownload_attempts:\ntry:\nself.__download_once(resource_identifier, output_path)\nreturn\nexcept CommunicationRetrievalError:\nif os.path.exists(output_path):\nremove_path(output_path)\nattempt += 1\nraise CommunicationRetrievalError(f\"Could not download {resource_identifier}\")\n</code></pre>"},{"location":"reference/comms/entity_resources/sources/direct/#comms.entity_resources.sources.direct.DirectLinkSource.__download_once","title":"<code>__download_once(resource_identifier, output_path)</code>","text":"<p>Downloads a direct-download-link file by streaming its contents. source: https://stackoverflow.com/questions/16694907/download-large-file-in-python-with-requests</p> Source code in <code>cli/medperf/comms/entity_resources/sources/direct.py</code> <pre><code>def __download_once(self, resource_identifier: str, output_path: str):\n\"\"\"Downloads a direct-download-link file by streaming its contents. source:\n    https://stackoverflow.com/questions/16694907/download-large-file-in-python-with-requests\n    \"\"\"\nwith requests.get(resource_identifier, stream=True) as res:\nif res.status_code != 200:\nlog_response_error(res)\nmsg = (\n\"There was a problem retrieving the specified file at \"\n+ resource_identifier\n)\nraise CommunicationRetrievalError(msg)\nwith open(output_path, \"wb\") as f:\nfor chunk in res.iter_content(chunk_size=config.ddl_stream_chunk_size):\n# NOTE: if the response is chunk-encoded, this may not work\n# check whether this is common.\nf.write(chunk)\n</code></pre>"},{"location":"reference/comms/entity_resources/sources/direct/#comms.entity_resources.sources.direct.DirectLinkSource.download","title":"<code>download(resource_identifier, output_path)</code>","text":"<p>Downloads a direct-download-link file with multiple attempts. This is done due to facing transient network failure from some direct download link servers.</p> Source code in <code>cli/medperf/comms/entity_resources/sources/direct.py</code> <pre><code>def download(self, resource_identifier: str, output_path: str):\n\"\"\"Downloads a direct-download-link file with multiple attempts. This is\n    done due to facing transient network failure from some direct download\n    link servers.\"\"\"\nattempt = 0\nwhile attempt &lt; config.ddl_max_redownload_attempts:\ntry:\nself.__download_once(resource_identifier, output_path)\nreturn\nexcept CommunicationRetrievalError:\nif os.path.exists(output_path):\nremove_path(output_path)\nattempt += 1\nraise CommunicationRetrievalError(f\"Could not download {resource_identifier}\")\n</code></pre>"},{"location":"reference/comms/entity_resources/sources/direct/#comms.entity_resources.sources.direct.DirectLinkSource.validate_resource","title":"<code>validate_resource(value)</code>  <code>classmethod</code>","text":"<p>This class expects a resource string of the form <code>direct:&lt;URL&gt;</code> or only a URL.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>str</code> <p>the resource string</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>The URL if the pattern matches, else None</p> Source code in <code>cli/medperf/comms/entity_resources/sources/direct.py</code> <pre><code>@classmethod\ndef validate_resource(cls, value: str):\n\"\"\"This class expects a resource string of the form\n    `direct:&lt;URL&gt;` or only a URL.\n    Args:\n        resource (str): the resource string\n    Returns:\n        (str|None): The URL if the pattern matches, else None\n    \"\"\"\nprefix = cls.prefix\nif value.startswith(prefix):\nprefix_len = len(prefix)\nvalue = value[prefix_len:]\nif validators.url(value):\nreturn value\n</code></pre>"},{"location":"reference/comms/entity_resources/sources/source/","title":"Source","text":""},{"location":"reference/comms/entity_resources/sources/source/#comms.entity_resources.sources.source.BaseSource","title":"<code>BaseSource</code>","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>cli/medperf/comms/entity_resources/sources/source.py</code> <pre><code>class BaseSource(ABC):\n@classmethod\n@abstractmethod\ndef validate_resource(cls, value: str):\n\"\"\"Checks if an input resource can be downloaded by this class\"\"\"\n@abstractmethod\ndef __init__(self):\n\"\"\"Initialize\"\"\"\n@abstractmethod\ndef authenticate(self):\n\"\"\"Authenticates with the source server, if needed.\"\"\"\n@abstractmethod\ndef download(self, resource_identifier: str, output_path: str):\n\"\"\"Downloads the requested resource to the specified location\n        Args:\n            resource_identifier (str): The identifier that is used to download\n            the resource (e.g. URL, asset ID, ...) It is the parsed output\n            by `validate_resource`\n            output_path (str): The path to download the resource to\n        \"\"\"\n</code></pre>"},{"location":"reference/comms/entity_resources/sources/source/#comms.entity_resources.sources.source.BaseSource.__init__","title":"<code>__init__()</code>  <code>abstractmethod</code>","text":"<p>Initialize</p> Source code in <code>cli/medperf/comms/entity_resources/sources/source.py</code> <pre><code>@abstractmethod\ndef __init__(self):\n\"\"\"Initialize\"\"\"\n</code></pre>"},{"location":"reference/comms/entity_resources/sources/source/#comms.entity_resources.sources.source.BaseSource.authenticate","title":"<code>authenticate()</code>  <code>abstractmethod</code>","text":"<p>Authenticates with the source server, if needed.</p> Source code in <code>cli/medperf/comms/entity_resources/sources/source.py</code> <pre><code>@abstractmethod\ndef authenticate(self):\n\"\"\"Authenticates with the source server, if needed.\"\"\"\n</code></pre>"},{"location":"reference/comms/entity_resources/sources/source/#comms.entity_resources.sources.source.BaseSource.download","title":"<code>download(resource_identifier, output_path)</code>  <code>abstractmethod</code>","text":"<p>Downloads the requested resource to the specified location</p> <p>Parameters:</p> Name Type Description Default <code>resource_identifier</code> <code>str</code> <p>The identifier that is used to download</p> required <code>output_path</code> <code>str</code> <p>The path to download the resource to</p> required Source code in <code>cli/medperf/comms/entity_resources/sources/source.py</code> <pre><code>@abstractmethod\ndef download(self, resource_identifier: str, output_path: str):\n\"\"\"Downloads the requested resource to the specified location\n    Args:\n        resource_identifier (str): The identifier that is used to download\n        the resource (e.g. URL, asset ID, ...) It is the parsed output\n        by `validate_resource`\n        output_path (str): The path to download the resource to\n    \"\"\"\n</code></pre>"},{"location":"reference/comms/entity_resources/sources/source/#comms.entity_resources.sources.source.BaseSource.validate_resource","title":"<code>validate_resource(value)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Checks if an input resource can be downloaded by this class</p> Source code in <code>cli/medperf/comms/entity_resources/sources/source.py</code> <pre><code>@classmethod\n@abstractmethod\ndef validate_resource(cls, value: str):\n\"\"\"Checks if an input resource can be downloaded by this class\"\"\"\n</code></pre>"},{"location":"reference/comms/entity_resources/sources/synapse/","title":"Synapse","text":""},{"location":"reference/comms/entity_resources/sources/synapse/#comms.entity_resources.sources.synapse.SynapseSource","title":"<code>SynapseSource</code>","text":"<p>             Bases: <code>BaseSource</code></p> Source code in <code>cli/medperf/comms/entity_resources/sources/synapse.py</code> <pre><code>class SynapseSource(BaseSource):\nprefix = \"synapse:\"\n@classmethod\ndef validate_resource(cls, value: str):\n\"\"\"This class expects a resource string of the form\n        `synapse:&lt;synapse_id&gt;`, where &lt;synapse_id&gt; is in the form `syn&lt;Integer&gt;`.\n        Args:\n            resource (str): the resource string\n        Returns:\n            (str|None): The synapse ID if the pattern matches, else None\n        \"\"\"\nprefix = cls.prefix\nif not value.startswith(prefix):\nreturn\nprefix_len = len(prefix)\nvalue = value[prefix_len:]\nif re.match(r\"syn\\d+$\", value):\nreturn value\ndef __init__(self):\nself.client = synapseclient.Synapse()\ndef authenticate(self):\ntry:\nself.client.login(silent=True)\nexcept SynapseNoCredentialsError:\nmsg = \"There was an attempt to download resources from the Synapse \"\nmsg += \"platform, but couldn't find Synapse credentials.\"\nmsg += \"\\nDid you run 'medperf auth synapse_login' before?\"\nraise CommunicationAuthenticationError(msg)\ndef download(self, resource_identifier: str, output_path: str):\n# we can specify target folder only. File name depends on how it was stored\ndownload_location = os.path.dirname(output_path)\nos.makedirs(download_location, exist_ok=True)\ntry:\nresource_file = self.client.get(\nresource_identifier, downloadLocation=download_location\n)\nexcept (SynapseHTTPError, SynapseUnmetAccessRestrictions) as e:\nraise CommunicationRetrievalError(str(e))\nresource_path = os.path.join(download_location, resource_file.name)\n# synapseclient may only throw a warning in some cases\n# (e.g. read permissions but no download permissions)\nif not os.path.exists(resource_path):\nraise CommunicationRetrievalError(\n\"There was a problem retrieving a file from Synapse\"\n)\nshutil.move(resource_path, output_path)\n</code></pre>"},{"location":"reference/comms/entity_resources/sources/synapse/#comms.entity_resources.sources.synapse.SynapseSource.validate_resource","title":"<code>validate_resource(value)</code>  <code>classmethod</code>","text":"<p>This class expects a resource string of the form <code>synapse:&lt;synapse_id&gt;</code>, where  is in the form <code>syn&lt;Integer&gt;</code>. <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>str</code> <p>the resource string</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>The synapse ID if the pattern matches, else None</p> Source code in <code>cli/medperf/comms/entity_resources/sources/synapse.py</code> <pre><code>@classmethod\ndef validate_resource(cls, value: str):\n\"\"\"This class expects a resource string of the form\n    `synapse:&lt;synapse_id&gt;`, where &lt;synapse_id&gt; is in the form `syn&lt;Integer&gt;`.\n    Args:\n        resource (str): the resource string\n    Returns:\n        (str|None): The synapse ID if the pattern matches, else None\n    \"\"\"\nprefix = cls.prefix\nif not value.startswith(prefix):\nreturn\nprefix_len = len(prefix)\nvalue = value[prefix_len:]\nif re.match(r\"syn\\d+$\", value):\nreturn value\n</code></pre>"},{"location":"reference/config_management/config_management/","title":"Config management","text":""},{"location":"reference/containers/parsers/factory/","title":"Factory","text":""},{"location":"reference/containers/parsers/mlcube/","title":"Mlcube","text":""},{"location":"reference/containers/parsers/parser/","title":"Parser","text":""},{"location":"reference/containers/parsers/simple_container/","title":"Simple container","text":""},{"location":"reference/containers/runners/docker_runner/","title":"Docker runner","text":""},{"location":"reference/containers/runners/docker_utils/","title":"Docker utils","text":""},{"location":"reference/containers/runners/factory/","title":"Factory","text":""},{"location":"reference/containers/runners/runner/","title":"Runner","text":""},{"location":"reference/containers/runners/singularity_runner/","title":"Singularity runner","text":""},{"location":"reference/containers/runners/singularity_utils/","title":"Singularity utils","text":""},{"location":"reference/containers/runners/utils/","title":"Utils","text":""},{"location":"reference/entities/aggregator/","title":"Aggregator","text":""},{"location":"reference/entities/aggregator/#entities.aggregator.Aggregator","title":"<code>Aggregator</code>","text":"<p>             Bases: <code>Entity</code>, <code>MedperfSchema</code></p> <p>Class representing a compatibility test report entry</p> <p>A test report is comprised of the components of a test execution: - data used, which can be:     - a demo aggregator url and its hash, or     - a raw data path and its labels path, or     - a prepared aggregator uid - Data preparation cube if the data used was not already prepared - model cube - evaluator cube - results</p> Source code in <code>cli/medperf/entities/aggregator.py</code> <pre><code>class Aggregator(Entity, MedperfSchema):\n\"\"\"\n    Class representing a compatibility test report entry\n    A test report is comprised of the components of a test execution:\n    - data used, which can be:\n        - a demo aggregator url and its hash, or\n        - a raw data path and its labels path, or\n        - a prepared aggregator uid\n    - Data preparation cube if the data used was not already prepared\n    - model cube\n    - evaluator cube\n    - results\n    \"\"\"\nmetadata: dict = {}\nconfig: dict\naggregation_mlcube: int\n@staticmethod\ndef get_type():\nreturn \"aggregator\"\n@staticmethod\ndef get_storage_path():\nreturn config.aggregators_folder\n@staticmethod\ndef get_comms_retriever():\nreturn config.comms.get_aggregator\n@staticmethod\ndef get_metadata_filename():\nreturn config.agg_file\n@staticmethod\ndef get_comms_uploader():\nreturn config.comms.upload_aggregator\n@validator(\"config\", pre=True, always=True)\ndef check_config(cls, v, *, values, **kwargs):\nkeys = set(v.keys())\nallowed_keys = {\n\"address\",\n\"port\",\n}\nif keys != allowed_keys:\nraise ValueError(\"config must contain two keys only: address and port\")\nreturn v\ndef __init__(self, *args, **kwargs):\nsuper().__init__(*args, **kwargs)\nself.address = self.config[\"address\"]\nself.port = self.config[\"port\"]\nself.config_path = os.path.join(self.path, config.agg_config_file)\n@property\ndef local_id(self):\nreturn self.name\n@classmethod\ndef from_experiment(cls, training_exp_uid: int) -&gt; \"Aggregator\":\nmeta = config.comms.get_experiment_aggregator(training_exp_uid)\nagg = cls(**meta)\nagg.write()\nreturn agg\n@classmethod\ndef remote_prefilter(cls, filters: dict) -&gt; callable:\n\"\"\"Applies filtering logic that must be done before retrieving remote entities\n        Args:\n            filters (dict): filters to apply\n        Returns:\n            callable: A function for retrieving remote entities with the applied prefilters\n        \"\"\"\ncomms_fn = config.comms.get_aggregators\nif \"owner\" in filters and filters[\"owner\"] == get_medperf_user_data()[\"id\"]:\ncomms_fn = config.comms.get_user_aggregators\nreturn comms_fn\ndef prepare_config(self):\nwith open(self.config_path, \"w\") as f:\nyaml.dump(self.config, f)\ndef display_dict(self):\nreturn {\n\"UID\": self.identifier,\n\"Name\": self.name,\n\"Address\": self.address,\n\"Container\": int(self.aggregation_mlcube),\n\"Port\": self.port,\n\"Created At\": self.created_at,\n\"Registered\": self.is_registered,\n}\n</code></pre>"},{"location":"reference/entities/aggregator/#entities.aggregator.Aggregator.remote_prefilter","title":"<code>remote_prefilter(filters)</code>  <code>classmethod</code>","text":"<p>Applies filtering logic that must be done before retrieving remote entities</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict</code> <p>filters to apply</p> required <p>Returns:</p> Name Type Description <code>callable</code> <code>callable</code> <p>A function for retrieving remote entities with the applied prefilters</p> Source code in <code>cli/medperf/entities/aggregator.py</code> <pre><code>@classmethod\ndef remote_prefilter(cls, filters: dict) -&gt; callable:\n\"\"\"Applies filtering logic that must be done before retrieving remote entities\n    Args:\n        filters (dict): filters to apply\n    Returns:\n        callable: A function for retrieving remote entities with the applied prefilters\n    \"\"\"\ncomms_fn = config.comms.get_aggregators\nif \"owner\" in filters and filters[\"owner\"] == get_medperf_user_data()[\"id\"]:\ncomms_fn = config.comms.get_user_aggregators\nreturn comms_fn\n</code></pre>"},{"location":"reference/entities/benchmark/","title":"Benchmark","text":""},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark","title":"<code>Benchmark</code>","text":"<p>             Bases: <code>Entity</code>, <code>ApprovableSchema</code>, <code>DeployableSchema</code></p> <p>Class representing a Benchmark</p> <p>a benchmark is a bundle of assets that enables quantitative measurement of the performance of AI models for a specific clinical problem. A Benchmark instance contains information regarding how to prepare datasets for execution, as well as what models to run and how to evaluate them.</p> Source code in <code>cli/medperf/entities/benchmark.py</code> <pre><code>class Benchmark(Entity, ApprovableSchema, DeployableSchema):\n\"\"\"\n    Class representing a Benchmark\n    a benchmark is a bundle of assets that enables quantitative\n    measurement of the performance of AI models for a specific\n    clinical problem. A Benchmark instance contains information\n    regarding how to prepare datasets for execution, as well as\n    what models to run and how to evaluate them.\n    \"\"\"\ndescription: Optional[str] = Field(None, max_length=20)\ndocs_url: Optional[HttpUrl]\ndemo_dataset_tarball_url: str\ndemo_dataset_tarball_hash: Optional[str]\ndemo_dataset_generated_uid: Optional[str]\ndata_preparation_mlcube: int\nreference_model_mlcube: int\ndata_evaluator_mlcube: int\nmetadata: dict = {}\nuser_metadata: dict = {}\nis_active: bool = True\ndataset_auto_approval_allow_list: list[str] = []\ndataset_auto_approval_mode: str = \"NEVER\"\nmodel_auto_approval_allow_list: list[str] = []\nmodel_auto_approval_mode: str = \"NEVER\"\n@staticmethod\ndef get_type():\nreturn \"benchmark\"\n@staticmethod\ndef get_storage_path():\nreturn config.benchmarks_folder\n@staticmethod\ndef get_comms_retriever():\nreturn config.comms.get_benchmark\n@staticmethod\ndef get_metadata_filename():\nreturn config.benchmarks_filename\n@staticmethod\ndef get_comms_uploader():\nreturn config.comms.upload_benchmark\ndef __init__(self, *args, **kwargs):\n\"\"\"Creates a new benchmark instance\n        Args:\n            bmk_desc (Union[dict, BenchmarkModel]): Benchmark instance description\n        \"\"\"\nsuper().__init__(*args, **kwargs)\n@property\ndef local_id(self):\nreturn self.name\n@staticmethod\ndef remote_prefilter(filters: dict) -&gt; callable:\n\"\"\"Applies filtering logic that must be done before retrieving remote entities\n        Args:\n            filters (dict): filters to apply\n        Returns:\n            callable: A function for retrieving remote entities with the applied prefilters\n        \"\"\"\ncomms_fn = config.comms.get_benchmarks\nif \"owner\" in filters and filters[\"owner\"] == get_medperf_user_data()[\"id\"]:\ncomms_fn = config.comms.get_user_benchmarks\nreturn comms_fn\n@classmethod\ndef get_models_uids(cls, benchmark_uid: int) -&gt; List[int]:\n\"\"\"Retrieves the list of models associated to the benchmark\n        Args:\n            benchmark_uid (int): UID of the benchmark.\n        Returns:\n            List[int]: List of mlcube uids\n        \"\"\"\nassociations = get_experiment_associations(\nexperiment_id=benchmark_uid,\nexperiment_type=\"benchmark\",\ncomponent_type=\"model_mlcube\",\napproval_status=\"APPROVED\",\n)\nmodels_uids = [assoc[\"model_mlcube\"] for assoc in associations]\nreturn models_uids\n@classmethod\ndef get_datasets_with_users(cls, benchmark_uid: int) -&gt; List[dict]:\n\"\"\"Retrieves the list of datasets and their owner info, associated to the benchmark\n        Args:\n            benchmark_uid (int): UID of the benchmark.\n        Returns:\n            List[dict]: List of dicts of dataset IDs with their owner info\n        \"\"\"\nuids_with_users = config.comms.get_benchmark_datasets_with_users(benchmark_uid)\nreturn uids_with_users\n@classmethod\ndef get_models_associations(cls, benchmark_uid: int) -&gt; List[dict]:\n\"\"\"Retrieves the list of model associations to the benchmark\n        Args:\n            benchmark_uid (int): UID of the benchmark.\n        Returns:\n            List[dict]: List of associations\n        \"\"\"\nexperiment_type = \"benchmark\"\ncomponent_type = \"model_mlcube\"\nassociations = get_user_associations(\nexperiment_type=experiment_type,\ncomponent_type=component_type,\napproval_status=None,\n)\nassociations = [a for a in associations if a[\"benchmark\"] == benchmark_uid]\nreturn associations\n@classmethod\ndef get_datasets_associations(cls, benchmark_uid: int) -&gt; List[dict]:\n\"\"\"Retrieves the list of models associated to the benchmark\n        Args:\n            benchmark_uid (int): UID of the benchmark.\n        Returns:\n            List[dict]: List of associations\n        \"\"\"\nexperiment_type = \"benchmark\"\ncomponent_type = \"dataset\"\nassociations = get_user_associations(\nexperiment_type=experiment_type,\ncomponent_type=component_type,\napproval_status=None,\n)\nassociations = [a for a in associations if a[\"benchmark\"] == benchmark_uid]\nreturn associations\ndef display_dict(self):\nreturn {\n\"UID\": self.identifier,\n\"Name\": self.name,\n\"Description\": self.description,\n\"Documentation\": self.docs_url,\n\"Created At\": self.created_at,\n\"Data Preparation Container\": int(self.data_preparation_mlcube),\n\"Reference Model Container\": int(self.reference_model_mlcube),\n\"Data Evaluator Container\": int(self.data_evaluator_mlcube),\n\"State\": self.state,\n\"Approval Status\": self.approval_status,\n\"Registered\": self.is_registered,\n}\n</code></pre>"},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Creates a new benchmark instance</p> <p>Parameters:</p> Name Type Description Default <code>bmk_desc</code> <code>Union[dict, BenchmarkModel]</code> <p>Benchmark instance description</p> required Source code in <code>cli/medperf/entities/benchmark.py</code> <pre><code>def __init__(self, *args, **kwargs):\n\"\"\"Creates a new benchmark instance\n    Args:\n        bmk_desc (Union[dict, BenchmarkModel]): Benchmark instance description\n    \"\"\"\nsuper().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark.get_datasets_associations","title":"<code>get_datasets_associations(benchmark_uid)</code>  <code>classmethod</code>","text":"<p>Retrieves the list of models associated to the benchmark</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>UID of the benchmark.</p> required <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List of associations</p> Source code in <code>cli/medperf/entities/benchmark.py</code> <pre><code>@classmethod\ndef get_datasets_associations(cls, benchmark_uid: int) -&gt; List[dict]:\n\"\"\"Retrieves the list of models associated to the benchmark\n    Args:\n        benchmark_uid (int): UID of the benchmark.\n    Returns:\n        List[dict]: List of associations\n    \"\"\"\nexperiment_type = \"benchmark\"\ncomponent_type = \"dataset\"\nassociations = get_user_associations(\nexperiment_type=experiment_type,\ncomponent_type=component_type,\napproval_status=None,\n)\nassociations = [a for a in associations if a[\"benchmark\"] == benchmark_uid]\nreturn associations\n</code></pre>"},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark.get_datasets_with_users","title":"<code>get_datasets_with_users(benchmark_uid)</code>  <code>classmethod</code>","text":"<p>Retrieves the list of datasets and their owner info, associated to the benchmark</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>UID of the benchmark.</p> required <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List of dicts of dataset IDs with their owner info</p> Source code in <code>cli/medperf/entities/benchmark.py</code> <pre><code>@classmethod\ndef get_datasets_with_users(cls, benchmark_uid: int) -&gt; List[dict]:\n\"\"\"Retrieves the list of datasets and their owner info, associated to the benchmark\n    Args:\n        benchmark_uid (int): UID of the benchmark.\n    Returns:\n        List[dict]: List of dicts of dataset IDs with their owner info\n    \"\"\"\nuids_with_users = config.comms.get_benchmark_datasets_with_users(benchmark_uid)\nreturn uids_with_users\n</code></pre>"},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark.get_models_associations","title":"<code>get_models_associations(benchmark_uid)</code>  <code>classmethod</code>","text":"<p>Retrieves the list of model associations to the benchmark</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>UID of the benchmark.</p> required <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List of associations</p> Source code in <code>cli/medperf/entities/benchmark.py</code> <pre><code>@classmethod\ndef get_models_associations(cls, benchmark_uid: int) -&gt; List[dict]:\n\"\"\"Retrieves the list of model associations to the benchmark\n    Args:\n        benchmark_uid (int): UID of the benchmark.\n    Returns:\n        List[dict]: List of associations\n    \"\"\"\nexperiment_type = \"benchmark\"\ncomponent_type = \"model_mlcube\"\nassociations = get_user_associations(\nexperiment_type=experiment_type,\ncomponent_type=component_type,\napproval_status=None,\n)\nassociations = [a for a in associations if a[\"benchmark\"] == benchmark_uid]\nreturn associations\n</code></pre>"},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark.get_models_uids","title":"<code>get_models_uids(benchmark_uid)</code>  <code>classmethod</code>","text":"<p>Retrieves the list of models associated to the benchmark</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_uid</code> <code>int</code> <p>UID of the benchmark.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>List[int]: List of mlcube uids</p> Source code in <code>cli/medperf/entities/benchmark.py</code> <pre><code>@classmethod\ndef get_models_uids(cls, benchmark_uid: int) -&gt; List[int]:\n\"\"\"Retrieves the list of models associated to the benchmark\n    Args:\n        benchmark_uid (int): UID of the benchmark.\n    Returns:\n        List[int]: List of mlcube uids\n    \"\"\"\nassociations = get_experiment_associations(\nexperiment_id=benchmark_uid,\nexperiment_type=\"benchmark\",\ncomponent_type=\"model_mlcube\",\napproval_status=\"APPROVED\",\n)\nmodels_uids = [assoc[\"model_mlcube\"] for assoc in associations]\nreturn models_uids\n</code></pre>"},{"location":"reference/entities/benchmark/#entities.benchmark.Benchmark.remote_prefilter","title":"<code>remote_prefilter(filters)</code>  <code>staticmethod</code>","text":"<p>Applies filtering logic that must be done before retrieving remote entities</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict</code> <p>filters to apply</p> required <p>Returns:</p> Name Type Description <code>callable</code> <code>callable</code> <p>A function for retrieving remote entities with the applied prefilters</p> Source code in <code>cli/medperf/entities/benchmark.py</code> <pre><code>@staticmethod\ndef remote_prefilter(filters: dict) -&gt; callable:\n\"\"\"Applies filtering logic that must be done before retrieving remote entities\n    Args:\n        filters (dict): filters to apply\n    Returns:\n        callable: A function for retrieving remote entities with the applied prefilters\n    \"\"\"\ncomms_fn = config.comms.get_benchmarks\nif \"owner\" in filters and filters[\"owner\"] == get_medperf_user_data()[\"id\"]:\ncomms_fn = config.comms.get_user_benchmarks\nreturn comms_fn\n</code></pre>"},{"location":"reference/entities/ca/","title":"Ca","text":""},{"location":"reference/entities/ca/#entities.ca.CA","title":"<code>CA</code>","text":"<p>             Bases: <code>Entity</code>, <code>MedperfSchema</code></p> <p>Class representing a compatibility test report entry</p> <p>A test report is comprised of the components of a test execution: - data used, which can be:     - a demo aggregator url and its hash, or     - a raw data path and its labels path, or     - a prepared aggregator uid - Data preparation cube if the data used was not already prepared - model cube - evaluator cube - results</p> Source code in <code>cli/medperf/entities/ca.py</code> <pre><code>class CA(Entity, MedperfSchema):\n\"\"\"\n    Class representing a compatibility test report entry\n    A test report is comprised of the components of a test execution:\n    - data used, which can be:\n        - a demo aggregator url and its hash, or\n        - a raw data path and its labels path, or\n        - a prepared aggregator uid\n    - Data preparation cube if the data used was not already prepared\n    - model cube\n    - evaluator cube\n    - results\n    \"\"\"\nmetadata: dict = {}\nclient_mlcube: int\nserver_mlcube: int\nca_mlcube: int\nconfig: dict\n@staticmethod\ndef get_type():\nreturn \"ca\"\n@staticmethod\ndef get_storage_path():\nreturn config.cas_folder\n@staticmethod\ndef get_comms_retriever():\nreturn config.comms.get_ca\n@staticmethod\ndef get_metadata_filename():\nreturn config.ca_file\n@staticmethod\ndef get_comms_uploader():\nreturn config.comms.upload_ca\n@validator(\"config\", pre=True, always=True)\ndef check_config(cls, v, *, values, **kwargs):\nkeys = set(v.keys())\nallowed_keys = {\n\"address\",\n\"port\",\n\"fingerprint\",\n\"client_provisioner\",\n\"server_provisioner\",\n}\nif keys != allowed_keys:\nraise ValueError(\"config must contain two keys only: address and port\")\nreturn v\ndef __init__(self, *args, **kwargs):\nsuper().__init__(*args, **kwargs)\nself.address = self.config[\"address\"]\nself.port = self.config[\"port\"]\nself.fingerprint = self.config[\"fingerprint\"]\nself.client_provisioner = self.config[\"client_provisioner\"]\nself.server_provisioner = self.config[\"server_provisioner\"]\nself.config_path = os.path.join(self.path, config.ca_config_file)\nself.pki_assets = os.path.join(self.path, config.ca_cert_folder)\n@property\ndef local_id(self):\nreturn self.name\n@classmethod\ndef from_experiment(cls, training_exp_uid: int) -&gt; \"CA\":\nmeta = config.comms.get_experiment_ca(training_exp_uid)\nca = cls(**meta)\nca.write()\nreturn ca\n@classmethod\ndef remote_prefilter(cls, filters: dict) -&gt; callable:\n\"\"\"Applies filtering logic that must be done before retrieving remote entities\n        Args:\n            filters (dict): filters to apply\n        Returns:\n            callable: A function for retrieving remote entities with the applied prefilters\n        \"\"\"\ncomms_fn = config.comms.get_cas\nif \"owner\" in filters and filters[\"owner\"] == get_medperf_user_data()[\"id\"]:\ncomms_fn = config.comms.get_user_cas\nreturn comms_fn\ndef prepare_config(self):\nwith open(self.config_path, \"w\") as f:\njson.dump(self.config, f)\ndef display_dict(self):\nreturn {\n\"UID\": self.identifier,\n\"Name\": self.name,\n\"Address\": self.address,\n\"fingerprint\": self.fingerprint,\n\"Port\": self.port,\n\"Created At\": self.created_at,\n\"Registered\": self.is_registered,\n}\n</code></pre>"},{"location":"reference/entities/ca/#entities.ca.CA.remote_prefilter","title":"<code>remote_prefilter(filters)</code>  <code>classmethod</code>","text":"<p>Applies filtering logic that must be done before retrieving remote entities</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict</code> <p>filters to apply</p> required <p>Returns:</p> Name Type Description <code>callable</code> <code>callable</code> <p>A function for retrieving remote entities with the applied prefilters</p> Source code in <code>cli/medperf/entities/ca.py</code> <pre><code>@classmethod\ndef remote_prefilter(cls, filters: dict) -&gt; callable:\n\"\"\"Applies filtering logic that must be done before retrieving remote entities\n    Args:\n        filters (dict): filters to apply\n    Returns:\n        callable: A function for retrieving remote entities with the applied prefilters\n    \"\"\"\ncomms_fn = config.comms.get_cas\nif \"owner\" in filters and filters[\"owner\"] == get_medperf_user_data()[\"id\"]:\ncomms_fn = config.comms.get_user_cas\nreturn comms_fn\n</code></pre>"},{"location":"reference/entities/cube/","title":"Cube","text":""},{"location":"reference/entities/cube/#entities.cube.Cube","title":"<code>Cube</code>","text":"<p>             Bases: <code>Entity</code>, <code>DeployableSchema</code></p> <p>Class representing an MLCube Container</p> <p>Medperf platform uses the MLCube container for components such as Dataset Preparation, Evaluation, and the Registered Models. MLCube containers are software containers (e.g., Docker and Singularity) with standard metadata and a consistent file-system level interface.</p> Source code in <code>cli/medperf/entities/cube.py</code> <pre><code>class Cube(Entity, DeployableSchema):\n\"\"\"\n    Class representing an MLCube Container\n    Medperf platform uses the MLCube container for components such as\n    Dataset Preparation, Evaluation, and the Registered Models. MLCube\n    containers are software containers (e.g., Docker and Singularity)\n    with standard metadata and a consistent file-system level interface.\n    \"\"\"\ngit_mlcube_url: str\nmlcube_hash: Optional[str]\ngit_parameters_url: Optional[str]\nparameters_hash: Optional[str]\nimage_tarball_url: Optional[str]\nimage_tarball_hash: Optional[str]\nimage_hash: Optional[str]\nadditional_files_tarball_url: Optional[str] = Field(None, alias=\"tarball_url\")\nadditional_files_tarball_hash: Optional[str] = Field(None, alias=\"tarball_hash\")\nmetadata: dict = {}\nuser_metadata: dict = {}\n@staticmethod\ndef get_type():\nreturn \"container\"\n@staticmethod\ndef get_storage_path():\nreturn config.cubes_folder\n@staticmethod\ndef get_comms_retriever():\nreturn config.comms.get_cube_metadata\n@staticmethod\ndef get_metadata_filename():\nreturn config.cube_metadata_filename\n@staticmethod\ndef get_comms_uploader():\nreturn config.comms.upload_mlcube\ndef __init__(self, *args, **kwargs):\n\"\"\"Creates a Cube instance\n        Args:\n            cube_desc (Union[dict, CubeModel]): MLCube Instance description\n        \"\"\"\nsuper().__init__(*args, **kwargs)\nself.cube_path = os.path.join(self.path, config.cube_filename)\nself.params_path = None\nself.additiona_files_folder_path = None\nself.params_path = os.path.join(\nself.path, config.workspace_path, config.params_filename\n)\nself.additiona_files_folder_path = os.path.join(\nself.path, config.additional_path\n)\nself._parser = None\nself._runner = None\n@property\ndef parser(self):\nif self._parser is None:\nself._parser = load_parser(self.cube_path)\nreturn self._parser\n@property\ndef runner(self):\nif self._runner is None:\nself._runner = load_runner(self.parser, self.path)\nreturn self._runner\n@property\ndef local_id(self):\nreturn self.name\n@staticmethod\ndef remote_prefilter(filters: dict):\n\"\"\"Applies filtering logic that must be done before retrieving remote entities\n        Args:\n            filters (dict): filters to apply\n        Returns:\n            callable: A function for retrieving remote entities with the applied prefilters\n        \"\"\"\ncomms_fn = config.comms.get_cubes\nif \"owner\" in filters and filters[\"owner\"] == get_medperf_user_data()[\"id\"]:\ncomms_fn = config.comms.get_user_cubes\nreturn comms_fn\n@classmethod\ndef get(\ncls,\ncube_uid: Union[str, int],\nlocal_only: bool = False,\nvalid_only: bool = True,\n) -&gt; \"Cube\":\n\"\"\"Retrieves and creates a Cube instance from the comms. If cube already exists\n        inside the user's computer then retrieves it from there.\n        Args:\n            valid_only: if to raise an error in case of invalidated Cube\n            cube_uid (str): UID of the cube.\n        Returns:\n            Cube : a Cube instance with the retrieved data.\n        \"\"\"\ncube = super().get(cube_uid, local_only)\nif not cube.is_valid and valid_only:\nraise InvalidEntityError(\"The requested container is marked as INVALID.\")\ncube.download_config_files()\nreturn cube\ndef download_mlcube(self):\nurl = self.git_mlcube_url\npath, file_hash = resources.get_cube(url, self.path, self.mlcube_hash)\nself.cube_path = path\nself.mlcube_hash = file_hash\ndef download_parameters(self):\nurl = self.git_parameters_url\nif url:\npath, file_hash = resources.get_cube_params(\nurl, self.path, self.parameters_hash\n)\nself.params_path = path\nself.parameters_hash = file_hash\ndef download_additional(self):\nurl = self.additional_files_tarball_url\nif url:\npath, file_hash = resources.get_cube_additional(\nurl, self.path, self.additional_files_tarball_hash\n)\nself.additiona_files_folder_path = path\nself.additional_files_tarball_hash = file_hash\ndef download_config_files(self):\ntry:\nself.download_mlcube()\nexcept InvalidEntityError as e:\nraise InvalidEntityError(f\"Container {self.name} config file: {e}\")\ntry:\nself.download_parameters()\nexcept InvalidEntityError as e:\nraise InvalidEntityError(f\"Container {self.name} parameters file: {e}\")\ndef download_run_files(self):\ntry:\nself.download_additional()\nexcept InvalidEntityError as e:\nraise InvalidEntityError(f\"Container {self.name} additional files: {e}\")\nalternative_image_hash = None\nif self.metadata is not None:\nalternative_image_hash = self.metadata.get(\"digest\", None)\ntry:\nself.image_hash = self.runner.download(\nexpected_image_hash=self.image_hash,\ndownload_timeout=config.mlcube_configure_timeout,\nget_hash_timeout=config.mlcube_inspect_timeout,\nalternative_image_hash=alternative_image_hash,\n)\nexcept InvalidEntityError as e:\nraise InvalidEntityError(f\"Container {self.name} image: {e}\")\ndef run(\nself,\ntask: str,\noutput_logs: str = None,\ntimeout: int = None,\nmounts: dict = {},\nenv: dict = {},\nports: list = [],\ndisable_network: bool = True,\n):\nos.makedirs(self.additiona_files_folder_path, exist_ok=True)\nextra_mounts = {\n\"parameters_file\": self.params_path,\n\"additional_files\": self.additiona_files_folder_path,\n}\nextra_env = {}\nif config.container_loglevel is not None:\nextra_env[\"MEDPERF_LOGLEVEL\"] = config.container_loglevel.upper()\ntmp_folder = generate_tmp_path()\nos.makedirs(tmp_folder, exist_ok=True)\nself.runner.run(\ntask,\ntmp_folder,\noutput_logs,\ntimeout,\nmedperf_mounts={**mounts, **extra_mounts},\nmedperf_env={**env, **extra_env},\nports=ports,\ndisable_network=disable_network,\n)\ndef is_report_specified(self):\nreturn self.parser.is_report_specified()\ndef is_metadata_specified(self):\nreturn self.parser.is_metadata_specified()\n@classmethod\ndef get_benchmarks_associations(cls, mlcube_uid: int) -&gt; List[dict]:\n\"\"\"Retrieves the list of benchmarks model is associated with\n        Args:\n            mlcube_uid (int): UID of the cube.\n        Returns:\n            List[dict]: List of associations\n        \"\"\"\nexperiment_type = \"benchmark\"\ncomponent_type = \"model_mlcube\"\nassociations = get_user_associations(\nexperiment_type=experiment_type,\ncomponent_type=component_type,\napproval_status=None,\n)\nassociations = [a for a in associations if a[\"model_mlcube\"] == mlcube_uid]\nreturn associations\ndef display_dict(self):\nreturn {\n\"UID\": self.identifier,\n\"Name\": self.name,\n\"Config File\": self.git_mlcube_url,\n\"State\": self.state,\n\"Created At\": self.created_at,\n\"Registered\": self.is_registered,\n}\n</code></pre>"},{"location":"reference/entities/cube/#entities.cube.Cube.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Creates a Cube instance</p> <p>Parameters:</p> Name Type Description Default <code>cube_desc</code> <code>Union[dict, CubeModel]</code> <p>MLCube Instance description</p> required Source code in <code>cli/medperf/entities/cube.py</code> <pre><code>def __init__(self, *args, **kwargs):\n\"\"\"Creates a Cube instance\n    Args:\n        cube_desc (Union[dict, CubeModel]): MLCube Instance description\n    \"\"\"\nsuper().__init__(*args, **kwargs)\nself.cube_path = os.path.join(self.path, config.cube_filename)\nself.params_path = None\nself.additiona_files_folder_path = None\nself.params_path = os.path.join(\nself.path, config.workspace_path, config.params_filename\n)\nself.additiona_files_folder_path = os.path.join(\nself.path, config.additional_path\n)\nself._parser = None\nself._runner = None\n</code></pre>"},{"location":"reference/entities/cube/#entities.cube.Cube.get","title":"<code>get(cube_uid, local_only=False, valid_only=True)</code>  <code>classmethod</code>","text":"<p>Retrieves and creates a Cube instance from the comms. If cube already exists inside the user's computer then retrieves it from there.</p> <p>Parameters:</p> Name Type Description Default <code>valid_only</code> <code>bool</code> <p>if to raise an error in case of invalidated Cube</p> <code>True</code> <code>cube_uid</code> <code>str</code> <p>UID of the cube.</p> required <p>Returns:</p> Name Type Description <code>Cube</code> <code>Cube</code> <p>a Cube instance with the retrieved data.</p> Source code in <code>cli/medperf/entities/cube.py</code> <pre><code>@classmethod\ndef get(\ncls,\ncube_uid: Union[str, int],\nlocal_only: bool = False,\nvalid_only: bool = True,\n) -&gt; \"Cube\":\n\"\"\"Retrieves and creates a Cube instance from the comms. If cube already exists\n    inside the user's computer then retrieves it from there.\n    Args:\n        valid_only: if to raise an error in case of invalidated Cube\n        cube_uid (str): UID of the cube.\n    Returns:\n        Cube : a Cube instance with the retrieved data.\n    \"\"\"\ncube = super().get(cube_uid, local_only)\nif not cube.is_valid and valid_only:\nraise InvalidEntityError(\"The requested container is marked as INVALID.\")\ncube.download_config_files()\nreturn cube\n</code></pre>"},{"location":"reference/entities/cube/#entities.cube.Cube.get_benchmarks_associations","title":"<code>get_benchmarks_associations(mlcube_uid)</code>  <code>classmethod</code>","text":"<p>Retrieves the list of benchmarks model is associated with</p> <p>Parameters:</p> Name Type Description Default <code>mlcube_uid</code> <code>int</code> <p>UID of the cube.</p> required <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List of associations</p> Source code in <code>cli/medperf/entities/cube.py</code> <pre><code>@classmethod\ndef get_benchmarks_associations(cls, mlcube_uid: int) -&gt; List[dict]:\n\"\"\"Retrieves the list of benchmarks model is associated with\n    Args:\n        mlcube_uid (int): UID of the cube.\n    Returns:\n        List[dict]: List of associations\n    \"\"\"\nexperiment_type = \"benchmark\"\ncomponent_type = \"model_mlcube\"\nassociations = get_user_associations(\nexperiment_type=experiment_type,\ncomponent_type=component_type,\napproval_status=None,\n)\nassociations = [a for a in associations if a[\"model_mlcube\"] == mlcube_uid]\nreturn associations\n</code></pre>"},{"location":"reference/entities/cube/#entities.cube.Cube.remote_prefilter","title":"<code>remote_prefilter(filters)</code>  <code>staticmethod</code>","text":"<p>Applies filtering logic that must be done before retrieving remote entities</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict</code> <p>filters to apply</p> required <p>Returns:</p> Name Type Description <code>callable</code> <p>A function for retrieving remote entities with the applied prefilters</p> Source code in <code>cli/medperf/entities/cube.py</code> <pre><code>@staticmethod\ndef remote_prefilter(filters: dict):\n\"\"\"Applies filtering logic that must be done before retrieving remote entities\n    Args:\n        filters (dict): filters to apply\n    Returns:\n        callable: A function for retrieving remote entities with the applied prefilters\n    \"\"\"\ncomms_fn = config.comms.get_cubes\nif \"owner\" in filters and filters[\"owner\"] == get_medperf_user_data()[\"id\"]:\ncomms_fn = config.comms.get_user_cubes\nreturn comms_fn\n</code></pre>"},{"location":"reference/entities/dataset/","title":"Dataset","text":""},{"location":"reference/entities/dataset/#entities.dataset.Dataset","title":"<code>Dataset</code>","text":"<p>             Bases: <code>Entity</code>, <code>DeployableSchema</code></p> <p>Class representing a Dataset</p> <p>Datasets are stored locally in the Data Owner's machine. They contain information regarding the prepared dataset, such as name and description, general statistics and an UID generated by hashing the contents of the data preparation output.</p> Source code in <code>cli/medperf/entities/dataset.py</code> <pre><code>class Dataset(Entity, DeployableSchema):\n\"\"\"\n    Class representing a Dataset\n    Datasets are stored locally in the Data Owner's machine. They contain\n    information regarding the prepared dataset, such as name and description,\n    general statistics and an UID generated by hashing the contents of the\n    data preparation output.\n    \"\"\"\ndescription: Optional[str] = Field(None, max_length=20)\nlocation: Optional[str] = Field(None, max_length=20)\ninput_data_hash: str\ngenerated_uid: str\ndata_preparation_mlcube: Union[int, str]\nsplit_seed: Optional[int]\ngenerated_metadata: dict = Field(..., alias=\"metadata\")\nuser_metadata: dict = {}\nreport: dict = {}\nsubmitted_as_prepared: bool\n@staticmethod\ndef get_type():\nreturn \"dataset\"\n@staticmethod\ndef get_storage_path():\nreturn config.datasets_folder\n@staticmethod\ndef get_comms_retriever():\nreturn config.comms.get_dataset\n@staticmethod\ndef get_metadata_filename():\nreturn config.reg_file\n@staticmethod\ndef get_comms_uploader():\nreturn config.comms.upload_dataset\n@validator(\"data_preparation_mlcube\", pre=True, always=True)\ndef check_data_preparation_mlcube(cls, v, *, values, **kwargs):\nif not isinstance(v, int) and not values[\"for_test\"]:\nraise ValueError(\n\"data_preparation_mlcube must be an integer if not running a compatibility test\"\n)\nreturn v\ndef __init__(self, *args, **kwargs):\nsuper().__init__(*args, **kwargs)\nself.data_path = os.path.join(self.path, \"data\")\nself.labels_path = os.path.join(self.path, \"labels\")\nself.report_path = os.path.join(self.path, config.report_file)\nself.metadata_path = os.path.join(self.path, config.metadata_folder)\nself.statistics_path = os.path.join(self.path, config.statistics_filename)\n@property\ndef local_id(self):\nreturn self.generated_uid\ndef set_raw_paths(self, raw_data_path: str, raw_labels_path: str):\nraw_paths_file = os.path.join(self.path, config.dataset_raw_paths_file)\ndata = {\"data_path\": raw_data_path, \"labels_path\": raw_labels_path}\nwith open(raw_paths_file, \"w\") as f:\nyaml.dump(data, f)\ndef get_raw_paths(self):\nraw_paths_file = os.path.join(self.path, config.dataset_raw_paths_file)\nwith open(raw_paths_file) as f:\ndata = yaml.safe_load(f)\nreturn data[\"data_path\"], data[\"labels_path\"]\ndef mark_as_ready(self):\nflag_file = os.path.join(self.path, config.ready_flag_file)\nwith open(flag_file, \"w\"):\npass\ndef unmark_as_ready(self):\nflag_file = os.path.join(self.path, config.ready_flag_file)\nremove_path(flag_file)\ndef is_ready(self):\nflag_file = os.path.join(self.path, config.ready_flag_file)\nreturn os.path.exists(flag_file)\n@staticmethod\ndef remote_prefilter(filters: dict) -&gt; callable:\n\"\"\"Applies filtering logic that must be done before retrieving remote entities\n        Args:\n            filters (dict): filters to apply\n        Returns:\n            callable: A function for retrieving remote entities with the applied prefilters\n        \"\"\"\ncomms_fn = config.comms.get_datasets\nif \"owner\" in filters and filters[\"owner\"] == get_medperf_user_data()[\"id\"]:\ncomms_fn = config.comms.get_user_datasets\nif \"mlcube\" in filters and filters[\"mlcube\"] is not None:\ndef func():\nreturn config.comms.get_mlcube_datasets(filters[\"mlcube\"])\ncomms_fn = func\nreturn comms_fn\n@classmethod\ndef get_benchmarks_associations(cls, dataset_uid: int) -&gt; List[dict]:\n\"\"\"Retrieves the list of benchmarks dataset is associated with\n        Args:\n            dataset_uid (int): UID of the dataset.\n        Returns:\n            List[dict]: List of associations\n        \"\"\"\nexperiment_type = \"benchmark\"\ncomponent_type = \"dataset\"\nassociations = get_user_associations(\nexperiment_type=experiment_type,\ncomponent_type=component_type,\napproval_status=None,\n)\nassociations = [a for a in associations if a[\"dataset\"] == dataset_uid]\nreturn associations\ndef read_report(self):\n\"\"\"Reads the report if it exists\"\"\"\nif os.path.exists(self.report_path):\nwith open(self.report_path, \"r\") as f:\nself.report = yaml.safe_load(f.read())\nreturn self.report\ndef read_statistics(self):\n\"\"\"Reads the report if it exists\"\"\"\nif os.path.exists(self.statistics_path):\nwith open(self.statistics_path, \"r\") as f:\nself.generated_metadata = yaml.safe_load(f.read())\nreturn self.generated_metadata\ndef display_dict(self):\nreturn {\n\"UID\": self.identifier,\n\"Name\": self.name,\n\"Description\": self.description,\n\"Location\": self.location,\n\"Data Preparation Container UID\": self.data_preparation_mlcube,\n\"Generated Hash\": self.generated_uid,\n\"State\": self.state,\n\"Created At\": self.created_at,\n\"Registered\": self.is_registered,\n\"Submitted as Prepared\": self.submitted_as_prepared,\n\"Status\": \"\\n\".join([f\"{k}: {v}\" for k, v in self.report.items()]),\n\"Owner\": self.owner,\n}\n</code></pre>"},{"location":"reference/entities/dataset/#entities.dataset.Dataset.get_benchmarks_associations","title":"<code>get_benchmarks_associations(dataset_uid)</code>  <code>classmethod</code>","text":"<p>Retrieves the list of benchmarks dataset is associated with</p> <p>Parameters:</p> Name Type Description Default <code>dataset_uid</code> <code>int</code> <p>UID of the dataset.</p> required <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List of associations</p> Source code in <code>cli/medperf/entities/dataset.py</code> <pre><code>@classmethod\ndef get_benchmarks_associations(cls, dataset_uid: int) -&gt; List[dict]:\n\"\"\"Retrieves the list of benchmarks dataset is associated with\n    Args:\n        dataset_uid (int): UID of the dataset.\n    Returns:\n        List[dict]: List of associations\n    \"\"\"\nexperiment_type = \"benchmark\"\ncomponent_type = \"dataset\"\nassociations = get_user_associations(\nexperiment_type=experiment_type,\ncomponent_type=component_type,\napproval_status=None,\n)\nassociations = [a for a in associations if a[\"dataset\"] == dataset_uid]\nreturn associations\n</code></pre>"},{"location":"reference/entities/dataset/#entities.dataset.Dataset.read_report","title":"<code>read_report()</code>","text":"<p>Reads the report if it exists</p> Source code in <code>cli/medperf/entities/dataset.py</code> <pre><code>def read_report(self):\n\"\"\"Reads the report if it exists\"\"\"\nif os.path.exists(self.report_path):\nwith open(self.report_path, \"r\") as f:\nself.report = yaml.safe_load(f.read())\nreturn self.report\n</code></pre>"},{"location":"reference/entities/dataset/#entities.dataset.Dataset.read_statistics","title":"<code>read_statistics()</code>","text":"<p>Reads the report if it exists</p> Source code in <code>cli/medperf/entities/dataset.py</code> <pre><code>def read_statistics(self):\n\"\"\"Reads the report if it exists\"\"\"\nif os.path.exists(self.statistics_path):\nwith open(self.statistics_path, \"r\") as f:\nself.generated_metadata = yaml.safe_load(f.read())\nreturn self.generated_metadata\n</code></pre>"},{"location":"reference/entities/dataset/#entities.dataset.Dataset.remote_prefilter","title":"<code>remote_prefilter(filters)</code>  <code>staticmethod</code>","text":"<p>Applies filtering logic that must be done before retrieving remote entities</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict</code> <p>filters to apply</p> required <p>Returns:</p> Name Type Description <code>callable</code> <code>callable</code> <p>A function for retrieving remote entities with the applied prefilters</p> Source code in <code>cli/medperf/entities/dataset.py</code> <pre><code>@staticmethod\ndef remote_prefilter(filters: dict) -&gt; callable:\n\"\"\"Applies filtering logic that must be done before retrieving remote entities\n    Args:\n        filters (dict): filters to apply\n    Returns:\n        callable: A function for retrieving remote entities with the applied prefilters\n    \"\"\"\ncomms_fn = config.comms.get_datasets\nif \"owner\" in filters and filters[\"owner\"] == get_medperf_user_data()[\"id\"]:\ncomms_fn = config.comms.get_user_datasets\nif \"mlcube\" in filters and filters[\"mlcube\"] is not None:\ndef func():\nreturn config.comms.get_mlcube_datasets(filters[\"mlcube\"])\ncomms_fn = func\nreturn comms_fn\n</code></pre>"},{"location":"reference/entities/event/","title":"Event","text":""},{"location":"reference/entities/event/#entities.event.TrainingEvent","title":"<code>TrainingEvent</code>","text":"<p>             Bases: <code>Entity</code>, <code>MedperfSchema</code></p> <p>Class representing a compatibility test report entry</p> <p>A test report is comprised of the components of a test execution: - data used, which can be:     - a demo aggregator url and its hash, or     - a raw data path and its labels path, or     - a prepared aggregator uid - Data preparation cube if the data used was not already prepared - model cube - evaluator cube - results</p> Source code in <code>cli/medperf/entities/event.py</code> <pre><code>class TrainingEvent(Entity, MedperfSchema):\n\"\"\"\n    Class representing a compatibility test report entry\n    A test report is comprised of the components of a test execution:\n    - data used, which can be:\n        - a demo aggregator url and its hash, or\n        - a raw data path and its labels path, or\n        - a prepared aggregator uid\n    - Data preparation cube if the data used was not already prepared\n    - model cube\n    - evaluator cube\n    - results\n    \"\"\"\ntraining_exp: int\nparticipants: dict\nfinished: bool = False\nfinished_at: Optional[datetime]\nreport: Optional[dict]\n@staticmethod\ndef get_type():\nreturn \"training event\"\n@staticmethod\ndef get_storage_path():\nreturn config.training_events_folder\n@staticmethod\ndef get_comms_retriever():\nreturn config.comms.get_training_event\n@staticmethod\ndef get_metadata_filename():\nreturn config.training_event_file\n@staticmethod\ndef get_comms_uploader():\nreturn config.comms.upload_training_event\ndef __init__(self, *args, **kwargs):\nsuper().__init__(*args, **kwargs)\nself.participants_list_path = os.path.join(\nself.path, config.participants_list_filename\n)\ntimestamp = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S_%f\")\nself.agg_out_logs = os.path.join(\nself.path, config.training_out_agg_logs + timestamp\n)\nself.col_out_logs = os.path.join(self.path, config.training_out_col_logs)\nself.out_weights = os.path.join(\nself.path, config.training_out_weights + timestamp\n)\nself.report_path = os.path.join(\nself.path,\nconfig.training_report_folder + timestamp,\nconfig.training_report_file,\n)\n@property\ndef local_id(self):\nreturn self.name\n@classmethod\ndef from_experiment(cls, training_exp_uid: int) -&gt; \"TrainingEvent\":\nmeta = config.comms.get_experiment_event(training_exp_uid)\nca = cls(**meta)\nca.write()\nreturn ca\n@classmethod\ndef remote_prefilter(cls, filters: dict) -&gt; callable:\n\"\"\"Applies filtering logic that must be done before retrieving remote entities\n        Args:\n            filters (dict): filters to apply\n        Returns:\n            callable: A function for retrieving remote entities with the applied prefilters\n        \"\"\"\ncomms_fn = config.comms.get_training_events\nif \"owner\" in filters and filters[\"owner\"] == get_medperf_user_data()[\"id\"]:\ncomms_fn = config.comms.get_user_training_events\nreturn comms_fn\ndef get_latest_report_path(self):\nlatest_timestamp = None\nlatest_report_path = None\nfor subpath in os.listdir(self.path):\nif subpath.startswith(config.training_report_folder) and os.path.isdir(\nos.path.join(self.path, subpath)\n):\ntimestamp = subpath[len(config.training_report_folder) :]  # noqa\ntimestamp = datetime.strptime(timestamp, \"%Y_%m_%d_%H_%M_%S_%f\")\nif latest_timestamp is None or timestamp &gt; latest_timestamp:\nlatest_timestamp = timestamp\nlatest_report_path = os.path.join(\nself.path, subpath, config.training_report_file\n)\nreturn latest_report_path\ndef prepare_participants_list(self):\nwith open(self.participants_list_path, \"w\") as f:\nyaml.dump(self.participants, f)\ndef display_dict(self):\nreturn {\n\"UID\": self.identifier,\n\"Name\": self.name,\n\"Experiment\": self.training_exp,\n\"Participants\": self.participants,\n\"Created At\": self.created_at,\n\"Registered\": self.is_registered,\n\"Finished\": self.finished,\n\"Report\": self.report,\n}\n</code></pre>"},{"location":"reference/entities/event/#entities.event.TrainingEvent.remote_prefilter","title":"<code>remote_prefilter(filters)</code>  <code>classmethod</code>","text":"<p>Applies filtering logic that must be done before retrieving remote entities</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict</code> <p>filters to apply</p> required <p>Returns:</p> Name Type Description <code>callable</code> <code>callable</code> <p>A function for retrieving remote entities with the applied prefilters</p> Source code in <code>cli/medperf/entities/event.py</code> <pre><code>@classmethod\ndef remote_prefilter(cls, filters: dict) -&gt; callable:\n\"\"\"Applies filtering logic that must be done before retrieving remote entities\n    Args:\n        filters (dict): filters to apply\n    Returns:\n        callable: A function for retrieving remote entities with the applied prefilters\n    \"\"\"\ncomms_fn = config.comms.get_training_events\nif \"owner\" in filters and filters[\"owner\"] == get_medperf_user_data()[\"id\"]:\ncomms_fn = config.comms.get_user_training_events\nreturn comms_fn\n</code></pre>"},{"location":"reference/entities/execution/","title":"Execution","text":""},{"location":"reference/entities/execution/#entities.execution.Execution","title":"<code>Execution</code>","text":"<p>             Bases: <code>Entity</code>, <code>ApprovableSchema</code></p> <p>Class representing an Execution entry</p> <p>Executions are obtained after successfully running a benchmark execution flow. They contain information regarding the components involved in obtaining metrics results, as well as the results themselves. This class provides methods for working with benchmark executions and how to upload them to the backend.</p> Source code in <code>cli/medperf/entities/execution.py</code> <pre><code>class Execution(Entity, ApprovableSchema):\n\"\"\"\n    Class representing an Execution entry\n    Executions are obtained after successfully running a benchmark\n    execution flow. They contain information regarding the\n    components involved in obtaining metrics results, as well as the\n    results themselves. This class provides methods for working with\n    benchmark executions and how to upload them to the backend.\n    \"\"\"\nbenchmark: int\nmodel: int\ndataset: int\nresults: dict = {}\nmetadata: dict = {}\nuser_metadata: dict = {}\nmodel_report: dict = {}\nevaluation_report: dict = {}\nfinalized: bool = False\nfinalized_at: Optional[datetime]\n@staticmethod\ndef get_type():\nreturn \"execution\"\n@staticmethod\ndef get_storage_path():\nreturn config.executions_folder\n@staticmethod\ndef get_comms_retriever():\nreturn config.comms.get_execution\n@staticmethod\ndef get_metadata_filename():\nreturn config.results_info_file\n@staticmethod\ndef get_comms_uploader():\nreturn config.comms.upload_execution\ndef __init__(self, *args, **kwargs):\n\"\"\"Creates a new execution instance\"\"\"\nsuper().__init__(*args, **kwargs)\nself.results_path = os.path.join(self.path, config.results_filename)\nself.local_outputs_path = os.path.join(self.path, config.local_metrics_outputs)\n@property\ndef local_id(self):\nreturn self.name\ndef is_executed(self):\nflag_file = os.path.join(self.path, config.executed_flag)\nreturn os.path.exists(flag_file)\ndef unmark_as_executed(self):\nflag_file = os.path.join(self.path, config.executed_flag)\nremove_path(flag_file)\ndef mark_as_executed(self):\nflag_file = os.path.join(self.path, config.executed_flag)\nwith open(flag_file, \"w\"):\npass\ndef save_results(self, results, partial):\nwith open(self.results_path, \"w\") as f:\nyaml.safe_dump(results, f)\nif partial:\nflag_file = os.path.join(self.path, config.partial_flag)\nwith open(flag_file, \"w\"):\npass\ndef read_results(self):\nif self.finalized:\nreturn self.results\nwith open(self.results_path) as f:\nresults = yaml.safe_load(f)\nreturn results\ndef is_partial(self):\nis_partial_from_metadata = self.metadata.get(\"partial\", None)\nif is_partial_from_metadata is not None:\nreturn is_partial_from_metadata\n# otherwise, check locally\nflag_file = os.path.join(self.path, config.partial_flag)\nreturn os.path.exists(flag_file)\n@staticmethod\ndef remote_prefilter(filters: dict) -&gt; callable:\n\"\"\"Applies filtering logic that must be done before retrieving remote entities\n        Args:\n            filters (dict): filters to apply\n        Returns:\n            callable: A function for retrieving remote entities with the applied prefilters\n        \"\"\"\ncomms_fn = config.comms.get_executions\nif \"owner\" in filters and filters[\"owner\"] == get_medperf_user_data()[\"id\"]:\ncomms_fn = config.comms.get_user_executions\ndel filters[\"owner\"]\nelif \"benchmark\" in filters and filters[\"benchmark\"] is not None:\n# Only override the communications method if its not related to\n# the current user\n# This is needed so that users can filter their own executions\n# without getting permission errors.\n# Users will still be able to filter by benchmark through query params\nbmk = filters[\"benchmark\"]\ndel filters[\"benchmark\"]\ndef get_benchmark_executions(*args, **kwargs):\n# Decorate the benchmark results remote function so it has the same signature\n# as all the comms_fns\nreturn config.comms.get_benchmark_executions(bmk, *args, **kwargs)\ncomms_fn = get_benchmark_executions\nreturn comms_fn\ndef display_dict(self):\nreturn {\n\"UID\": self.identifier,\n\"Name\": self.name,\n\"Benchmark\": self.benchmark,\n\"Model\": self.model,\n\"Dataset\": self.dataset,\n\"Partial\": self.metadata.get(\"partial\", \"N/A\"),\n\"Executed\": self.is_executed(),\n\"Approval Status\": self.approval_status,\n\"Created At\": self.created_at,\n\"Registered\": self.is_registered,\n\"Finalized\": self.finalized,\n}\n</code></pre>"},{"location":"reference/entities/execution/#entities.execution.Execution.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Creates a new execution instance</p> Source code in <code>cli/medperf/entities/execution.py</code> <pre><code>def __init__(self, *args, **kwargs):\n\"\"\"Creates a new execution instance\"\"\"\nsuper().__init__(*args, **kwargs)\nself.results_path = os.path.join(self.path, config.results_filename)\nself.local_outputs_path = os.path.join(self.path, config.local_metrics_outputs)\n</code></pre>"},{"location":"reference/entities/execution/#entities.execution.Execution.remote_prefilter","title":"<code>remote_prefilter(filters)</code>  <code>staticmethod</code>","text":"<p>Applies filtering logic that must be done before retrieving remote entities</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict</code> <p>filters to apply</p> required <p>Returns:</p> Name Type Description <code>callable</code> <code>callable</code> <p>A function for retrieving remote entities with the applied prefilters</p> Source code in <code>cli/medperf/entities/execution.py</code> <pre><code>@staticmethod\ndef remote_prefilter(filters: dict) -&gt; callable:\n\"\"\"Applies filtering logic that must be done before retrieving remote entities\n    Args:\n        filters (dict): filters to apply\n    Returns:\n        callable: A function for retrieving remote entities with the applied prefilters\n    \"\"\"\ncomms_fn = config.comms.get_executions\nif \"owner\" in filters and filters[\"owner\"] == get_medperf_user_data()[\"id\"]:\ncomms_fn = config.comms.get_user_executions\ndel filters[\"owner\"]\nelif \"benchmark\" in filters and filters[\"benchmark\"] is not None:\n# Only override the communications method if its not related to\n# the current user\n# This is needed so that users can filter their own executions\n# without getting permission errors.\n# Users will still be able to filter by benchmark through query params\nbmk = filters[\"benchmark\"]\ndel filters[\"benchmark\"]\ndef get_benchmark_executions(*args, **kwargs):\n# Decorate the benchmark results remote function so it has the same signature\n# as all the comms_fns\nreturn config.comms.get_benchmark_executions(bmk, *args, **kwargs)\ncomms_fn = get_benchmark_executions\nreturn comms_fn\n</code></pre>"},{"location":"reference/entities/interface/","title":"Interface","text":""},{"location":"reference/entities/interface/#entities.interface.Entity","title":"<code>Entity</code>","text":"<p>             Bases: <code>MedperfSchema</code>, <code>ABC</code></p> Source code in <code>cli/medperf/entities/interface.py</code> <pre><code>class Entity(MedperfSchema, ABC):\n@staticmethod\ndef get_type() -&gt; str:\nraise NotImplementedError()\n@staticmethod\ndef get_storage_path() -&gt; str:\nraise NotImplementedError()\n@staticmethod\ndef get_comms_retriever() -&gt; Callable[[int], dict]:\nraise NotImplementedError()\n@staticmethod\ndef get_metadata_filename() -&gt; str:\nraise NotImplementedError()\n@staticmethod\ndef get_comms_uploader() -&gt; Callable[[dict], dict]:\nraise NotImplementedError()\n@property\ndef local_id(self) -&gt; str:\nraise NotImplementedError()\n@property\ndef identifier(self) -&gt; Union[int, str]:\nreturn self.id or self.local_id\n@property\ndef is_registered(self) -&gt; bool:\nreturn self.id is not None\n@property\ndef path(self) -&gt; str:\nstorage_path = self.get_storage_path()\nreturn os.path.join(storage_path, str(self.identifier))\n@classmethod\ndef all(\ncls: Type[EntityType], unregistered: bool = False, filters: dict = {}\n) -&gt; List[EntityType]:\n\"\"\"Gets a list of all instances of the respective entity.\n        Whether the list is local or remote depends on the implementation.\n        Args:\n            unregistered (bool, optional): Wether to retrieve only unregistered local entities. Defaults to False.\n            filters (dict, optional): key-value pairs specifying filters to apply to the list of entities.\n        Returns:\n            List[Entity]: a list of entities.\n        \"\"\"\nlogging.info(f\"Retrieving all {cls.get_type()} entities\")\nif unregistered:\nif filters:\nraise InvalidArgumentError(\n\"Filtering is not supported for unregistered entities\"\n)\nreturn cls.__unregistered_all()\nreturn cls.__remote_all(filters=filters)\n@classmethod\ndef __remote_all(cls: Type[EntityType], filters: dict) -&gt; List[EntityType]:\ncomms_fn = cls.remote_prefilter(filters)\nentity_meta = comms_fn(filters=filters)\nentities = [cls(**meta) for meta in entity_meta]\nreturn entities\n@classmethod\ndef __unregistered_all(cls: Type[EntityType]) -&gt; List[EntityType]:\nentities = []\nstorage_path = cls.get_storage_path()\ntry:\nuids = next(os.walk(storage_path))[1]\nexcept StopIteration:\nmsg = f\"Couldn't iterate over the {cls.get_type()} storage\"\nlogging.warning(msg)\nraise MedperfException(msg)\nfor uid in uids:\nif uid.isdigit():\ncontinue\nentity = cls.__local_get(uid)\nentities.append(entity)\nreturn entities\n@staticmethod\ndef remote_prefilter(filters: dict) -&gt; callable:\n\"\"\"Applies filtering logic that must be done before retrieving remote entities\n        Args:\n            filters (dict): filters to apply\n        Returns:\n            callable: A function for retrieving remote entities with the applied prefilters\n        \"\"\"\nraise NotImplementedError\n@classmethod\ndef get(\ncls: Type[EntityType],\nuid: Union[str, int],\nlocal_only: bool = False,\nvalid_only: bool = True,\n) -&gt; EntityType:\n\"\"\"Gets an instance of the respective entity.\n        Wether this requires only local read or remote calls depends\n        on the implementation.\n        Args:\n            uid (str): Unique Identifier to retrieve the entity\n            local_only (bool): If True, the entity will be retrieved locally\n            valid_only: if to raise en error in case of invalidated entity\n        Returns:\n            Entity: Entity Instance associated to the UID\n        \"\"\"\nif not str(uid).isdigit() or local_only:\nreturn cls.__local_get(uid)\nreturn cls.__remote_get(uid)\n@classmethod\ndef __remote_get(cls: Type[EntityType], uid: int) -&gt; EntityType:\n\"\"\"Retrieves and creates an entity instance from the comms instance.\n        Args:\n            uid (int): server UID of the entity\n        Returns:\n            Entity: Specified Entity Instance\n        \"\"\"\nlogging.debug(f\"Retrieving {cls.get_type()} {uid} remotely\")\ncomms_func = cls.get_comms_retriever()\nentity_dict = comms_func(uid)\nentity = cls(**entity_dict)\nentity.write()\nreturn entity\n@classmethod\ndef __local_get(cls: Type[EntityType], uid: Union[str, int]) -&gt; EntityType:\n\"\"\"Retrieves and creates an entity instance from the local storage.\n        Args:\n            uid (str|int): UID of the entity\n        Returns:\n            Entity: Specified Entity Instance\n        \"\"\"\nlogging.debug(f\"Retrieving {cls.get_type()} {uid} locally\")\nentity_dict = cls.__get_local_dict(uid)\nentity = cls(**entity_dict)\nreturn entity\n@classmethod\ndef __get_local_dict(cls: Type[EntityType], uid: Union[str, int]) -&gt; dict:\n\"\"\"Retrieves a local entity information\n        Args:\n            uid (str): uid of the local entity\n        Returns:\n            dict: information of the entity\n        \"\"\"\nlogging.info(f\"Retrieving {cls.get_type()} {uid} from local storage\")\nstorage_path = cls.get_storage_path()\nmetadata_filename = cls.get_metadata_filename()\nentity_file = os.path.join(storage_path, str(uid), metadata_filename)\nentity_file = sanitize_path(entity_file)\nif not os.path.exists(entity_file):\nraise InvalidArgumentError(\nf\"No {cls.get_type()} with the given uid could be found\"\n)\nwith open(entity_file, \"r\") as f:\ndata = yaml.safe_load(f)\nreturn data\ndef write(self) -&gt; str:\n\"\"\"Writes the entity to the local storage\n        Returns:\n            str: Path to the stored entity\n        \"\"\"\ndata = self.todict()\nmetadata_filename = self.get_metadata_filename()\nentity_file = os.path.join(self.path, metadata_filename)\nos.makedirs(self.path, exist_ok=True)\nwith open(entity_file, \"w\") as f:\nyaml.dump(data, f)\nreturn entity_file\ndef upload(self) -&gt; Dict:\n\"\"\"Upload the entity-related information to the communication's interface\n        Returns:\n            Dict: Dictionary with the updated entity information\n        \"\"\"\nif self.for_test:\nraise InvalidArgumentError(\nf\"This test {self.get_type()} cannot be uploaded.\"\n)\nbody = self.todict()\ncomms_func = self.get_comms_uploader()\nupdated_body = comms_func(body)\nreturn updated_body\ndef display_dict(self) -&gt; dict:\n\"\"\"Returns a dictionary of entity properties that can be displayed\n        to a user interface using a verbose name of the property rather than\n        the internal names\n        Returns:\n            dict: the display dictionary\n        \"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"reference/entities/interface/#entities.interface.Entity.__get_local_dict","title":"<code>__get_local_dict(uid)</code>  <code>classmethod</code>","text":"<p>Retrieves a local entity information</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <code>str</code> <p>uid of the local entity</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>information of the entity</p> Source code in <code>cli/medperf/entities/interface.py</code> <pre><code>@classmethod\ndef __get_local_dict(cls: Type[EntityType], uid: Union[str, int]) -&gt; dict:\n\"\"\"Retrieves a local entity information\n    Args:\n        uid (str): uid of the local entity\n    Returns:\n        dict: information of the entity\n    \"\"\"\nlogging.info(f\"Retrieving {cls.get_type()} {uid} from local storage\")\nstorage_path = cls.get_storage_path()\nmetadata_filename = cls.get_metadata_filename()\nentity_file = os.path.join(storage_path, str(uid), metadata_filename)\nentity_file = sanitize_path(entity_file)\nif not os.path.exists(entity_file):\nraise InvalidArgumentError(\nf\"No {cls.get_type()} with the given uid could be found\"\n)\nwith open(entity_file, \"r\") as f:\ndata = yaml.safe_load(f)\nreturn data\n</code></pre>"},{"location":"reference/entities/interface/#entities.interface.Entity.__local_get","title":"<code>__local_get(uid)</code>  <code>classmethod</code>","text":"<p>Retrieves and creates an entity instance from the local storage.</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <code>str | int</code> <p>UID of the entity</p> required <p>Returns:</p> Name Type Description <code>Entity</code> <code>EntityType</code> <p>Specified Entity Instance</p> Source code in <code>cli/medperf/entities/interface.py</code> <pre><code>@classmethod\ndef __local_get(cls: Type[EntityType], uid: Union[str, int]) -&gt; EntityType:\n\"\"\"Retrieves and creates an entity instance from the local storage.\n    Args:\n        uid (str|int): UID of the entity\n    Returns:\n        Entity: Specified Entity Instance\n    \"\"\"\nlogging.debug(f\"Retrieving {cls.get_type()} {uid} locally\")\nentity_dict = cls.__get_local_dict(uid)\nentity = cls(**entity_dict)\nreturn entity\n</code></pre>"},{"location":"reference/entities/interface/#entities.interface.Entity.__remote_get","title":"<code>__remote_get(uid)</code>  <code>classmethod</code>","text":"<p>Retrieves and creates an entity instance from the comms instance.</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <code>int</code> <p>server UID of the entity</p> required <p>Returns:</p> Name Type Description <code>Entity</code> <code>EntityType</code> <p>Specified Entity Instance</p> Source code in <code>cli/medperf/entities/interface.py</code> <pre><code>@classmethod\ndef __remote_get(cls: Type[EntityType], uid: int) -&gt; EntityType:\n\"\"\"Retrieves and creates an entity instance from the comms instance.\n    Args:\n        uid (int): server UID of the entity\n    Returns:\n        Entity: Specified Entity Instance\n    \"\"\"\nlogging.debug(f\"Retrieving {cls.get_type()} {uid} remotely\")\ncomms_func = cls.get_comms_retriever()\nentity_dict = comms_func(uid)\nentity = cls(**entity_dict)\nentity.write()\nreturn entity\n</code></pre>"},{"location":"reference/entities/interface/#entities.interface.Entity.all","title":"<code>all(unregistered=False, filters={})</code>  <code>classmethod</code>","text":"<p>Gets a list of all instances of the respective entity. Whether the list is local or remote depends on the implementation.</p> <p>Parameters:</p> Name Type Description Default <code>unregistered</code> <code>bool</code> <p>Wether to retrieve only unregistered local entities. Defaults to False.</p> <code>False</code> <code>filters</code> <code>dict</code> <p>key-value pairs specifying filters to apply to the list of entities.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[EntityType]</code> <p>List[Entity]: a list of entities.</p> Source code in <code>cli/medperf/entities/interface.py</code> <pre><code>@classmethod\ndef all(\ncls: Type[EntityType], unregistered: bool = False, filters: dict = {}\n) -&gt; List[EntityType]:\n\"\"\"Gets a list of all instances of the respective entity.\n    Whether the list is local or remote depends on the implementation.\n    Args:\n        unregistered (bool, optional): Wether to retrieve only unregistered local entities. Defaults to False.\n        filters (dict, optional): key-value pairs specifying filters to apply to the list of entities.\n    Returns:\n        List[Entity]: a list of entities.\n    \"\"\"\nlogging.info(f\"Retrieving all {cls.get_type()} entities\")\nif unregistered:\nif filters:\nraise InvalidArgumentError(\n\"Filtering is not supported for unregistered entities\"\n)\nreturn cls.__unregistered_all()\nreturn cls.__remote_all(filters=filters)\n</code></pre>"},{"location":"reference/entities/interface/#entities.interface.Entity.display_dict","title":"<code>display_dict()</code>","text":"<p>Returns a dictionary of entity properties that can be displayed to a user interface using a verbose name of the property rather than the internal names</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>the display dictionary</p> Source code in <code>cli/medperf/entities/interface.py</code> <pre><code>def display_dict(self) -&gt; dict:\n\"\"\"Returns a dictionary of entity properties that can be displayed\n    to a user interface using a verbose name of the property rather than\n    the internal names\n    Returns:\n        dict: the display dictionary\n    \"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"reference/entities/interface/#entities.interface.Entity.get","title":"<code>get(uid, local_only=False, valid_only=True)</code>  <code>classmethod</code>","text":"<p>Gets an instance of the respective entity. Wether this requires only local read or remote calls depends on the implementation.</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <code>str</code> <p>Unique Identifier to retrieve the entity</p> required <code>local_only</code> <code>bool</code> <p>If True, the entity will be retrieved locally</p> <code>False</code> <code>valid_only</code> <code>bool</code> <p>if to raise en error in case of invalidated entity</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Entity</code> <code>EntityType</code> <p>Entity Instance associated to the UID</p> Source code in <code>cli/medperf/entities/interface.py</code> <pre><code>@classmethod\ndef get(\ncls: Type[EntityType],\nuid: Union[str, int],\nlocal_only: bool = False,\nvalid_only: bool = True,\n) -&gt; EntityType:\n\"\"\"Gets an instance of the respective entity.\n    Wether this requires only local read or remote calls depends\n    on the implementation.\n    Args:\n        uid (str): Unique Identifier to retrieve the entity\n        local_only (bool): If True, the entity will be retrieved locally\n        valid_only: if to raise en error in case of invalidated entity\n    Returns:\n        Entity: Entity Instance associated to the UID\n    \"\"\"\nif not str(uid).isdigit() or local_only:\nreturn cls.__local_get(uid)\nreturn cls.__remote_get(uid)\n</code></pre>"},{"location":"reference/entities/interface/#entities.interface.Entity.remote_prefilter","title":"<code>remote_prefilter(filters)</code>  <code>staticmethod</code>","text":"<p>Applies filtering logic that must be done before retrieving remote entities</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict</code> <p>filters to apply</p> required <p>Returns:</p> Name Type Description <code>callable</code> <code>callable</code> <p>A function for retrieving remote entities with the applied prefilters</p> Source code in <code>cli/medperf/entities/interface.py</code> <pre><code>@staticmethod\ndef remote_prefilter(filters: dict) -&gt; callable:\n\"\"\"Applies filtering logic that must be done before retrieving remote entities\n    Args:\n        filters (dict): filters to apply\n    Returns:\n        callable: A function for retrieving remote entities with the applied prefilters\n    \"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"reference/entities/interface/#entities.interface.Entity.upload","title":"<code>upload()</code>","text":"<p>Upload the entity-related information to the communication's interface</p> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>Dictionary with the updated entity information</p> Source code in <code>cli/medperf/entities/interface.py</code> <pre><code>def upload(self) -&gt; Dict:\n\"\"\"Upload the entity-related information to the communication's interface\n    Returns:\n        Dict: Dictionary with the updated entity information\n    \"\"\"\nif self.for_test:\nraise InvalidArgumentError(\nf\"This test {self.get_type()} cannot be uploaded.\"\n)\nbody = self.todict()\ncomms_func = self.get_comms_uploader()\nupdated_body = comms_func(body)\nreturn updated_body\n</code></pre>"},{"location":"reference/entities/interface/#entities.interface.Entity.write","title":"<code>write()</code>","text":"<p>Writes the entity to the local storage</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the stored entity</p> Source code in <code>cli/medperf/entities/interface.py</code> <pre><code>def write(self) -&gt; str:\n\"\"\"Writes the entity to the local storage\n    Returns:\n        str: Path to the stored entity\n    \"\"\"\ndata = self.todict()\nmetadata_filename = self.get_metadata_filename()\nentity_file = os.path.join(self.path, metadata_filename)\nos.makedirs(self.path, exist_ok=True)\nwith open(entity_file, \"w\") as f:\nyaml.dump(data, f)\nreturn entity_file\n</code></pre>"},{"location":"reference/entities/report/","title":"Report","text":""},{"location":"reference/entities/report/#entities.report.TestReport","title":"<code>TestReport</code>","text":"<p>             Bases: <code>Entity</code></p> <p>Class representing a compatibility test report entry</p> <p>A test report consists of the components of a test execution: - data used, which can be:     - a demo dataset url and its hash, or     - a raw data path and its labels path, or     - a prepared dataset uid - Data preparation cube if the data used was not already prepared - model cube - evaluator cube - results</p> This entity is only a local one, there is no TestReports on the server <p>However, we still use the same Entity interface used by other entities in order to reduce repeated code. Consequently, we mocked a few methods and attributes inherited from the Entity interface that are not relevant to this entity, such as the <code>name</code> and <code>id</code> attributes, and such as the <code>get</code> and <code>all</code> methods.</p> Source code in <code>cli/medperf/entities/report.py</code> <pre><code>class TestReport(Entity):\n\"\"\"\n    Class representing a compatibility test report entry\n    A test report consists of the components of a test execution:\n    - data used, which can be:\n        - a demo dataset url and its hash, or\n        - a raw data path and its labels path, or\n        - a prepared dataset uid\n    - Data preparation cube if the data used was not already prepared\n    - model cube\n    - evaluator cube\n    - results\n    Note: This entity is only a local one, there is no TestReports on the server\n          However, we still use the same Entity interface used by other entities\n          in order to reduce repeated code. Consequently, we mocked a few methods\n          and attributes inherited from the Entity interface that are not relevant to\n          this entity, such as the `name` and `id` attributes, and such as\n          the `get` and `all` methods.\n    \"\"\"\nname: Optional[str] = \"name\"\ndemo_dataset_url: Optional[str]\ndemo_dataset_hash: Optional[str]\ndata_path: Optional[str]\nlabels_path: Optional[str]\nprepared_data_hash: Optional[str]\ndata_preparation_mlcube: Optional[Union[int, str]]\nmodel: Union[int, str]\ndata_evaluator_mlcube: Union[int, str]\nresults: Optional[dict]\n@staticmethod\ndef get_type():\nreturn \"report\"\n@staticmethod\ndef get_storage_path():\nreturn config.tests_folder\n@staticmethod\ndef get_metadata_filename():\nreturn config.test_report_file\ndef __init__(self, *args, **kwargs):\nsuper().__init__(*args, **kwargs)\nself.id = None\nself.for_test = True\n@property\ndef local_id(self):\n\"\"\"A helper that generates a unique hash for a test report.\"\"\"\nparams = self.todict()\ndel params[\"results\"]\nparams = str(params)\nreturn hashlib.sha256(params.encode()).hexdigest()\ndef set_results(self, results):\nself.results = results\n@classmethod\ndef all(cls, unregistered: bool = False, filters: dict = {}) -&gt; List[\"TestReport\"]:\nassert unregistered, \"Reports are only unregistered\"\nassert filters == {}, \"Reports cannot be filtered\"\nreturn super().all(unregistered=True, filters={})\n@classmethod\ndef get(cls, uid: str, local_only: bool = False, valid_only: bool = True) -&gt; \"TestReport\":\n\"\"\"Gets an instance of the TestReport. ignores local_only inherited flag as TestReport is always a local entity.\n        Args:\n            uid (str): Report Unique Identifier\n            local_only (bool): ignored. Left for aligning with parent Entity class\n            valid_only: if to raise an error in case of invalidated entity\n        Returns:\n            TestReport: Report Instance associated to the UID\n        \"\"\"\nreturn super().get(uid, local_only=True, valid_only=valid_only)\ndef display_dict(self):\nif self.data_path:\ndata_source = f\"{self.data_path}\"[:27] + \"...\"\nelif self.demo_dataset_url:\ndata_source = f\"{self.demo_dataset_url}\"[:27] + \"...\"\nelse:\ndata_source = f\"{self.prepared_data_hash}\"\nreturn {\n\"UID\": self.local_id,\n\"Data Source\": data_source,\n\"Model\": (\nself.model if isinstance(self.model, int) else self.model[:27] + \"...\"\n),\n\"Evaluator\": (\nself.data_evaluator_mlcube\nif isinstance(self.data_evaluator_mlcube, int)\nelse self.data_evaluator_mlcube[:27] + \"...\"\n),\n}\n</code></pre>"},{"location":"reference/entities/report/#entities.report.TestReport.local_id","title":"<code>local_id</code>  <code>property</code>","text":"<p>A helper that generates a unique hash for a test report.</p>"},{"location":"reference/entities/report/#entities.report.TestReport.get","title":"<code>get(uid, local_only=False, valid_only=True)</code>  <code>classmethod</code>","text":"<p>Gets an instance of the TestReport. ignores local_only inherited flag as TestReport is always a local entity.</p> <p>Parameters:</p> Name Type Description Default <code>uid</code> <code>str</code> <p>Report Unique Identifier</p> required <code>local_only</code> <code>bool</code> <p>ignored. Left for aligning with parent Entity class</p> <code>False</code> <code>valid_only</code> <code>bool</code> <p>if to raise an error in case of invalidated entity</p> <code>True</code> <p>Returns:</p> Name Type Description <code>TestReport</code> <code>TestReport</code> <p>Report Instance associated to the UID</p> Source code in <code>cli/medperf/entities/report.py</code> <pre><code>@classmethod\ndef get(cls, uid: str, local_only: bool = False, valid_only: bool = True) -&gt; \"TestReport\":\n\"\"\"Gets an instance of the TestReport. ignores local_only inherited flag as TestReport is always a local entity.\n    Args:\n        uid (str): Report Unique Identifier\n        local_only (bool): ignored. Left for aligning with parent Entity class\n        valid_only: if to raise an error in case of invalidated entity\n    Returns:\n        TestReport: Report Instance associated to the UID\n    \"\"\"\nreturn super().get(uid, local_only=True, valid_only=valid_only)\n</code></pre>"},{"location":"reference/entities/schemas/","title":"Schemas","text":""},{"location":"reference/entities/schemas/#entities.schemas.MedperfSchema","title":"<code>MedperfSchema</code>","text":"<p>             Bases: <code>BaseModel</code></p> Source code in <code>cli/medperf/entities/schemas.py</code> <pre><code>class MedperfSchema(BaseModel):\nfor_test: bool = False\nid: Optional[int]\nname: str = Field(..., max_length=64)\nowner: Optional[int]\nis_valid: bool = True\ncreated_at: Optional[datetime]\nmodified_at: Optional[datetime]\ndef __init__(self, *args, **kwargs):\n\"\"\"Override the ValidationError procedure so we can\n        format the error message in our desired way\n        \"\"\"\ntry:\nsuper().__init__(*args, **kwargs)\nexcept ValidationError as e:\nerrors_dict = defaultdict(list)\nfor error in e.errors():\nfield = error[\"loc\"]\nmsg = error[\"msg\"]\nerrors_dict[field].append(msg)\nerror_msg = \"Field Validation Error:\"\nerror_msg += format_errors_dict(errors_dict)\nraise MedperfException(error_msg)\ndef dict(self, *args, **kwargs) -&gt; dict:\n\"\"\"Overrides dictionary implementation so it filters out\n        fields not defined in the pydantic model\n        Returns:\n            dict: filtered dictionary\n        \"\"\"\nfields = self.__fields__\nvalid_fields = []\n# Gather all the field names, both original an alias names\nfor field_name, field_item in fields.items():\nvalid_fields.append(field_name)\nvalid_fields.append(field_item.alias)\n# Remove duplicates\nvalid_fields = set(valid_fields)\nmodel_dict = super().dict(*args, **kwargs)\nout_dict = {k: v for k, v in model_dict.items() if k in valid_fields}\nreturn out_dict\ndef todict(self) -&gt; dict:\n\"\"\"Dictionary containing both original and alias fields\n        Returns:\n            dict: Extended dictionary representation\n        \"\"\"\nog_dict = self.dict()\nalias_dict = self.dict(by_alias=True)\nog_dict.update(alias_dict)\nfor k, v in og_dict.items():\nif v is None:\nog_dict[k] = \"\"\nif isinstance(v, HttpUrl):\nog_dict[k] = str(v)\nreturn og_dict\n@validator(\"*\", pre=True)\ndef empty_str_to_none(cls, v):\nif v == \"\":\nreturn None\nreturn v\n@validator(\"name\", pre=True, always=True)\ndef name_max_length(cls, v, *, values, **kwargs):\nif not values[\"for_test\"] and len(v) &gt; 20:\nraise ValueError(\"The name must have no more than 20 characters\")\nreturn v\nclass Config:\nallow_population_by_field_name = True\nextra = \"allow\"\nuse_enum_values = True\n</code></pre>"},{"location":"reference/entities/schemas/#entities.schemas.MedperfSchema.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Override the ValidationError procedure so we can format the error message in our desired way</p> Source code in <code>cli/medperf/entities/schemas.py</code> <pre><code>def __init__(self, *args, **kwargs):\n\"\"\"Override the ValidationError procedure so we can\n    format the error message in our desired way\n    \"\"\"\ntry:\nsuper().__init__(*args, **kwargs)\nexcept ValidationError as e:\nerrors_dict = defaultdict(list)\nfor error in e.errors():\nfield = error[\"loc\"]\nmsg = error[\"msg\"]\nerrors_dict[field].append(msg)\nerror_msg = \"Field Validation Error:\"\nerror_msg += format_errors_dict(errors_dict)\nraise MedperfException(error_msg)\n</code></pre>"},{"location":"reference/entities/schemas/#entities.schemas.MedperfSchema.dict","title":"<code>dict(*args, **kwargs)</code>","text":"<p>Overrides dictionary implementation so it filters out fields not defined in the pydantic model</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>filtered dictionary</p> Source code in <code>cli/medperf/entities/schemas.py</code> <pre><code>def dict(self, *args, **kwargs) -&gt; dict:\n\"\"\"Overrides dictionary implementation so it filters out\n    fields not defined in the pydantic model\n    Returns:\n        dict: filtered dictionary\n    \"\"\"\nfields = self.__fields__\nvalid_fields = []\n# Gather all the field names, both original an alias names\nfor field_name, field_item in fields.items():\nvalid_fields.append(field_name)\nvalid_fields.append(field_item.alias)\n# Remove duplicates\nvalid_fields = set(valid_fields)\nmodel_dict = super().dict(*args, **kwargs)\nout_dict = {k: v for k, v in model_dict.items() if k in valid_fields}\nreturn out_dict\n</code></pre>"},{"location":"reference/entities/schemas/#entities.schemas.MedperfSchema.todict","title":"<code>todict()</code>","text":"<p>Dictionary containing both original and alias fields</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Extended dictionary representation</p> Source code in <code>cli/medperf/entities/schemas.py</code> <pre><code>def todict(self) -&gt; dict:\n\"\"\"Dictionary containing both original and alias fields\n    Returns:\n        dict: Extended dictionary representation\n    \"\"\"\nog_dict = self.dict()\nalias_dict = self.dict(by_alias=True)\nog_dict.update(alias_dict)\nfor k, v in og_dict.items():\nif v is None:\nog_dict[k] = \"\"\nif isinstance(v, HttpUrl):\nog_dict[k] = str(v)\nreturn og_dict\n</code></pre>"},{"location":"reference/entities/training_exp/","title":"Training exp","text":""},{"location":"reference/entities/training_exp/#entities.training_exp.TrainingExp","title":"<code>TrainingExp</code>","text":"<p>             Bases: <code>Entity</code>, <code>MedperfSchema</code>, <code>ApprovableSchema</code>, <code>DeployableSchema</code></p> <p>Class representing a TrainingExp</p> <p>a training_exp is a bundle of assets that enables quantitative measurement of the performance of AI models for a specific clinical problem. A TrainingExp instance contains information regarding how to prepare datasets for execution, as well as what models to run and how to evaluate them.</p> Source code in <code>cli/medperf/entities/training_exp.py</code> <pre><code>class TrainingExp(Entity, MedperfSchema, ApprovableSchema, DeployableSchema):\n\"\"\"\n    Class representing a TrainingExp\n    a training_exp is a bundle of assets that enables quantitative\n    measurement of the performance of AI models for a specific\n    clinical problem. A TrainingExp instance contains information\n    regarding how to prepare datasets for execution, as well as\n    what models to run and how to evaluate them.\n    \"\"\"\ndescription: Optional[str] = Field(None, max_length=20)\ndocs_url: Optional[HttpUrl]\ndemo_dataset_tarball_url: str\ndemo_dataset_tarball_hash: Optional[str]\ndemo_dataset_generated_uid: Optional[str]\ndata_preparation_mlcube: int\nfl_mlcube: int\nfl_admin_mlcube: Optional[int]\nplan: dict = {}\nmetadata: dict = {}\nuser_metadata: dict = {}\n@staticmethod\ndef get_type():\nreturn \"training experiment\"\n@staticmethod\ndef get_storage_path():\nreturn config.training_folder\n@staticmethod\ndef get_comms_retriever():\nreturn config.comms.get_training_exp\n@staticmethod\ndef get_metadata_filename():\nreturn config.training_exps_filename\n@staticmethod\ndef get_comms_uploader():\nreturn config.comms.upload_training_exp\ndef __init__(self, *args, **kwargs):\n\"\"\"Creates a new training_exp instance\n        Args:\n            training_exp_desc (Union[dict, TrainingExpModel]): TrainingExp instance description\n        \"\"\"\nsuper().__init__(*args, **kwargs)\nself.plan_path = os.path.join(self.path, config.training_exp_plan_filename)\nself.status_path = os.path.join(self.path, config.training_exp_status_filename)\n@property\ndef local_id(self):\nreturn self.name\n@classmethod\ndef remote_prefilter(cls, filters: dict) -&gt; callable:\n\"\"\"Applies filtering logic that must be done before retrieving remote entities\n        Args:\n            filters (dict): filters to apply\n        Returns:\n            callable: A function for retrieving remote entities with the applied prefilters\n        \"\"\"\ncomms_fn = config.comms.get_training_exps\nif \"owner\" in filters and filters[\"owner\"] == get_medperf_user_data()[\"id\"]:\ncomms_fn = config.comms.get_user_training_exps\nreturn comms_fn\n@classmethod\ndef get_datasets_uids(cls, training_exp_uid: int) -&gt; List[int]:\n\"\"\"Retrieves the list of models associated to the training_exp\n        Args:\n            training_exp_uid (int): UID of the training_exp.\n            comms (Comms): Instance of the communications interface.\n        Returns:\n            List[int]: List of mlcube uids\n        \"\"\"\nassociations = get_experiment_associations(\nexperiment_id=training_exp_uid,\nexperiment_type=\"training_exp\",\ncomponent_type=\"dataset\",\napproval_status=\"APPROVED\",\n)\ndatasets_uids = [assoc[\"dataset\"] for assoc in associations]\nreturn datasets_uids\n@classmethod\ndef get_datasets_with_users(cls, training_exp_uid: int) -&gt; List[int]:\n\"\"\"Retrieves the list of models associated to the training_exp\n        Args:\n            training_exp_uid (int): UID of the training_exp.\n            comms (Comms): Instance of the communications interface.\n        Returns:\n            List[int]: List of mlcube uids\n        \"\"\"\nuids_with_users = config.comms.get_training_datasets_with_users(\ntraining_exp_uid\n)\nreturn uids_with_users\ndef prepare_plan(self):\nwith open(self.plan_path, \"w\") as f:\nyaml.dump(self.plan, f)\ndef display_dict(self):\nreturn {\n\"UID\": self.identifier,\n\"Name\": self.name,\n\"Description\": self.description,\n\"Documentation\": self.docs_url,\n\"Created At\": self.created_at,\n\"FL Container\": int(self.fl_mlcube),\n\"Plan\": self.plan,\n\"State\": self.state,\n\"Registered\": self.is_registered,\n\"Approval Status\": self.approval_status,\n}\n</code></pre>"},{"location":"reference/entities/training_exp/#entities.training_exp.TrainingExp.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Creates a new training_exp instance</p> <p>Parameters:</p> Name Type Description Default <code>training_exp_desc</code> <code>Union[dict, TrainingExpModel]</code> <p>TrainingExp instance description</p> required Source code in <code>cli/medperf/entities/training_exp.py</code> <pre><code>def __init__(self, *args, **kwargs):\n\"\"\"Creates a new training_exp instance\n    Args:\n        training_exp_desc (Union[dict, TrainingExpModel]): TrainingExp instance description\n    \"\"\"\nsuper().__init__(*args, **kwargs)\nself.plan_path = os.path.join(self.path, config.training_exp_plan_filename)\nself.status_path = os.path.join(self.path, config.training_exp_status_filename)\n</code></pre>"},{"location":"reference/entities/training_exp/#entities.training_exp.TrainingExp.get_datasets_uids","title":"<code>get_datasets_uids(training_exp_uid)</code>  <code>classmethod</code>","text":"<p>Retrieves the list of models associated to the training_exp</p> <p>Parameters:</p> Name Type Description Default <code>training_exp_uid</code> <code>int</code> <p>UID of the training_exp.</p> required <code>comms</code> <code>Comms</code> <p>Instance of the communications interface.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>List[int]: List of mlcube uids</p> Source code in <code>cli/medperf/entities/training_exp.py</code> <pre><code>@classmethod\ndef get_datasets_uids(cls, training_exp_uid: int) -&gt; List[int]:\n\"\"\"Retrieves the list of models associated to the training_exp\n    Args:\n        training_exp_uid (int): UID of the training_exp.\n        comms (Comms): Instance of the communications interface.\n    Returns:\n        List[int]: List of mlcube uids\n    \"\"\"\nassociations = get_experiment_associations(\nexperiment_id=training_exp_uid,\nexperiment_type=\"training_exp\",\ncomponent_type=\"dataset\",\napproval_status=\"APPROVED\",\n)\ndatasets_uids = [assoc[\"dataset\"] for assoc in associations]\nreturn datasets_uids\n</code></pre>"},{"location":"reference/entities/training_exp/#entities.training_exp.TrainingExp.get_datasets_with_users","title":"<code>get_datasets_with_users(training_exp_uid)</code>  <code>classmethod</code>","text":"<p>Retrieves the list of models associated to the training_exp</p> <p>Parameters:</p> Name Type Description Default <code>training_exp_uid</code> <code>int</code> <p>UID of the training_exp.</p> required <code>comms</code> <code>Comms</code> <p>Instance of the communications interface.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>List[int]: List of mlcube uids</p> Source code in <code>cli/medperf/entities/training_exp.py</code> <pre><code>@classmethod\ndef get_datasets_with_users(cls, training_exp_uid: int) -&gt; List[int]:\n\"\"\"Retrieves the list of models associated to the training_exp\n    Args:\n        training_exp_uid (int): UID of the training_exp.\n        comms (Comms): Instance of the communications interface.\n    Returns:\n        List[int]: List of mlcube uids\n    \"\"\"\nuids_with_users = config.comms.get_training_datasets_with_users(\ntraining_exp_uid\n)\nreturn uids_with_users\n</code></pre>"},{"location":"reference/entities/training_exp/#entities.training_exp.TrainingExp.remote_prefilter","title":"<code>remote_prefilter(filters)</code>  <code>classmethod</code>","text":"<p>Applies filtering logic that must be done before retrieving remote entities</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>dict</code> <p>filters to apply</p> required <p>Returns:</p> Name Type Description <code>callable</code> <code>callable</code> <p>A function for retrieving remote entities with the applied prefilters</p> Source code in <code>cli/medperf/entities/training_exp.py</code> <pre><code>@classmethod\ndef remote_prefilter(cls, filters: dict) -&gt; callable:\n\"\"\"Applies filtering logic that must be done before retrieving remote entities\n    Args:\n        filters (dict): filters to apply\n    Returns:\n        callable: A function for retrieving remote entities with the applied prefilters\n    \"\"\"\ncomms_fn = config.comms.get_training_exps\nif \"owner\" in filters and filters[\"owner\"] == get_medperf_user_data()[\"id\"]:\ncomms_fn = config.comms.get_user_training_exps\nreturn comms_fn\n</code></pre>"},{"location":"reference/storage/utils/","title":"Utils","text":""},{"location":"reference/ui/cli/","title":"Cli","text":""},{"location":"reference/ui/cli/#ui.cli.CLI","title":"<code>CLI</code>","text":"<p>             Bases: <code>UI</code></p> Source code in <code>cli/medperf/ui/cli.py</code> <pre><code>class CLI(UI):\ndef __init__(self):\nself.spinner = yaspin(color=\"green\")\nself.is_interactive = False\ndef print(self, msg: str = \"\"):\n\"\"\"Display a message on the command line\n        Args:\n            msg (str): message to print\n        \"\"\"\nself._print(msg)\ndef print_error(self, msg: str):\n\"\"\"Display an error message on the command line\n        Args:\n            msg (str): error message to display\n        \"\"\"\nmsg = f\"\u274c {msg}\"\nmsg = typer.style(msg, fg=typer.colors.RED, bold=True)\nself._print(msg)\ndef print_warning(self, msg: str):\n\"\"\"Display a warning message on the command line\n        Args:\n            msg (str): warning message to display\n        \"\"\"\nmsg = typer.style(msg, fg=typer.colors.YELLOW, bold=True)\nself._print(msg)\ndef print_url(self, msg: str):\nself._print(msg)\ndef print_code(self, msg: str):\nself._print(msg)\ndef _print(self, msg: str = \"\"):\nif self.is_interactive:\nself.spinner.write(msg)\nelse:\ntyper.echo(msg)\ndef start_interactive(self):\n\"\"\"Start an interactive session where messages can be overwritten\n        and animations can be displayed\"\"\"\nself.is_interactive = True\nself.spinner.start()\ndef stop_interactive(self):\n\"\"\"Stop an interactive session\"\"\"\nself.is_interactive = False\nself.spinner.stop()\n@contextmanager\ndef interactive(self):\n\"\"\"Context managed interactive session.\n        Yields:\n            CLI: Yields the current CLI instance with an interactive session initialized\n        \"\"\"\nif self.is_interactive:\n# if already interactive, do nothing\nyield self\nelse:\nself.start_interactive()\ntry:\nyield self\nfinally:\nself.stop_interactive()\n@property\ndef text(self):\nreturn self.spinner.text\n@text.setter\ndef text(self, msg: str = \"\"):\n\"\"\"Displays a message that overwrites previous messages if they\n        were created during an interactive ui session.\n        If not on interactive session already, then it calls the ui print function\n        Args:\n            msg (str): message to display\n        \"\"\"\nif not self.is_interactive:\nself.print(msg)\nself.spinner.text = msg\ndef prompt(self, msg: str) -&gt; str:\n\"\"\"Displays a prompt to the user and waits for an answer\n        Args:\n            msg (str): message to use for the prompt\n        Returns:\n            str: user input\n        \"\"\"\nreturn input(msg)\ndef hidden_prompt(self, msg: str) -&gt; str:\n\"\"\"Displays a prompt to the user and waits for an aswer. User input is not displayed\n        Args:\n            msg (str): message to use for the prompt\n        Returns:\n            str: user input\n        \"\"\"\nreturn getpass(msg)\ndef print_highlight(self, msg: str = \"\"):\n\"\"\"Display a highlighted message\n        Args:\n            msg (str): message to print\n        \"\"\"\nmsg = typer.style(msg, fg=typer.colors.GREEN)\nself._print(msg)\ndef print_yaml(self, msg: str):\n\"\"\"Display a yaml object on the command line\n        Args:\n            msg (str): message to display\n        \"\"\"\nself.print()\nself.print(\"=\" * 20)\nself.print(msg)\nself.print(\"=\" * 20)\n</code></pre>"},{"location":"reference/ui/cli/#ui.cli.CLI.hidden_prompt","title":"<code>hidden_prompt(msg)</code>","text":"<p>Displays a prompt to the user and waits for an aswer. User input is not displayed</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>message to use for the prompt</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>user input</p> Source code in <code>cli/medperf/ui/cli.py</code> <pre><code>def hidden_prompt(self, msg: str) -&gt; str:\n\"\"\"Displays a prompt to the user and waits for an aswer. User input is not displayed\n    Args:\n        msg (str): message to use for the prompt\n    Returns:\n        str: user input\n    \"\"\"\nreturn getpass(msg)\n</code></pre>"},{"location":"reference/ui/cli/#ui.cli.CLI.interactive","title":"<code>interactive()</code>","text":"<p>Context managed interactive session.</p> <p>Yields:</p> Name Type Description <code>CLI</code> <p>Yields the current CLI instance with an interactive session initialized</p> Source code in <code>cli/medperf/ui/cli.py</code> <pre><code>@contextmanager\ndef interactive(self):\n\"\"\"Context managed interactive session.\n    Yields:\n        CLI: Yields the current CLI instance with an interactive session initialized\n    \"\"\"\nif self.is_interactive:\n# if already interactive, do nothing\nyield self\nelse:\nself.start_interactive()\ntry:\nyield self\nfinally:\nself.stop_interactive()\n</code></pre>"},{"location":"reference/ui/cli/#ui.cli.CLI.print","title":"<code>print(msg='')</code>","text":"<p>Display a message on the command line</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>message to print</p> <code>''</code> Source code in <code>cli/medperf/ui/cli.py</code> <pre><code>def print(self, msg: str = \"\"):\n\"\"\"Display a message on the command line\n    Args:\n        msg (str): message to print\n    \"\"\"\nself._print(msg)\n</code></pre>"},{"location":"reference/ui/cli/#ui.cli.CLI.print_error","title":"<code>print_error(msg)</code>","text":"<p>Display an error message on the command line</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>error message to display</p> required Source code in <code>cli/medperf/ui/cli.py</code> <pre><code>def print_error(self, msg: str):\n\"\"\"Display an error message on the command line\n    Args:\n        msg (str): error message to display\n    \"\"\"\nmsg = f\"\u274c {msg}\"\nmsg = typer.style(msg, fg=typer.colors.RED, bold=True)\nself._print(msg)\n</code></pre>"},{"location":"reference/ui/cli/#ui.cli.CLI.print_highlight","title":"<code>print_highlight(msg='')</code>","text":"<p>Display a highlighted message</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>message to print</p> <code>''</code> Source code in <code>cli/medperf/ui/cli.py</code> <pre><code>def print_highlight(self, msg: str = \"\"):\n\"\"\"Display a highlighted message\n    Args:\n        msg (str): message to print\n    \"\"\"\nmsg = typer.style(msg, fg=typer.colors.GREEN)\nself._print(msg)\n</code></pre>"},{"location":"reference/ui/cli/#ui.cli.CLI.print_warning","title":"<code>print_warning(msg)</code>","text":"<p>Display a warning message on the command line</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>warning message to display</p> required Source code in <code>cli/medperf/ui/cli.py</code> <pre><code>def print_warning(self, msg: str):\n\"\"\"Display a warning message on the command line\n    Args:\n        msg (str): warning message to display\n    \"\"\"\nmsg = typer.style(msg, fg=typer.colors.YELLOW, bold=True)\nself._print(msg)\n</code></pre>"},{"location":"reference/ui/cli/#ui.cli.CLI.print_yaml","title":"<code>print_yaml(msg)</code>","text":"<p>Display a yaml object on the command line</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>message to display</p> required Source code in <code>cli/medperf/ui/cli.py</code> <pre><code>def print_yaml(self, msg: str):\n\"\"\"Display a yaml object on the command line\n    Args:\n        msg (str): message to display\n    \"\"\"\nself.print()\nself.print(\"=\" * 20)\nself.print(msg)\nself.print(\"=\" * 20)\n</code></pre>"},{"location":"reference/ui/cli/#ui.cli.CLI.prompt","title":"<code>prompt(msg)</code>","text":"<p>Displays a prompt to the user and waits for an answer</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>message to use for the prompt</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>user input</p> Source code in <code>cli/medperf/ui/cli.py</code> <pre><code>def prompt(self, msg: str) -&gt; str:\n\"\"\"Displays a prompt to the user and waits for an answer\n    Args:\n        msg (str): message to use for the prompt\n    Returns:\n        str: user input\n    \"\"\"\nreturn input(msg)\n</code></pre>"},{"location":"reference/ui/cli/#ui.cli.CLI.start_interactive","title":"<code>start_interactive()</code>","text":"<p>Start an interactive session where messages can be overwritten and animations can be displayed</p> Source code in <code>cli/medperf/ui/cli.py</code> <pre><code>def start_interactive(self):\n\"\"\"Start an interactive session where messages can be overwritten\n    and animations can be displayed\"\"\"\nself.is_interactive = True\nself.spinner.start()\n</code></pre>"},{"location":"reference/ui/cli/#ui.cli.CLI.stop_interactive","title":"<code>stop_interactive()</code>","text":"<p>Stop an interactive session</p> Source code in <code>cli/medperf/ui/cli.py</code> <pre><code>def stop_interactive(self):\n\"\"\"Stop an interactive session\"\"\"\nself.is_interactive = False\nself.spinner.stop()\n</code></pre>"},{"location":"reference/ui/factory/","title":"Factory","text":""},{"location":"reference/ui/interface/","title":"Interface","text":""},{"location":"reference/ui/interface/#ui.interface.UI","title":"<code>UI</code>","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>cli/medperf/ui/interface.py</code> <pre><code>class UI(ABC):\n@abstractmethod\ndef print(self, msg: str = \"\"):\n\"\"\"Display a message to the interface. If on interactive session overrides\n        previous message\n        \"\"\"\n@abstractmethod\ndef print_error(self, msg: str):\n\"\"\"Display an error message to the interface\"\"\"\ndef print_warning(self, msg: str):\n\"\"\"Display a warning message on the command line\"\"\"\n@abstractmethod\ndef start_interactive(self):\n\"\"\"Initialize an interactive session for animations or overriding messages.\n        If the UI doesn't support this, the function can be left empty.\n        \"\"\"\n@abstractmethod\ndef stop_interactive(self):\n\"\"\"Terminate an interactive session.\n        If the UI doesn't support this, the function can be left empty.\n        \"\"\"\n@abstractmethod\n@contextmanager\ndef interactive(self):\n\"\"\"Context managed interactive session. Expected to yield the same instance\"\"\"\n@abstractmethod\ndef text(self, msg: str):\n\"\"\"Displays a messages that overwrites previous messages if they were created\n        during an interactive session.\n        If not supported or not on an interactive session, it is expected to fallback\n        to the UI print function.\n        Args:\n            msg (str): message to display\n        \"\"\"\n@abstractmethod\ndef prompt(msg: str) -&gt; str:\n\"\"\"Displays a prompt to the user and waits for an answer\"\"\"\n@abstractmethod\ndef hidden_prompt(self, msg: str) -&gt; str:\n\"\"\"Displays a prompt to the user and waits for an aswer. User input is not displayed\n        Args:\n            msg (str): message to use for the prompt\n        Returns:\n            str: user input\n        \"\"\"\n@abstractmethod\ndef print_highlight(self, msg: str = \"\"):\n\"\"\"Display a message on the command line with green color\n        Args:\n            msg (str): message to print\n        \"\"\"\n@abstractmethod\ndef print_url(self, msg: str = \"\"):\n\"\"\"Display a url\n        Args:\n            msg (str): message to print\n        \"\"\"\n@abstractmethod\ndef print_code(self, msg: str = \"\"):\n\"\"\"Display a user code for authentication\n        Args:\n            msg (str): message to print\n        \"\"\"\n@abstractmethod\ndef print_yaml(self, msg: str = \"\"):\n\"\"\"Display a yaml object on the command line\n        Args:\n            msg (str): message to display\n        \"\"\"\n</code></pre>"},{"location":"reference/ui/interface/#ui.interface.UI.hidden_prompt","title":"<code>hidden_prompt(msg)</code>  <code>abstractmethod</code>","text":"<p>Displays a prompt to the user and waits for an aswer. User input is not displayed</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>message to use for the prompt</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>user input</p> Source code in <code>cli/medperf/ui/interface.py</code> <pre><code>@abstractmethod\ndef hidden_prompt(self, msg: str) -&gt; str:\n\"\"\"Displays a prompt to the user and waits for an aswer. User input is not displayed\n    Args:\n        msg (str): message to use for the prompt\n    Returns:\n        str: user input\n    \"\"\"\n</code></pre>"},{"location":"reference/ui/interface/#ui.interface.UI.interactive","title":"<code>interactive()</code>  <code>abstractmethod</code>","text":"<p>Context managed interactive session. Expected to yield the same instance</p> Source code in <code>cli/medperf/ui/interface.py</code> <pre><code>@abstractmethod\n@contextmanager\ndef interactive(self):\n\"\"\"Context managed interactive session. Expected to yield the same instance\"\"\"\n</code></pre>"},{"location":"reference/ui/interface/#ui.interface.UI.print","title":"<code>print(msg='')</code>  <code>abstractmethod</code>","text":"<p>Display a message to the interface. If on interactive session overrides previous message</p> Source code in <code>cli/medperf/ui/interface.py</code> <pre><code>@abstractmethod\ndef print(self, msg: str = \"\"):\n\"\"\"Display a message to the interface. If on interactive session overrides\n    previous message\n    \"\"\"\n</code></pre>"},{"location":"reference/ui/interface/#ui.interface.UI.print_code","title":"<code>print_code(msg='')</code>  <code>abstractmethod</code>","text":"<p>Display a user code for authentication</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>message to print</p> <code>''</code> Source code in <code>cli/medperf/ui/interface.py</code> <pre><code>@abstractmethod\ndef print_code(self, msg: str = \"\"):\n\"\"\"Display a user code for authentication\n    Args:\n        msg (str): message to print\n    \"\"\"\n</code></pre>"},{"location":"reference/ui/interface/#ui.interface.UI.print_error","title":"<code>print_error(msg)</code>  <code>abstractmethod</code>","text":"<p>Display an error message to the interface</p> Source code in <code>cli/medperf/ui/interface.py</code> <pre><code>@abstractmethod\ndef print_error(self, msg: str):\n\"\"\"Display an error message to the interface\"\"\"\n</code></pre>"},{"location":"reference/ui/interface/#ui.interface.UI.print_highlight","title":"<code>print_highlight(msg='')</code>  <code>abstractmethod</code>","text":"<p>Display a message on the command line with green color</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>message to print</p> <code>''</code> Source code in <code>cli/medperf/ui/interface.py</code> <pre><code>@abstractmethod\ndef print_highlight(self, msg: str = \"\"):\n\"\"\"Display a message on the command line with green color\n    Args:\n        msg (str): message to print\n    \"\"\"\n</code></pre>"},{"location":"reference/ui/interface/#ui.interface.UI.print_url","title":"<code>print_url(msg='')</code>  <code>abstractmethod</code>","text":"<p>Display a url</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>message to print</p> <code>''</code> Source code in <code>cli/medperf/ui/interface.py</code> <pre><code>@abstractmethod\ndef print_url(self, msg: str = \"\"):\n\"\"\"Display a url\n    Args:\n        msg (str): message to print\n    \"\"\"\n</code></pre>"},{"location":"reference/ui/interface/#ui.interface.UI.print_warning","title":"<code>print_warning(msg)</code>","text":"<p>Display a warning message on the command line</p> Source code in <code>cli/medperf/ui/interface.py</code> <pre><code>def print_warning(self, msg: str):\n\"\"\"Display a warning message on the command line\"\"\"\n</code></pre>"},{"location":"reference/ui/interface/#ui.interface.UI.print_yaml","title":"<code>print_yaml(msg='')</code>  <code>abstractmethod</code>","text":"<p>Display a yaml object on the command line</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>message to display</p> <code>''</code> Source code in <code>cli/medperf/ui/interface.py</code> <pre><code>@abstractmethod\ndef print_yaml(self, msg: str = \"\"):\n\"\"\"Display a yaml object on the command line\n    Args:\n        msg (str): message to display\n    \"\"\"\n</code></pre>"},{"location":"reference/ui/interface/#ui.interface.UI.prompt","title":"<code>prompt(msg)</code>  <code>abstractmethod</code>","text":"<p>Displays a prompt to the user and waits for an answer</p> Source code in <code>cli/medperf/ui/interface.py</code> <pre><code>@abstractmethod\ndef prompt(msg: str) -&gt; str:\n\"\"\"Displays a prompt to the user and waits for an answer\"\"\"\n</code></pre>"},{"location":"reference/ui/interface/#ui.interface.UI.start_interactive","title":"<code>start_interactive()</code>  <code>abstractmethod</code>","text":"<p>Initialize an interactive session for animations or overriding messages. If the UI doesn't support this, the function can be left empty.</p> Source code in <code>cli/medperf/ui/interface.py</code> <pre><code>@abstractmethod\ndef start_interactive(self):\n\"\"\"Initialize an interactive session for animations or overriding messages.\n    If the UI doesn't support this, the function can be left empty.\n    \"\"\"\n</code></pre>"},{"location":"reference/ui/interface/#ui.interface.UI.stop_interactive","title":"<code>stop_interactive()</code>  <code>abstractmethod</code>","text":"<p>Terminate an interactive session. If the UI doesn't support this, the function can be left empty.</p> Source code in <code>cli/medperf/ui/interface.py</code> <pre><code>@abstractmethod\ndef stop_interactive(self):\n\"\"\"Terminate an interactive session.\n    If the UI doesn't support this, the function can be left empty.\n    \"\"\"\n</code></pre>"},{"location":"reference/ui/interface/#ui.interface.UI.text","title":"<code>text(msg)</code>  <code>abstractmethod</code>","text":"<p>Displays a messages that overwrites previous messages if they were created during an interactive session. If not supported or not on an interactive session, it is expected to fallback to the UI print function.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>message to display</p> required Source code in <code>cli/medperf/ui/interface.py</code> <pre><code>@abstractmethod\ndef text(self, msg: str):\n\"\"\"Displays a messages that overwrites previous messages if they were created\n    during an interactive session.\n    If not supported or not on an interactive session, it is expected to fallback\n    to the UI print function.\n    Args:\n        msg (str): message to display\n    \"\"\"\n</code></pre>"},{"location":"reference/ui/stdin/","title":"Stdin","text":""},{"location":"reference/ui/stdin/#ui.stdin.StdIn","title":"<code>StdIn</code>","text":"<p>             Bases: <code>UI</code></p> <p>Class for using sys.stdin/sys.stdout exclusively. Used mainly for automating execution with class-like objects. Using only basic IO methods ensures that piping from the command-line. Should not be used in normal execution, as hidden prompts and interactive prints will not work as expected.</p> Source code in <code>cli/medperf/ui/stdin.py</code> <pre><code>class StdIn(UI):\n\"\"\"\n    Class for using sys.stdin/sys.stdout exclusively. Used mainly for automating\n    execution with class-like objects. Using only basic IO methods ensures that\n    piping from the command-line. Should not be used in normal execution, as\n    hidden prompts and interactive prints will not work as expected.\n    \"\"\"\ndef print(self, msg: str = \"\"):\nreturn print(msg)\ndef print_error(self, msg: str):\nreturn self.print(msg)\ndef start_interactive(self):\npass\ndef stop_interactive(self):\npass\n@contextmanager\ndef interactive(self):\nyield self\n@property\ndef text(self):\nreturn \"\"\n@text.setter\ndef text(self, msg: str = \"\"):\nreturn\ndef prompt(self, msg: str) -&gt; str:\nreturn input(msg)\ndef hidden_prompt(self, msg: str) -&gt; str:\nreturn self.prompt(msg)\n</code></pre>"},{"location":"reference/ui/web_ui/","title":"Web ui","text":""},{"location":"reference/ui/web_ui/#ui.web_ui.WebUI","title":"<code>WebUI</code>","text":"<p>             Bases: <code>CLI</code></p> Source code in <code>cli/medperf/ui/web_ui.py</code> <pre><code>class WebUI(CLI):\ndef __init__(self):\nsuper().__init__()\nself.events: Queue[dict] = Queue()\nself.responses: Queue[dict] = Queue()\nself.is_interactive = False\nself.spinner = yaspin(color=\"green\")\nself.task_id = None\nself.request = None\ndef print(self, msg: str = \"\"):\n\"\"\"Display a message on the command line\n        Args:\n            msg (str): message to print\n        \"\"\"\nself._print(msg, \"print\")\ndef print_error(self, msg: str):\n\"\"\"Display an error message on the command line\n        Args:\n            msg (str): error message to display\n        \"\"\"\nmsg = f\"\u274c {msg}\"\nmsg = typer.style(msg, fg=typer.colors.RED, bold=True)\nself._print(msg, \"error\")\ndef print_warning(self, msg: str):\n\"\"\"Display a warning message on the command line\n        Args:\n            msg (str): warning message to display\n        \"\"\"\nmsg = typer.style(msg, fg=typer.colors.YELLOW, bold=True)\nself._print(msg, \"warning\")\ndef _print(self, msg: str = \"\", type: str = \"print\"):\nif self.is_interactive:\nself.spinner.write(msg)\nelse:\ntyper.echo(msg)\nif type == \"print\" and self.is_interactive:\nreturn\nself.set_event(\n{\n\"type\": type,\n\"message\": msg,\n\"interactive\": self.is_interactive,\n\"end\": False,\n}\n)\ndef start_interactive(self):\n\"\"\"Start an interactive session where messages can be overwritten\n        and animations can be displayed\"\"\"\nself.is_interactive = True\nself.spinner.start()  # TODO\ndef stop_interactive(self):\n\"\"\"Stop an interactive session\"\"\"\nself.is_interactive = False\nself.spinner.stop()  # TODO\n@contextmanager\ndef interactive(self):\n\"\"\"Context managed interactive session.\n        Yields:\n            CLI: Yields the current CLI instance with an interactive session initialized\n        \"\"\"\nif self.is_interactive:\n# if already interactive, do nothing\nyield self\nelse:\nself.start_interactive()\ntry:\nyield self\nfinally:\nself.stop_interactive()\n@property\ndef text(self):\nreturn self.spinner.text  # TODO\n@text.setter\ndef text(self, msg: str = \"\"):\n\"\"\"Displays a message that overwrites previous messages if they\n        were created during an interactive ui session.\n        If not on interactive session already, then it calls the ui print function\n        Args:\n            msg (str): message to display\n        \"\"\"\n# if not self.is_interactive:\n#     self.print(msg)\nself.set_event(\n{\n\"type\": \"text\",\n\"message\": msg,\n\"interactive\": self.is_interactive,\n\"end\": False,\n}\n)\nself.spinner.text = msg  # TODO\ndef prompt(self, msg: str) -&gt; str:\n\"\"\"Displays a prompt to the user and waits for an answer\n        Args:\n            msg (str): message to use for the prompt\n        Returns:\n            str: user input\n        \"\"\"\nmsg = msg.replace(\" [Y/n]\", \"\")\nself.set_event(\n{\n\"type\": \"prompt\",\n\"message\": msg,\n\"interactive\": self.is_interactive,\n\"end\": False,\n}\n)\nadd_notification(\nself.request,\nmessage=\"A prompt is waiting for your response in the current running task\",\nreturn_response={\"status\": \"info\"},\n)\nresp = self.get_response()\nif resp[\"value\"]:\nreturn \"y\"\nreturn \"n\"\ndef hidden_prompt(self, msg: str) -&gt; str:\n\"\"\"Displays a prompt to the user and waits for an aswer. User input is not displayed\n        Args:\n            msg (str): message to use for the prompt\n        Returns:\n            str: user input\n        \"\"\"\nreturn super().hidden_prompt()\ndef print_highlight(self, msg: str = \"\"):\n\"\"\"Display a highlighted message\n        Args:\n            msg (str): message to print\n        \"\"\"\nself._print(msg, \"highlight\")\ndef print_yaml(self, msg: str):\n\"\"\"Display a yaml object on the command line\n        Args:\n            msg (str): message to display\n        \"\"\"\nself._print(msg, \"yaml\")\ndef print_url(self, msg: str):\nself._print(msg, \"url\")\ndef print_code(self, msg: str):\nself._print(msg, \"code\")\ndef set_event(self, event):\nevent[\"task_id\"] = self.task_id\nself.events.put(event)\ndef get_event(self):\nreturn self.events.get()\ndef set_response(self, event):\nself.responses.put(event)\ndef get_response(self):\nreturn self.responses.get()\ndef end_task(self, response=None):\nself.set_event(\n{\n\"type\": \"highlight\",\n\"message\": \"\",\n\"interactive\": self.is_interactive,\n\"end\": True,\n\"response\": response,\n}\n)\nself.unset_task_id()\ndef set_task_id(self, task_id):\nself.task_id = task_id\ndef set_request(self, request):\nself.request = request\ndef unset_task_id(self):\nself.task_id = None\n</code></pre>"},{"location":"reference/ui/web_ui/#ui.web_ui.WebUI.hidden_prompt","title":"<code>hidden_prompt(msg)</code>","text":"<p>Displays a prompt to the user and waits for an aswer. User input is not displayed</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>message to use for the prompt</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>user input</p> Source code in <code>cli/medperf/ui/web_ui.py</code> <pre><code>def hidden_prompt(self, msg: str) -&gt; str:\n\"\"\"Displays a prompt to the user and waits for an aswer. User input is not displayed\n    Args:\n        msg (str): message to use for the prompt\n    Returns:\n        str: user input\n    \"\"\"\nreturn super().hidden_prompt()\n</code></pre>"},{"location":"reference/ui/web_ui/#ui.web_ui.WebUI.interactive","title":"<code>interactive()</code>","text":"<p>Context managed interactive session.</p> <p>Yields:</p> Name Type Description <code>CLI</code> <p>Yields the current CLI instance with an interactive session initialized</p> Source code in <code>cli/medperf/ui/web_ui.py</code> <pre><code>@contextmanager\ndef interactive(self):\n\"\"\"Context managed interactive session.\n    Yields:\n        CLI: Yields the current CLI instance with an interactive session initialized\n    \"\"\"\nif self.is_interactive:\n# if already interactive, do nothing\nyield self\nelse:\nself.start_interactive()\ntry:\nyield self\nfinally:\nself.stop_interactive()\n</code></pre>"},{"location":"reference/ui/web_ui/#ui.web_ui.WebUI.print","title":"<code>print(msg='')</code>","text":"<p>Display a message on the command line</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>message to print</p> <code>''</code> Source code in <code>cli/medperf/ui/web_ui.py</code> <pre><code>def print(self, msg: str = \"\"):\n\"\"\"Display a message on the command line\n    Args:\n        msg (str): message to print\n    \"\"\"\nself._print(msg, \"print\")\n</code></pre>"},{"location":"reference/ui/web_ui/#ui.web_ui.WebUI.print_error","title":"<code>print_error(msg)</code>","text":"<p>Display an error message on the command line</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>error message to display</p> required Source code in <code>cli/medperf/ui/web_ui.py</code> <pre><code>def print_error(self, msg: str):\n\"\"\"Display an error message on the command line\n    Args:\n        msg (str): error message to display\n    \"\"\"\nmsg = f\"\u274c {msg}\"\nmsg = typer.style(msg, fg=typer.colors.RED, bold=True)\nself._print(msg, \"error\")\n</code></pre>"},{"location":"reference/ui/web_ui/#ui.web_ui.WebUI.print_highlight","title":"<code>print_highlight(msg='')</code>","text":"<p>Display a highlighted message</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>message to print</p> <code>''</code> Source code in <code>cli/medperf/ui/web_ui.py</code> <pre><code>def print_highlight(self, msg: str = \"\"):\n\"\"\"Display a highlighted message\n    Args:\n        msg (str): message to print\n    \"\"\"\nself._print(msg, \"highlight\")\n</code></pre>"},{"location":"reference/ui/web_ui/#ui.web_ui.WebUI.print_warning","title":"<code>print_warning(msg)</code>","text":"<p>Display a warning message on the command line</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>warning message to display</p> required Source code in <code>cli/medperf/ui/web_ui.py</code> <pre><code>def print_warning(self, msg: str):\n\"\"\"Display a warning message on the command line\n    Args:\n        msg (str): warning message to display\n    \"\"\"\nmsg = typer.style(msg, fg=typer.colors.YELLOW, bold=True)\nself._print(msg, \"warning\")\n</code></pre>"},{"location":"reference/ui/web_ui/#ui.web_ui.WebUI.print_yaml","title":"<code>print_yaml(msg)</code>","text":"<p>Display a yaml object on the command line</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>message to display</p> required Source code in <code>cli/medperf/ui/web_ui.py</code> <pre><code>def print_yaml(self, msg: str):\n\"\"\"Display a yaml object on the command line\n    Args:\n        msg (str): message to display\n    \"\"\"\nself._print(msg, \"yaml\")\n</code></pre>"},{"location":"reference/ui/web_ui/#ui.web_ui.WebUI.prompt","title":"<code>prompt(msg)</code>","text":"<p>Displays a prompt to the user and waits for an answer</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>message to use for the prompt</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>user input</p> Source code in <code>cli/medperf/ui/web_ui.py</code> <pre><code>def prompt(self, msg: str) -&gt; str:\n\"\"\"Displays a prompt to the user and waits for an answer\n    Args:\n        msg (str): message to use for the prompt\n    Returns:\n        str: user input\n    \"\"\"\nmsg = msg.replace(\" [Y/n]\", \"\")\nself.set_event(\n{\n\"type\": \"prompt\",\n\"message\": msg,\n\"interactive\": self.is_interactive,\n\"end\": False,\n}\n)\nadd_notification(\nself.request,\nmessage=\"A prompt is waiting for your response in the current running task\",\nreturn_response={\"status\": \"info\"},\n)\nresp = self.get_response()\nif resp[\"value\"]:\nreturn \"y\"\nreturn \"n\"\n</code></pre>"},{"location":"reference/ui/web_ui/#ui.web_ui.WebUI.start_interactive","title":"<code>start_interactive()</code>","text":"<p>Start an interactive session where messages can be overwritten and animations can be displayed</p> Source code in <code>cli/medperf/ui/web_ui.py</code> <pre><code>def start_interactive(self):\n\"\"\"Start an interactive session where messages can be overwritten\n    and animations can be displayed\"\"\"\nself.is_interactive = True\nself.spinner.start()  # TODO\n</code></pre>"},{"location":"reference/ui/web_ui/#ui.web_ui.WebUI.stop_interactive","title":"<code>stop_interactive()</code>","text":"<p>Stop an interactive session</p> Source code in <code>cli/medperf/ui/web_ui.py</code> <pre><code>def stop_interactive(self):\n\"\"\"Stop an interactive session\"\"\"\nself.is_interactive = False\nself.spinner.stop()  # TODO\n</code></pre>"},{"location":"reference/web_ui/app/","title":"App","text":""},{"location":"reference/web_ui/app/#web_ui.app.run","title":"<code>run(port=typer.Option(8100, '--port', help='port to use'))</code>","text":"<p>Runs a local web UI</p> Source code in <code>cli/medperf/web_ui/app.py</code> <pre><code>@app.command(\"run\")\n@clean_except\ndef run(\nport: int = typer.Option(8100, \"--port\", help=\"port to use\"),\n):\n\"\"\"Runs a local web UI\"\"\"\nimport uvicorn\nhost = \"127.0.0.1\"\nweb_app.state.host_props = {\"host\": host, \"port\": port}\nuvicorn.run(\nweb_app,\nhost=host,\nport=port,\nlog_level=config.loglevel,\n)\n</code></pre>"},{"location":"reference/web_ui/auth/","title":"Auth","text":""},{"location":"reference/web_ui/common/","title":"Common","text":""},{"location":"reference/web_ui/common/#web_ui.common.sort_associations_display","title":"<code>sort_associations_display(associations)</code>","text":"<p>Sorts associations: - by approval status (pending, approved, rejected) - by date (recent first)</p> <p>Parameters:</p> Name Type Description Default <code>associations</code> <code>list[dict]</code> <p>associations to sort</p> required Source code in <code>cli/medperf/web_ui/common.py</code> <pre><code>def sort_associations_display(associations: list[dict]) -&gt; list[dict]:\n\"\"\"\n    Sorts associations:\n    - by approval status (pending, approved, rejected)\n    - by date (recent first)\n    Args:\n        associations: associations to sort\n    Returns: sorted list\n    \"\"\"\napproval_status_order = {\nStatus.PENDING: 0,\nStatus.APPROVED: 1,\nStatus.REJECTED: 2,\n}\ndef assoc_sorting_key(assoc):\n# lower status - first\nstatus_order = approval_status_order.get(assoc[\"approval_status\"], -1)\n# recent associations - first\ndate_order = -parse_datetime(\nassoc[\"approved_at\"] or assoc[\"created_at\"]\n).timestamp()\nreturn status_order, date_order\nreturn sorted(associations, key=assoc_sorting_key)\n</code></pre>"},{"location":"reference/web_ui/events/","title":"Events","text":""},{"location":"reference/web_ui/medperf_login/","title":"Medperf login","text":""},{"location":"reference/web_ui/profiles/","title":"Profiles","text":""},{"location":"reference/web_ui/security_check/","title":"Security check","text":""},{"location":"reference/web_ui/security_check/#web_ui.security_check.sanitize_redirect_url","title":"<code>sanitize_redirect_url(url, fallback='/')</code>","text":"<p>Validate that the URL is a relative path or matches allowed hosts.</p> Source code in <code>cli/medperf/web_ui/security_check.py</code> <pre><code>def sanitize_redirect_url(url: str, fallback: str = \"/\") -&gt; bool:\n\"\"\"Validate that the URL is a relative path or matches allowed hosts.\"\"\"\nnormalized_url = url.replace(\"\\\\\", \"\")  # Normalize backslashes\nparsed = urlparse(normalized_url)\nif not parsed.netloc and not parsed.scheme:\nreturn normalized_url\nreturn fallback\n</code></pre>"},{"location":"reference/web_ui/api/routes/","title":"Routes","text":""},{"location":"reference/web_ui/benchmarks/routes/","title":"Routes","text":""},{"location":"reference/web_ui/containers/routes/","title":"Routes","text":""},{"location":"reference/web_ui/datasets/routes/","title":"Routes","text":""},{"location":"reference/web_ui/yaml_fetch/routes/","title":"Routes","text":""}]}