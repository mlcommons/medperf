import os
from medperf.enums import Status
import yaml
import logging
from typing import List

from medperf.utils import (
    get_uids,
    storage_path,
)
from medperf.entities.interface import Entity
from medperf.exceptions import InvalidArgumentError
import medperf.config as config


class Dataset(Entity):
    """
    Class representing a Dataset

    Datasets are stored locally in the Data Owner's machine. They contain
    information regarding the prepared dataset, such as name and description,
    general statistics and an UID generated by hashing the contents of the
    data preparation output.
    """

    def __init__(self, dataset_dict: dict):
        """Creates a new dataset instance

        Args:
            data_uid (int): The dataset UID as found inside ~/medperf/data/

        Raises:
            NameError: If the dataset with the given UID can't be found, this is thrown.
        """

        self.generated_uid = dataset_dict["generated_uid"]
        self.name = dataset_dict["name"]
        self.description = dataset_dict["description"]
        self.location = dataset_dict["location"]
        self.preparation_cube_uid = dataset_dict["data_preparation_mlcube"]
        self.input_data_hash = dataset_dict["input_data_hash"]
        self.separate_labels = dataset_dict.get(
            "separate_labels", None
        )  # not in the server
        self.split_seed = dataset_dict["split_seed"]
        if "metadata" in dataset_dict:
            # Make sure it is backwards-compatible
            self.generated_metadata = dataset_dict["metadata"]
        else:
            self.generated_metadata = dataset_dict["generated_metadata"]
        if "status" in dataset_dict:
            self.status = Status(dataset_dict["status"])  # not in the server
        else:
            self.status = (
                Status.PENDING if dataset_dict["id"] is None else Status.APPROVED
            )
        self.state = dataset_dict["state"]
        self.is_valid = dataset_dict["is_valid"]
        self.user_metadata = dataset_dict["user_metadata"]

        self.uid = dataset_dict["id"]
        self.created_at = dataset_dict["created_at"]
        self.modified_at = dataset_dict["modified_at"]
        self.owner = dataset_dict["owner"]

        self.dataset_path = os.path.join(
            storage_path(config.data_storage), str(self.generated_uid)
        )
        self.data_path = os.path.join(self.dataset_path, "data")
        self.labels_path = self.data_path
        if self.separate_labels:
            self.labels_path = os.path.join(self.dataset_path, "labels")

    def todict(self):
        return {
            "id": self.uid,
            "name": self.name,
            "description": self.description,
            "location": self.location,
            "data_preparation_mlcube": self.preparation_cube_uid,
            "input_data_hash": self.input_data_hash,
            "generated_uid": self.generated_uid,
            "split_seed": self.split_seed,
            "generated_metadata": self.generated_metadata,
            "status": self.status.value,  # not in the server
            "state": self.state,
            "separate_labels": self.separate_labels,  # not in the server
            "is_valid": self.is_valid,
            "user_metadata": self.user_metadata,
            "created_at": self.created_at,
            "modified_at": self.modified_at,
            "owner": self.owner,
        }

    @classmethod
    def from_generated_uid(cls, generated_uid: str) -> "Dataset":
        generated_uid = cls.__full_uid(generated_uid)
        reg = cls.__get_local_dict(generated_uid)
        return cls(reg)

    @classmethod
    def all(cls) -> List["Dataset"]:
        """Gets and creates instances of all the locally prepared datasets

        Returns:
            List[Dataset]: a list of Dataset instances.
        """
        logging.info("Retrieving all datasets")
        data_storage = storage_path(config.data_storage)
        try:
            generated_uids = next(os.walk(data_storage))[1]
        except StopIteration:
            logging.warning("Couldn't iterate over the dataset directory")
            raise RuntimeError("Couldn't iterate over the dataset directory")
        dsets = []
        for generated_uid in generated_uids:
            dsets.append(cls.from_generated_uid(generated_uid))
        return dsets

    @classmethod
    def get(cls, dset_uid: str) -> "Dataset":
        """Retrieves and creates a Dataset instance from the comms instance.
        If the dataset is present in the user's machine then it retrieves it from there.

        Args:
            dset_uid (str): server UID of the dataset

        Returns:
            Dataset: Specified Dataset Instance
        """
        logging.debug(f"Retrieving dataset {dset_uid}")
        comms = config.comms
        local_dset = list(
            filter(lambda dset: str(dset.uid) == str(dset_uid), cls.all())
        )
        if len(local_dset) == 1:
            logging.debug("Found dataset locally")
            return local_dset[0]

        meta = comms.get_dataset(dset_uid)
        dataset = cls(meta)
        dataset.write()
        return dataset

    @staticmethod
    def __full_uid(uid_hint: str) -> str:
        """Returns the found UID that starts with the provided UID hint

        Args:
            uid_hint (int): a small initial portion of an existing local dataset UID

        Raises:
            NameError: If no dataset is found starting with the given hint, this is thrown.
            NameError: If multiple datasets are found starting with the given hint, this is thrown.

        Returns:
            str: the complete UID
        """
        data_storage = storage_path(config.data_storage)
        dsets = get_uids(data_storage)
        match = [uid for uid in dsets if uid.startswith(str(uid_hint))]
        if len(match) == 0:
            msg = f"No dataset was found with uid hint {uid_hint}."
        elif len(match) > 1:
            msg = f"Multiple datasets were found with uid hint {uid_hint}."
        else:
            return match[0]
        raise InvalidArgumentError(msg)

    def write(self):
        logging.info(f"Updating registration information for dataset: {self.uid}")
        logging.debug(f"registration information: {self.todict()}")
        regfile = os.path.join(self.dataset_path, config.reg_file)
        os.makedirs(self.dataset_path, exist_ok=True)
        with open(regfile, "w") as f:
            yaml.dump(self.todict(), f)

    def upload(self):
        """Uploads the registration information to the comms.

        Args:
            comms (Comms): Instance of the comms interface.
        """
        dataset_dict = self.todict()
        updated_dataset_dict = config.comms.upload_dataset(dataset_dict)
        updated_dataset_dict["status"] = dataset_dict["status"]
        updated_dataset_dict["separate_labels"] = dataset_dict["separate_labels"]
        return updated_dataset_dict

    @classmethod
    def __get_local_dict(cls, generated_uid):
        dataset_path = os.path.join(
            storage_path(config.data_storage), str(generated_uid)
        )
        regfile = os.path.join(dataset_path, config.reg_file)
        with open(regfile, "r") as f:
            reg = yaml.safe_load(f)
        return reg
