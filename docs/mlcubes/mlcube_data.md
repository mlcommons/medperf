---
name: Data Preparator MLCube
url: https://github.com/mlcommons/medperf/examples/HelloWorld/data_preparator
slug: data_preparator
---
# Data Preparator MLCube
## Purpose
Data Preparators are responsible for standardizing the format of input data that models expect to receive. Furthermore, they provide you with tools to test data integrity and extract valuable insights.

{% include "mlcubes/shared/hello_world.md" %}

{% include "mlcubes/shared/setup.md" %}

### Running the Prepare Task
The first step that should be executed with a Data Preparator MLCube is the `prepare` task. This task ingests raw data and makes preliminary transformations to ensure it is ready to be consumed by the Model MLCube in the following steps.

1.  First of all, consider the raw data for the Hello World as an example:
   ```bash
   cat workspace/names/names.txt
   ```
   ```title="names/names.txt"
   --8<-- "examples/HelloWorld/data_preparator/mlcube/workspace/names/names.txt"
   ```
    Note that it contains the names that are expected to be used by the model to generate greetings:
   ```bash
   cat workspace/labels/labels.csv
   ```
   ```title="labels/labels.csv"
   --8<-- "examples/HelloWorld/data_preparator/mlcube/workspace/labels/labels.csv"
   ```
    Additionally, we have the labels for this task. In this case, labels represent what we expect to be generated by the models for this problem.

2. Run the `prepare` task with the mlcube as follows:
    ```bash
    mlcube run --task=prepare
    ```
    Running this task will execute the HelloWorld MLCube with the default values as seen in the corresponding [`mlcube.yaml`](#mlcubemlcubeyaml). You can also define different values for the parameters in the following manner
    ```bash
    mlcube run --task=prepare data_path=names
    ```

3. Check the resulting data using
    ```bash
    cat workspace/data/names.csv
    ```
    ```title="data/names.csv"
    --8<-- "examples/HelloWorld/data_preparator/mlcube/workspace/data/names.csv"
    ```
    Note that the Data Preparator has transformed the raw names into a `.csv` file, which no longer includes the full name but only the first and last name. This is an example of a transformation that a Data Preparator MLCube can perform. By providing logic for handling raw data, you allow model authors to focus on the model design and ideally standardize the data, even if it comes from different sources.

### Running the Sanity Check Task
Now that you have executed the logic to ingest and transform the raw data into usable data, the next step is to ensure that these transformations have gone well. For that, a Data Preparator contains the `sanity_check` task. This task ingests the prepared dataset and computes assertions and quality checks to ensure everything looks as expected.

1. Run the `sanity_check` task with
     ```bash
     mlcube run --task=sanity_check
     ```

    **The `sanity_check` task is only expected to fail and exit if it identifies data anomalies**. Therefore, you should not receive anything from the CLI after a successful 'sanity_check`. **Note:** You can try this out too! We have provided an invalid version of the prepared data.

2. Check the contents of the corrupted prepared data using
   ```bash
   cat workspace/invalid_data/names.csv
   ```
   ```title="invalid_data/names.csv"
   --8<-- "examples/HelloWorld/data_preparator/mlcube/workspace/invalid_data/names.csv"
   ```
   You may notice that this version now contains an unexpected column. This is something that the `sanity_check` task does not expect, and it should react accordingly.
3. Run the `sanity_check` task with the invalid data:
   ```bash
   mlcube run --task=sanity_check data_path=invalid_data
   ```
   You should see the command failing. This is exactly what it is expect from the Sanity Check Task.

### Running the Statistics Task
Lastly, the data preparator contains a `statistics` task for obtaining and sharing insights on the prepared dataset. These insights are to be shared under the Data Owner's approval, which should not disclose private data but general and aggregated statistics.

1. Run the `statistics` task using
    ```bash
    mlcube run --task=statistics
    ```

    By running the `statistics` task, a new file that contains the computed statistics is created (`workspace/statistics.yaml`).

2.  Check the resulting statistics using the following command:
    ```bash
    cat workspace/statistics.yaml
    ```
    ```title="statistics.yaml"
    --8<-- "examples/HelloWorld/data_preparator/mlcube/workspace/statistics.yaml"
    ```
    Here you can see some simple statistics computed on the `names.csv` file we inspected before. These statistics may serve Benchmark Owners and Model Owners to understand the nature and the challenges of this specific dataset. 

That's it! You just built and ran a hello-world data preparator MLCube! Now it's time to build one from scratch!

{% include "mlcubes/shared/build.md" %}

```bash
$ medperf mlcube create data_preparator                      
MedPerf 0.0.0
project_name [Data Preparator MLCube]: Hello World Data Preparator MLCube # (1)!
project_slug [hello_world_data_preparator_mlcube]: data_preparator_mlcube # (2)!
description [Data Preparator MLCube Template. Provided by MLCommons]: Hello World Data Preparator implementation from scratch # (3)!
author_name [John Smith]: John Smith # (4)!
accelerator_count [0]: 0 # (5)!
docker_image_name [docker/image:latest]: johnsmith/hello_world_dataprep:0.0.1 # (6)!
use_separate_output_labels [n]: n # (7)!
```

1. Gives a human-readable name to the MLCube Project.
2. Determines how the Data Preparator root folder will be named.
3. Gives a human-readable description to the MLCube Project.
4. Documents the MLCube implementation by specifying the author. Please use your own name here.
5. Indicates how many GPUs should be visible by the MLCube, which is useful for Model MLCubes.
6. MLCubes use containers under the hood (Medperf supports both Docker and Singularity). Here you can provide an image tag to the image that will be created by this MLCube. **It is recommended to use a naming convention that allows you to upload it to Docker Hub.**
7. Data Preparators can output the labels in a separate location to the data. This is useful in situations where you cannot trust Model Owners to access the labels during inference (during challenges, for example). For this example, we will output data and labels on the same folder.

{% include "mlcubes/shared/cookiecutter.md" %}

After running the previous command, you should be able to see a folder created, named after the `project_slug` provided during configuration.

### Contents
Let's take a look at what the previous command generated. First, lets look at the whole folder structure:
```bash
tree 
```
```bash
.
└── data_preparator_mlcube
    ├── mlcube
    │   ├── mlcube.yaml # (1)!
    │   └── workspace # (2)!
    │       ├── data # (3)!
    │       ├── input_data # (4)!
    │       ├── input_labels # (5)!
    │       └── parameters.yaml # (6)!
    └── project # (7)!
        ├── Dockerfile # (8)!
        └── mlcube.py # (9)!

7 directories, 4 files
```

1. The `mlcube.yaml` file contains metadata about your data preparation procedure, including its interface. For MedPerf, three tasks are required: `prepare`, `sanity_check`, and `statistics`.
2. The `workspace` contains all the files and paths that can be used by the MLCube, as long as those paths are specified inside the `mlcube.yaml`.
3. The `data` folder is where the prepared data will be contained after running the `prepare` task.
4. The `input_data` is where the MLCube will look for raw data by default.
5. The `input_labels` is where the MLCube will look for labels by default.
6. This file provides ways to parameterize the data preparation process. You can set any key-value pairs that should be easily modifiable to adjust your MLCubes' behavior. This file is mandatory but can be left blank if parametrization is unnecessary, like in this example.
7. Contains the actual implementation code of the mlcube.
8. A default `Dockerfile` which by default installs `python3.6` and any requirements for this MLCube to work.
9. `mlcube.py` provides a bare-bones command-line interface for a Data Preparator MLcube to run. The logic inside each command is intentionally left blank.

You can go through each of the created files and modify them as needed to create a Hello World {{ page.meta.name }}.

#### `mlcube/mlcube.yaml`
The `mlcube.yaml` file contains metadata about the mlcube tasks. 

```yaml title="mlcube.yaml"
name: Hello World Data Preparator MLCube # (1)!
description: Hello World Data Preparator implementation from scratch # (2)!
authors:
 - {name: John Smith} # (3)!

platform:
  accelerator_count: 0 # (4)!

docker:
  # Image name
  image: johnsmith/hello_world_dataprep:0.0.1 # (5)!
  # Docker build context relative to $MLCUBE_ROOT. Default is `build`.
  build_context: "../project"
  # Docker file name within docker build context, default is `Dockerfile`.
  build_file: "Dockerfile"

tasks:
  prepare: # (6)!
    parameters:
      inputs: {
        data_path: input_data, # (7)!
        labels_path: input_labels, # (8)!
        parameters_file: parameters.yaml # (9)!
      }
      outputs: {
        output_path: data/, # (10)!
        # output_labels_path: labels/ # (11)!
      }
  sanity_check: # (12)!
    parameters:
      inputs: {
        data_path: data/,
        parameters_file: parameters.yaml
      }
  statistics: # (13)!
    parameters:
      inputs: {
        data_path: data/,
        parameters_file: parameters.yaml
      }
      outputs: {
        output_path: {type: file, default: statistics.yaml} # (14)!
      }
```

1. This is the name it was given during the configuration procedure.
2. This is the description we provided before.
3. Here you can see a list of authors. It is populated with the given author during setup.
4. The accelerator count defines the number of GPUS that should be visible by the MLCube. This was filled during setup.
5. This is the docker image name we provided before.
6. The prepare task transforms the input data and labels into the format expected by model cubes.
7. **Required**. Path to find the input raw data. **MUST** be a folder.
8. **Required**. Path to find the input labels. **MUST** be a folder.
9. **Required**. Helper file to provide additional arguments. The value **MUST** be `parameters.yaml`
10. **Required**. Path to store prepared data and labels. **MUST** be a folder
11. If we had answered yes when asked for separated output labels, this would not be commented.
12. **Required**. The sanity_check task verifies the quality of the transformed data. If the data does not pass the quality check, the `sanity_check` task **MUST** throw an error.
13. **Required**. The statistics task computes general statistics over the prepared data. This serves as a brief description of the data being prepared.
14. **Required**. Path to store the value of the statistics. **MUST** be `statistics.yaml`

In most cases, the `mlcube.yaml` file provided by our template will suffice. For now, let's leave it as is and continue with the other files.

---

#### `mlcube/workspace/parameters.yaml`

This file provides ways to parameterize the data preparation process. You can set any key-value pairs that should be easily modifiable to adjust your MLCubes' behavior. This file is mandatory but can be left blank if parametrization is unnecessary, like in this example. You can see how to work with this file in the Model MLCube documentation.

---

#### `project/mlcube.py`
   
MLCube expects an entry point to the project to run each specified task. It expects this entry point to behave like a CLI, in which each MLCube task (e.g., `prepare`) is executed as a subcommand – and each input/output parameter is passed as a CLI argument. 

``` bash
python3 project/mlcube.py prepare --data_path=<DATA_PATH>  --labels_path=<LABELS_PATH> --parameters_file=<PARAMETERS_FILE> --output_path=<OUTPUT_PATH>
```

!!! note 
    You can implement such a CLI interface with any language or tool as long as you follow the command structure demonstrated above.

```python title="mlcube.py"
"""MLCube handler file"""
import typer


app = typer.Typer()


@app.command("prepare")
def prepare(
    data_path: str = typer.Option(..., "--data_path"),
    labels_path: str = typer.Option(..., "--labels_path"),
    parameters_file: str = typer.Option(..., "--parameters_file"),
    output_path: str = typer.Option(..., "--output_path"),
    
):
    # Modify the prepare command as needed
    raise NotImplementedError("The prepare method is not yet implemented")


@app.command("sanity_check")
def sanity_check(
    data_path: str = typer.Option(..., "--data_path"),
    parameters_file: str = typer.Option(..., "--parameters_file"),
):
    # Modify the sanity_check command as needed
    raise NotImplementedError("The sanity check method is not yet implemented")


@app.command("statistics")
def sanity_check(
    data_path: str = typer.Option(..., "--data_path"),
    parameters_file: str = typer.Option(..., "--parameters_file"),
    out_path: str = typer.Option(..., "--output_path"),
):
    # Modify the statistics command as needed
    raise NotImplementedError("The statistics method is not yet implemented")


if __name__ == "__main__":
    app()
```

The `mlcube.py` generated by the template will already define the CLI interface, but no implementation of each task is provided. This must be filled by you, so let's do a quick implementation for each of the tasks. If you want to see a more detailed implementation, check the [Hello World Data Preparator MLCube](https://github.com/mlcommons/medperf/examples/HelloWorld/data_preparator) that was executed previously in this example.

---

##### Implement the Prepare task
The Prepare task should take paths for data and labels and output a transformed and standardized version of the raw data. The Prepare task defines how it expects to consume the raw data. For real medical tasks, it should ideally provide methods for handling different data formats and standards for the same task.

For this example, we want to consume `.txt` names and `.csv` labels. The output data should be a `.csv` file containing only the subject's `First Name` and `Last Name`. To simplify the process, assume the raw data includes the name in the following format `{first_name} {last_name}`. As for the labels, consider that they are already well-formatted, so we only need to place them in the output path. 

Here is a short implementation of the `prepare` task given the previous assumptions.

```python
import os # (1)!
import shutil

@app.command("prepare") 
def prepare(
    data_path: str = typer.Option(..., "--data_path"),
    labels_path: str = typer.Option(..., "--labels_path"),
    parameters_file: str = typer.Option(..., "--parameters_file"),
    output_path: str = typer.Option(..., "--output_path"),
):
    input_data_file = os.path.join(data_path, "names.txt") # (2)!
    input_labels_file = os.path.join(labels_path, "labels.csv") # (3)!
    output_data_file = os.path.join(output_path, "names.csv") # (4)!
    output_labels_file = os.path.join(output_path, "labels.csv") # (5)!

    # Create the data.csv file
    with open(output_data_file, "w") as output_data:
        output_data.write("First Name,Last Name\n")
        with open(input_data_file, "r") as input_data:
            for line in input_data:
                first_name, last_name = line.split() # (6)!
                output_data.write(f"{first_name},{last_name}\n")
    
    # Copy the labels to its corresponding output file
    shutil.copyfile(input_labels_file, output_labels_file) # (7)!
```
The following numbering corresponds to what is presented in the code snippet above:

1. The necessary imports for this function are provided here, but please place them at the top of your file.
2. Here we are specifying how the input data is expected to look like. For this example, it is expected that all data contain a `names.txt` file. But for real-world tasks, this should be more robust and support many different ways of handling data.
3. Here we are specifying how the labels are expected to look like. Sometimes, labels are obtained automatically from the data. In such cases, the label's path can be left unused during the implementation. A labels path will still need to be passed by Data Owners, so they must be informed of these decisions.
4. Here we are specifying what the output data will look like. This is left to the MLCube author to decide what's the best way to store the data for Model MLCubes to ingest. 
5. Here we are specifying what the labels will look like. The labels are to be handled later on by the Evaluator MLCube.
6. It is assumed the input data only contains names in the format of `{first_name} {last_name}`.
7. It is assumed the labels are already well formatted.

---

##### Implement the Sanity Check task
The `sanity_check` task ensures the prepared data looks as expected. It should check for anomalies, like values outside the expected range, `nan`s, and other undesired outcomes that could occur during the preparation step. **Running this command should raise an error and exit if any tests fail.**

For this example, we can check that the columns are named appropriately and that no field is empty.
```python
import os # (1)!
import pandas

@app.command("sanity_check")
def sanity_check(
    data_path: str = typer.Option(..., "--data_path"), # (2)!
    parameters_file: str = typer.Option(..., "--parameters_file"),
):
    data_file = os.path.join(data_path, "names.csv")
    data = pandas.read_csv(data_file)
    assert data.columns.tolist() == ["First Name", "Last Name"], "Column mismatch"
    assert data["First Name"].isna().sum() == 0, "There are empty fields"
    assert data["Last Name"].isna().sum() == 0, "There are empty fields"
```

1. The necessary imports for this function are provided here, but please place them at the top of your file.
2. You may notice only the `data_path` is being used here. It is still necessary to provide the other arguments so that the command signature matches the expected signature by MLCube.
3. Using `assert` is a very natural way to implement the sanity check. If the assert is False, the program raises an `AssertionError` and exits.

This provides an elementary sanity check for our prepared data. For real-world tasks, checks should be more robust and comprehensive.

---

##### Implement the Statistics task
The `statistics` task computes general statistics from the prepared data. This is meant to inform other users about the nature of the data, so it should not expose private information. For this example, we will compute statistics about the data length. Those statistics should be stored in YAML format in the provided output file.

```python
import os # (1)!
import pandas
import yaml

@app.command("statistics")
def sanity_check(
    data_path: str = typer.Option(..., "--data_path"),
    parameters_file: str = typer.Option(..., "--parameters_file"),
    out_path: str = typer.Option(..., "--output_path"),
):
    data_file = os.path.join(data_path, "names.csv")
    data = pandas.read_csv(data_file)

    fname_len = data["First Name"].str.len()
    lname_len = data["Last Name"].str.len()

    stats = { # (2)!
        "First Name": {
            "length mean": float(fname_len.mean()),
            "length min": int(fname_len.min()),
            "length max": int(fname_len.max())
        },
        "Last Name": {
            "length mean": float(lname_len.mean()),
            "length min": int(lname_len.min()),
            "length max": int(lname_len.max())
        },
        "size": len(data)
    }

    with open(out_path, "w") as f: # (3)!
        yaml.dump(stats, f)
```

1. The necessary imports for this function are provided here, but please place them at the top of your file.
2. The computed statistics are stored as a dictionary. The structure can be arbitrary
3. This is one of the few cases in which a MedPerf MLCube parameter points directly to a file instead of a folder. This is true for all statistics tasks across all Data Preparator MLCubes.

---
##### The complete implementation
Here you can find the whole `mlcube.py` file after all the implementation changes that have been made:

```python title="project/mlcube.py"
"""MLCube handler file"""
import typer
import os
import shutil
import pandas
import yaml


app = typer.Typer()


@app.command("prepare")
def prepare(
    data_path: str = typer.Option(..., "--data_path"),
    labels_path: str = typer.Option(..., "--labels_path"),
    parameters_file: str = typer.Option(..., "--parameters_file"),
    output_path: str = typer.Option(..., "--output_path"),
):
    input_data_file = os.path.join(data_path, "names.txt")
    input_labels_file = os.path.join(labels_path, "labels.csv")
    output_data_file = os.path.join(output_path, "names.csv")
    output_labels_file = os.path.join(output_path, "labels.csv")

    # Create the data.csv file
    with open(output_data_file, "w") as output_data:
        output_data.write("First Name,Last Name\n")
        with open(input_data_file, "r") as input_data:
            for line in input_data:
                first_name, last_name = line.split()
                output_data.write(f"{first_name},{last_name}\n")

    # Copy the labels to its corresponding output file
    shutil.copyfile(input_labels_file, output_labels_file)


@app.command("sanity_check")
def sanity_check(
    data_path: str = typer.Option(..., "--data_path"),
    parameters_file: str = typer.Option(..., "--parameters_file"),
):
    data_file = os.path.join(data_path, "names.csv")
    data = pandas.read_csv(data_file)
    assert data.columns.tolist() == ["First Name", "Last Name"], "Column mismatch"
    assert data["First Name"].isna().sum() == 0, "There are empty fields"
    assert data["Last Name"].isna().sum() == 0, "There are empty fields"


@app.command("statistics")
def sanity_check(
    data_path: str = typer.Option(..., "--data_path"),
    parameters_file: str = typer.Option(..., "--parameters_file"),
    out_path: str = typer.Option(..., "--output_path"),
):
    data_file = os.path.join(data_path, "names.csv")
    data = pandas.read_csv(data_file)

    fname_len = data["First Name"].str.len()
    lname_len = data["Last Name"].str.len()

    stats = {
        "First Name": {
            "length mean": float(fname_len.mean()),
            "length min": int(fname_len.min()),
            "length max": int(fname_len.max()),
        },
        "Last Name": {
            "length mean": float(lname_len.mean()),
            "length min": int(lname_len.min()),
            "length max": int(lname_len.max()),
        },
        "size": len(data),
    }

    with open(out_path, "w") as f:
        yaml.dump(stats, f)


if __name__ == "__main__":
    app()
```

---

{% include "mlcubes/shared/docker_file.md" %}

---

{% include "mlcubes/shared/requirements.md" %}

```python title="project/requirements.txt"
# Add your dependencies here
typer
PyYAML
pandas
```

---
That's it! With that, we should have a functional Data Preparator MLCube, that has been implemented for our Hello World Task. Congratulations!

{% include "mlcubes/shared/execute.md" %}

3. Provide input data to your mlcube. In this case, you can easily create sample data for our task. Assuming you are in the `tutorial/data_preparator_mlcube/mlcube` path, you can run
    ```bash
    echo "John Smith" >> workspace/input_data/names.txt
    echo "id,greeting\n0,\"Hello, John smith\"\n1,\"Howdy, John Smith\"\n2,\"Greetings, John Smith\"\n3,\"Bonjour, John Smith\"" >> workspace/input_labels/labels.csv
    ```
    This will create the raw data and labels files. You can inspect them with
    ```bash
    cat workspace/input_data/names.txt
    cat workspace/input_labels/labels.csv
    ```
    ```bash title="workspace/input_data/names.txt"
    John Smith
    ```
    ```bash title="workspace/input_labels/labels.csv"
    id,greeting
    0,"Hello, John smith"
    1,"Howdy, John Smith"
    2,"Greetings, John Smith"
    3,"Bonjour, John Smith"
    ```
4. Run the pipeline. If you want details of what is going on at each step, look for the [How to run](#how-to-run) section.
    ```bash
    mlcube run --task=prepare
    mlcube run --task=sanity_check
    mlcube run --task=statistics
    ```
That's it! You should now be able to write your own Data Preparator MLCubes from scratch!